[
    {
        "paper_id": 1,
        "title": "Advanced System Integration: Analyzing OpenAPI Chunking for\n  Retrieval-Augmented Generation",
        "summary": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score.",
        "published": "2024-11-29T16:09:43Z",
        "link": "http://arxiv.org/abs/2411.19804v1",
        "authors": [
            "Robin D. Pesl",
            "Jerin G. Mathew",
            "Massimo Mecella",
            "Marco Aiello"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.19804v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0001_Advanced_System_Integration_Analyzing_OpenAPI_Chunking_for_Retrieval-Augmented_Generation.pdf"
    },
    {
        "paper_id": 2,
        "title": "CantorNet: A Sandbox for Testing Topological and Geometrical Measures",
        "summary": "Many natural phenomena are characterized by self-similarity, for example the\nsymmetry of human faces, or a repetitive motif of a song. Studying of such\nsymmetries will allow us to gain deeper insights into the underlying mechanisms\nof complex systems. Recognizing the importance of understanding these patterns,\nwe propose a geometrically inspired framework to study such phenomena in\nartificial neural networks. To this end, we introduce \\emph{CantorNet},\ninspired by the triadic construction of the Cantor set, which was introduced by\nGeorg Cantor in the $19^\\text{th}$ century. In mathematics, the Cantor set is a\nset of points lying on a single line that is self-similar and has a counter\nintuitive property of being an uncountably infinite null set. Similarly, we\nintroduce CantorNet as a sandbox for studying self-similarity by means of novel\ntopological and geometrical complexity measures. CantorNet constitutes a family\nof ReLU neural networks that spans the whole spectrum of possible Kolmogorov\ncomplexities, including the two opposite descriptions (linear and exponential\nas measured by the description length). CantorNet's decision boundaries can be\narbitrarily ragged, yet are analytically known. Besides serving as a testing\nground for complexity measures, our work may serve to illustrate potential\npitfalls in geometry-ignorant data augmentation techniques and adversarial\nattacks.",
        "published": "2024-11-29T14:01:34Z",
        "link": "http://arxiv.org/abs/2411.19713v1",
        "authors": [
            "Michal Lewandowski",
            "Hamid Eghbalzadeh",
            "Bernhard A. Moser"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.19713v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0002_CantorNet_A_Sandbox_for_Testing_Topological_and_Geometrical_Measures.pdf"
    },
    {
        "paper_id": 3,
        "title": "Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating\n  RAG Systems",
        "summary": "Retrieval Augmented Generation (RAG) systems are a widespread application of\nLarge Language Models (LLMs) in the industry. While many tools exist empowering\ndevelopers to build their own systems, measuring their performance locally,\nwith datasets reflective of the system's use cases, is a technological\nchallenge. Solutions to this problem range from non-specific and cheap (most\npublic datasets) to specific and costly (generating data from local documents).\nIn this paper, we show that using public question and answer (Q&A) datasets to\nassess retrieval performance can lead to non-optimal systems design, and that\ncommon tools for RAG dataset generation can lead to unbalanced data. We propose\nsolutions to these issues based on the characterization of RAG datasets through\nlabels and through label-targeted data generation. Finally, we show that\nfine-tuned small LLMs can efficiently generate Q&A datasets. We believe that\nthese observations are invaluable to the know-your-data step of RAG systems\ndevelopment.",
        "published": "2024-11-29T13:57:07Z",
        "link": "http://arxiv.org/abs/2411.19710v1",
        "authors": [
            "Rafael Teixeira de Lima",
            "Shubham Gupta",
            "Cesar Berrospi",
            "Lokesh Mishra",
            "Michele Dolfi",
            "Peter Staar",
            "Panagiotis Vagenas"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.19710v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0003_Know_Your_RAG_Dataset_Taxonomy_and_Generation_Strategies_for_Evaluating_RAG_Systems.pdf"
    },
    {
        "paper_id": 4,
        "title": "Unimib Assistant: designing a student-friendly RAG-based chatbot for all\n  their needs",
        "summary": "Natural language processing skills of Large Language Models (LLMs) are\nunprecedented, having wide diffusion and application in different tasks. This\npilot study focuses on specializing ChatGPT behavior through a\nRetrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs\nfeature. The purpose of our chatbot, called Unimib Assistant, is to provide\ninformation and solutions to the specific needs of University of Milano-Bicocca\n(Unimib) students through a question-answering approach. We provided the system\nwith a prompt highlighting its specific purpose and behavior, as well as\nuniversity-related documents and links obtained from an initial need-finding\nphase, interviewing six students. After a preliminary customization phase, a\nqualitative usability test was conducted with six other students to identify\nthe strengths and weaknesses of the chatbot, with the goal of improving it in a\nsubsequent redesign phase. While the chatbot was appreciated for its\nuser-friendly experience, perceived general reliability, well-structured\nresponses, and conversational tone, several significant technical and\nfunctional limitations emerged. In particular, the satisfaction and overall\nexperience of the users was impaired by the system's inability to always\nprovide fully accurate information. Moreover, it would often neglect to report\nrelevant information even if present in the materials uploaded and prompt\ngiven. Furthermore, it sometimes generated unclickable links, undermining its\ntrustworthiness, since providing the source of information was an important\naspect for our users. Further in-depth studies and feedback from other users as\nwell as implementation iterations are planned to refine our Unimib Assistant.",
        "published": "2024-11-29T09:07:21Z",
        "link": "http://arxiv.org/abs/2411.19554v1",
        "authors": [
            "Chiara Antico",
            "Stefano Giordano",
            "Cansu Koyuturk",
            "Dimitri Ognibene"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.19554v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0004_Unimib_Assistant_designing_a_student-friendly_RAG-based_chatbot_for_all_their_needs.pdf"
    },
    {
        "paper_id": 5,
        "title": "Knowledge Management for Automobile Failure Analysis Using Graph RAG",
        "summary": "This paper presents a knowledge management system for automobile failure\nanalysis using retrieval-augmented generation (RAG) with large language models\n(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a\ngrowing demand for knowledge transfer of failure analysis from experienced\nengineers to young engineers. However, failure events are phenomena that occur\nin a chain reaction, making them difficult for beginners to analyze them. While\nknowledge graphs, which can describe semantic relationships and structure\ninformation is effective in representing failure events, due to their\ncapability of representing the relationships between components, there is much\ninformation in KGs, so it is challenging for young engineers to extract and\nunderstand sub-graphs from the KG. On the other hand, there is increasing\ninterest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for\nknowledge management. However, when using the current Graph RAG framework with\nan existing knowledge graph for automobile failures, several issues arise\nbecause it is difficult to generate executable queries for a knowledge graph\ndatabase which is not constructed by LLMs. To address this, we focused on\noptimizing the Graph RAG pipeline for existing knowledge graphs. Using an\noriginal Q&A dataset, the ROUGE F1 score of the sentences generated by the\nproposed method showed an average improvement of 157.6% compared to the current\nmethod. This highlights the effectiveness of the proposed method for automobile\nfailure analysis.",
        "published": "2024-11-29T08:34:07Z",
        "link": "http://arxiv.org/abs/2411.19539v1",
        "authors": [
            "Yuta Ojima",
            "Hiroki Sakaji",
            "Tadashi Nakamura",
            "Hiroaki Sakata",
            "Kazuya Seki",
            "Yuu Teshigawara",
            "Masami Yamashita",
            "Kazuhiro Aoyama"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.19539v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0005_Knowledge_Management_for_Automobile_Failure_Analysis_Using_Graph_RAG.pdf"
    },
    {
        "paper_id": 6,
        "title": "RAGDiffusion: Faithful Cloth Generation via External Knowledge\n  Assimilation",
        "summary": "Standard clothing asset generation involves creating forward-facing flat-lay\ngarment images displayed on a clear background by extracting clothing\ninformation from diverse real-world contexts, which presents significant\nchallenges due to highly standardized sampling distributions and precise\nstructural requirements in the generated images. Existing models have limited\nspatial perception and often exhibit structural hallucinations in this\nhigh-specification generative task. To address this issue, we propose a novel\nRetrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance\nstructure determinacy and mitigate hallucinations by assimilating external\nknowledge from LLM and databases. RAGDiffusion consists of two core processes:\n(1) Retrieval-based structure aggregation, which employs contrastive learning\nand a Structure Locally Linear Embedding (SLLE) to derive global structure and\nspatial landmarks, providing both soft and hard guidance to counteract\nstructural ambiguities; and (2) Omni-level faithful garment generation, which\nintroduces a three-level alignment that ensures fidelity in structural,\npattern, and decoding components within the diffusing. Extensive experiments on\nchallenging real-world datasets demonstrate that RAGDiffusion synthesizes\nstructurally and detail-faithful clothing assets with significant performance\nimprovements, representing a pioneering effort in high-specification faithful\ngeneration with RAG to confront intrinsic hallucinations and enhance fidelity.",
        "published": "2024-11-29T07:57:32Z",
        "link": "http://arxiv.org/abs/2411.19528v1",
        "authors": [
            "Xianfeng Tan",
            "Yuhan Li",
            "Wenxiang Shang",
            "Yubo Wu",
            "Jian Wang",
            "Xuanhong Chen",
            "Yi Zhang",
            "Ran Lin",
            "Bingbing Ni"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.19528v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0006_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation.pdf"
    },
    {
        "paper_id": 7,
        "title": "Towards Understanding Retrieval Accuracy and Prompt Quality in RAG\n  Systems",
        "summary": "Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the\ncapability of large language models (LLMs) and has demonstrated promising\nefficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show\nsuperior performance, they face unique challenges in stability and reliability.\nTheir complexity hinders developers' efforts to design, maintain, and optimize\neffective RAG systems. Therefore, it is crucial to understand how RAG's\nperformance is impacted by its design. In this work, we conduct an early\nexploratory study toward a better understanding of the mechanism of RAG\nsystems, covering three code datasets, three QA datasets, and two LLMs. We\nfocus on four design factors: retrieval document type, retrieval recall,\ndocument selection, and prompt techniques. Our study uncovers how each factor\nimpacts system correctness and confidence, providing valuable insights for\ndeveloping an accurate and reliable RAG system. Based on these findings, we\npresent nine actionable guidelines for detecting defects and optimizing the\nperformance of RAG systems. We hope our early exploration can inspire further\nadvancements in engineering, improving and maintaining LLM-driven intelligent\nsoftware systems for greater efficiency and reliability.",
        "published": "2024-11-29T04:25:31Z",
        "link": "http://arxiv.org/abs/2411.19463v1",
        "authors": [
            "Shengming Zhao",
            "Yuheng Huang",
            "Jiayang Song",
            "Zhijie Wang",
            "Chengcheng Wan",
            "Lei Ma"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.19463v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0007_Towards_Understanding_Retrieval_Accuracy_and_Prompt_Quality_in_RAG_Systems.pdf"
    },
    {
        "paper_id": 8,
        "title": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language\n  Models",
        "summary": "Iterative retrieval refers to the process in which the model continuously\nqueries the retriever during generation to enhance the relevance of the\nretrieved knowledge, thereby improving the performance of Retrieval-Augmented\nGeneration (RAG). Existing work typically employs few-shot prompting or\nmanually constructed rules to implement iterative retrieval. This introduces\nadditional inference overhead and overlooks the remarkable reasoning\ncapabilities of Large Language Models (LLMs). In this paper, we introduce\nAuto-RAG, an autonomous iterative retrieval model centered on the LLM's\npowerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues\nwith the retriever, systematically planning retrievals and refining queries to\nacquire valuable knowledge. This process continues until sufficient external\ninformation is gathered, at which point the results are presented to the user.\nTo this end, we develop a method for autonomously synthesizing reasoning-based\ndecision-making instructions in iterative retrieval and fine-tuned the latest\nopen-source LLMs. The experimental results indicate that Auto-RAG is capable of\nautonomous iterative interaction with the retriever, effectively leveraging the\nremarkable reasoning and decision-making abilities of LLMs, which lead to\noutstanding performance across six benchmarks. Further analysis reveals that\nAuto-RAG can autonomously adjust the number of iterations based on the\ndifficulty of the questions and the utility of the retrieved knowledge, without\nrequiring any human intervention. Moreover, Auto-RAG expresses the iterative\nretrieval process in natural language, enhancing interpretability while\nproviding users with a more intuitive experience\\footnote{Code is available at\n\\url{https://github.com/ictnlp/Auto-RAG}.",
        "published": "2024-11-29T03:01:05Z",
        "link": "http://arxiv.org/abs/2411.19443v1",
        "authors": [
            "Tian Yu",
            "Shaolei Zhang",
            "Yang Feng"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.19443v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0008_Auto-RAG_Autonomous_Retrieval-Augmented_Generation_for_Large_Language_Models.pdf"
    },
    {
        "paper_id": 9,
        "title": "Habit Coach: Customising RAG-based chatbots to support behavior change",
        "summary": "This paper presents the iterative development of Habit Coach, a GPT-based\nchatbot designed to support users in habit change through personalized\ninteraction. Employing a user-centered design approach, we developed the\nchatbot using a Retrieval-Augmented Generation (RAG) system, which enables\nbehavior personalization without retraining the underlying language model\n(GPT-4). The system leverages document retrieval and specialized prompts to\ntailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and\nnarrative therapy techniques. A key challenge in the development process was\nthe difficulty of translating declarative knowledge into effective interaction\nbehaviors. In the initial phase, the chatbot was provided with declarative\nknowledge about CBT via reference textbooks and high-level conversational\ngoals. However, this approach resulted in imprecise and inefficient behavior,\nas the GPT model struggled to convert static information into dynamic and\ncontextually appropriate interactions. This highlighted the limitations of\nrelying solely on declarative knowledge to guide chatbot behavior, particularly\nin nuanced, therapeutic conversations. Over four iterations, we addressed this\nissue by gradually transitioning towards procedural knowledge, refining the\nchatbot's interaction strategies, and improving its overall effectiveness. In\nthe final evaluation, 5 participants engaged with the chatbot over five\nconsecutive days, receiving individualized CBT interventions. The Self-Report\nHabit Index (SRHI) was used to measure habit strength before and after the\nintervention, revealing a reduction in habit strength post-intervention. These\nresults underscore the importance of procedural knowledge in driving effective,\npersonalized behavior change support in RAG-based systems.",
        "published": "2024-11-28T15:53:27Z",
        "link": "http://arxiv.org/abs/2411.19229v1",
        "authors": [
            "Arian Fooroogh Mand Arabi",
            "Cansu Koyuturk",
            "Michael O'Mahony",
            "Raffaella Calati",
            "Dimitri Ognibene"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.19229v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0009_Habit_Coach_Customising_RAG-based_chatbots_to_support_behavior_change.pdf"
    },
    {
        "paper_id": 10,
        "title": "Knowledge Database or Poison Base? Detecting RAG Poisoning Attack\n  through LLM Activations",
        "summary": "As Large Language Models (LLMs) are progressively deployed across diverse\nfields and real-world applications, ensuring the security and robustness of\nLLMs has become ever more critical. Retrieval-Augmented Generation (RAG) is a\ncutting-edge approach designed to address the limitations of large language\nmodels (LLMs). By retrieving information from the relevant knowledge database,\nRAG enriches the input to LLMs, enabling them to produce responses that are\nmore accurate and contextually appropriate. It is worth noting that the\nknowledge database, being sourced from publicly available channels such as\nWikipedia, inevitably introduces a new attack surface. RAG poisoning involves\ninjecting malicious texts into the knowledge database, ultimately leading to\nthe generation of the attacker's target response (also called poisoned\nresponse). However, there are currently limited methods available for detecting\nsuch poisoning attacks. We aim to bridge the gap in this work. Particularly, we\nintroduce RevPRAG, a flexible and automated detection pipeline that leverages\nthe activations of LLMs for poisoned response detection. Our investigation\nuncovers distinct patterns in LLMs' activations when generating correct\nresponses versus poisoned responses. Our results on multiple benchmark datasets\nand RAG architectures show our approach could achieve 98% true positive rate,\nwhile maintaining false positive rates close to 1%. We also evaluate recent\nbackdoor detection methods specifically designed for LLMs and applicable for\nidentifying poisoned responses in RAG. The results demonstrate that our\napproach significantly surpasses them.",
        "published": "2024-11-28T06:29:46Z",
        "link": "http://arxiv.org/abs/2411.18948v1",
        "authors": [
            "Xue Tan",
            "Hao Luan",
            "Mingyu Luo",
            "Xiaoyan Sun",
            "Ping Chen",
            "Jun Dai"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.18948v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0010_Knowledge_Database_or_Poison_Base_Detecting_RAG_Poisoning_Attack_through_LLM_Activations.pdf"
    },
    {
        "paper_id": 11,
        "title": "ICLERB: In-Context Learning Embedding and Reranker Benchmark",
        "summary": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform new\ntasks by conditioning on prompts with relevant information. Retrieval-Augmented\nGeneration (RAG) enhances ICL by incorporating retrieved documents into the\nLLM's context at query time. However, traditional retrieval methods focus on\nsemantic relevance, treating retrieval as a search problem. In this paper, we\npropose reframing retrieval for ICL as a recommendation problem, aiming to\nselect documents that maximize utility in ICL tasks. We introduce the\nIn-Context Learning Embedding and Reranker Benchmark (ICLERB), a novel\nevaluation framework that compares retrievers based on their ability to enhance\nLLM accuracy in ICL settings. Additionally, we propose a novel Reinforcement\nLearning-to-Rank from AI Feedback (RLRAIF) algorithm, designed to fine-tune\nretrieval models using minimal feedback from the LLM. Our experimental results\nreveal notable differences between ICLERB and existing benchmarks, and\ndemonstrate that small models fine-tuned with our RLRAIF algorithm outperform\nlarge state-of-the-art retrieval models. These findings highlight the\nlimitations of existing evaluation methods and the need for specialized\nbenchmarks and training strategies adapted to ICL.",
        "published": "2024-11-28T06:28:45Z",
        "link": "http://arxiv.org/abs/2411.18947v1",
        "authors": [
            "Marie Al Ghossein",
            "Emile Contal",
            "Alexandre Robicquet"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.18947v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0011_ICLERB_In-Context_Learning_Embedding_and_Reranker_Benchmark.pdf"
    },
    {
        "paper_id": 12,
        "title": "Automated Literature Review Using NLP Techniques and LLM-Based\n  Retrieval-Augmented Generation",
        "summary": "This research presents and compares multiple approaches to automate the\ngeneration of literature reviews using several Natural Language Processing\n(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language\nModel (LLM). The ever-increasing number of research articles provides a huge\nchallenge for manual literature review. It has resulted in an increased demand\nfor automation. Developing a system capable of automatically generating the\nliterature reviews from only the PDF files as input is the primary objective of\nthis research work. The effectiveness of several Natural Language Processing\n(NLP) strategies, such as the frequency-based method (spaCy), the transformer\nmodel (Simple T5), and retrieval-augmented generation (RAG) with Large Language\nModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR\ndataset is chosen for this research experiment and three distinct techniques\nare utilized to implement three different systems for auto-generating the\nliterature reviews. The ROUGE scores are used for the evaluation of all three\nsystems. Based on the evaluation, the Large Language Model GPT-3.5-turbo\nachieved the highest ROUGE-1 score, 0.364. The transformer model comes in\nsecond place and spaCy is at the last position. Finally, a graphical user\ninterface is created for the best system based on the large language model.",
        "published": "2024-11-27T18:27:07Z",
        "link": "http://arxiv.org/abs/2411.18583v1",
        "authors": [
            "Nurshat Fateh Ali",
            "Md. Mahdi Mohtasim",
            "Shakil Mosharrof",
            "T. Gopi Krishna"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.18583v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0012_Automated_Literature_Review_Using_NLP_Techniques_and_LLM-Based_Retrieval-Augmented_Generation.pdf"
    },
    {
        "paper_id": 13,
        "title": "Evaluating and Improving the Robustness of Security Attack Detectors\n  Generated by LLMs",
        "summary": "Large Language Models (LLMs) are increasingly used in software development to\ngenerate functions, such as attack detectors, that implement security\nrequirements. However, LLMs struggle to generate accurate code, resulting,\ne.g., in attack detectors that miss well-known attacks when used in practice.\nThis is most likely due to the LLM lacking knowledge about some existing\nattacks and to the generated code being not evaluated in real usage scenarios.\nWe propose a novel approach integrating Retrieval Augmented Generation (RAG)\nand Self-Ranking into the LLM pipeline. RAG enhances the robustness of the\noutput by incorporating external knowledge sources, while the Self-Ranking\ntechnique, inspired to the concept of Self-Consistency, generates multiple\nreasoning paths and creates ranks to select the most robust detector. Our\nextensive empirical study targets code generated by LLMs to detect two\nprevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL\ninjection (SQLi). Results show a significant improvement in detection\nperformance compared to baselines, with an increase of up to 71%pt and 37%pt in\nthe F2-Score for XSS and SQLi detection, respectively.",
        "published": "2024-11-27T10:48:37Z",
        "link": "http://arxiv.org/abs/2411.18216v1",
        "authors": [
            "Samuele Pasini",
            "Jinhan Kim",
            "Tommaso Aiello",
            "Rocio Cabrera Lozoya",
            "Antonino Sabetta",
            "Paolo Tonella"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.18216v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0013_Evaluating_and_Improving_the_Robustness_of_Security_Attack_Detectors_Generated_by_LLMs.pdf"
    },
    {
        "paper_id": 14,
        "title": "Path-RAG: Knowledge-Guided Key Region Retrieval for Open-ended Pathology\n  Visual Question Answering",
        "summary": "Accurate diagnosis and prognosis assisted by pathology images are essential\nfor cancer treatment selection and planning. Despite the recent trend of\nadopting deep-learning approaches for analyzing complex pathology images, they\nfall short as they often overlook the domain-expert understanding of tissue\nstructure and cell composition. In this work, we focus on a challenging\nOpen-ended Pathology VQA (PathVQA-Open) task and propose a novel framework\nnamed Path-RAG, which leverages HistoCartography to retrieve relevant domain\nknowledge from pathology images and significantly improves performance on\nPathVQA-Open. Admitting the complexity of pathology image analysis, Path-RAG\nadopts a human-centered AI approach by retrieving domain knowledge using\nHistoCartography to select the relevant patches from pathology images. Our\nexperiments suggest that domain guidance can significantly boost the accuracy\nof LLaVA-Med from 38% to 47%, with a notable gain of 28% for H&E-stained\npathology images in the PathVQA-Open dataset. For longer-form question and\nanswer pairs, our model consistently achieves significant improvements of 32.5%\nin ARCH-Open PubMed and 30.6% in ARCH-Open Books on H\\&E images. Our code and\ndataset is available here (https://github.com/embedded-robotics/path-rag).",
        "published": "2024-11-26T03:22:01Z",
        "link": "http://arxiv.org/abs/2411.17073v1",
        "authors": [
            "Awais Naeem",
            "Tianhao Li",
            "Huang-Ru Liao",
            "Jiawei Xu",
            "Aby M. Mathew",
            "Zehao Zhu",
            "Zhen Tan",
            "Ajay Kumar Jaiswal",
            "Raffi A. Salibian",
            "Ziniu Hu",
            "Tianlong Chen",
            "Ying Ding"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.17073v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0014_Path-RAG_Knowledge-Guided_Key_Region_Retrieval_for_Open-ended_Pathology_Visual_Question_Answering.pdf"
    },
    {
        "paper_id": 15,
        "title": "LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology\n  Report Generation",
        "summary": "In the current paradigm of image captioning, deep learning models are trained\nto generate text from image embeddings of latent features. We challenge the\nassumption that these latent features ought to be high-dimensional vectors\nwhich require model fine tuning to handle. Here we propose Label Boosted\nRetrieval Augmented Generation (LaB-RAG), a text-based approach to image\ncaptioning that leverages image descriptors in the form of categorical labels\nto boost standard retrieval augmented generation (RAG) with pretrained large\nlanguage models (LLMs). We study our method in the context of radiology report\ngeneration (RRG), where the task is to generate a clinician's report detailing\ntheir observations from a set of radiological images, such as X-rays. We argue\nthat simple linear classifiers over extracted image embeddings can effectively\ntransform X-rays into text-space as radiology-specific labels. In combination\nwith standard RAG, we show that these derived text labels can be used with\ngeneral-domain LLMs to generate radiology reports. Without ever training our\ngenerative language model or image feature encoder models, and without ever\ndirectly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves\nbetter results across natural language and radiology language metrics compared\nwith other retrieval-based RRG methods, while attaining competitive results\ncompared to other fine-tuned vision-language RRG models. We further present\nresults of our experiments with various components of LaB-RAG to better\nunderstand our method. Finally, we critique the use of a popular RRG metric,\narguing it is possible to artificially inflate its results without true\ndata-leakage.",
        "published": "2024-11-25T16:10:05Z",
        "link": "http://arxiv.org/abs/2411.16523v1",
        "authors": [
            "Steven Song",
            "Anirudh Subramanyam",
            "Irene Madejski",
            "Robert L. Grossman"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.16523v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0015_LaB-RAG_Label_Boosted_Retrieval_Augmented_Generation_for_Radiology_Report_Generation.pdf"
    },
    {
        "paper_id": 16,
        "title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning",
        "summary": "Recent advancements in large language models (LLMs) have led to significant\nimprovements in various natural language processing tasks, but it is still\nchallenging for LLMs to perform knowledge-intensive complex question answering\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\nchain-like sub-questions and applies iterative RAG at each sub-question.\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\nto decompose complex questions into combinations of three atomic knowledge\noperators, significantly enhancing the reasoning process at both the planning\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\nthat AtomR significantly outperforms state-of-the-art baselines across three\nsingle-source and two multi-source reasoning benchmarks, with notable\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.",
        "published": "2024-11-25T15:35:51Z",
        "link": "http://arxiv.org/abs/2411.16495v1",
        "authors": [
            "Amy Xin",
            "Jinxin Liu",
            "Zijun Yao",
            "Zhicheng Li",
            "Shulin Cao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.16495v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0016_AtomR_Atomic_Operator-Empowered_Large_Language_Models_for_Heterogeneous_Knowledge_Reasoning.pdf"
    },
    {
        "paper_id": 17,
        "title": "Human-Calibrated Automated Testing and Validation of Generative Language\n  Models",
        "summary": "This paper introduces a comprehensive framework for the evaluation and\nvalidation of generative language models (GLMs), with a focus on\nRetrieval-Augmented Generation (RAG) systems deployed in high-stakes domains\nsuch as banking. GLM evaluation is challenging due to open-ended outputs and\nsubjective quality assessments. Leveraging the structured nature of RAG\nsystems, where generated responses are grounded in a predefined document\ncollection, we propose the Human-Calibrated Automated Testing (HCAT) framework.\nHCAT integrates a) automated test generation using stratified sampling, b)\nembedding-based metrics for explainable assessment of functionality, risk and\nsafety attributes, and c) a two-stage calibration approach that aligns\nmachine-generated evaluations with human judgments through probability\ncalibration and conformal prediction.\n  In addition, the framework includes robustness testing to evaluate model\nperformance against adversarial, out-of-distribution, and varied input\nconditions, as well as targeted weakness identification using marginal and\nbivariate analysis to pinpoint specific areas for improvement. This\nhuman-calibrated, multi-layered evaluation framework offers a scalable,\ntransparent, and interpretable approach to GLM assessment, providing a\npractical and reliable solution for deploying GLMs in applications where\naccuracy, transparency, and regulatory compliance are paramount.",
        "published": "2024-11-25T13:53:36Z",
        "link": "http://arxiv.org/abs/2411.16391v1",
        "authors": [
            "Agus Sudjianto",
            "Aijun Zhang",
            "Srinivas Neppalli",
            "Tarun Joshi",
            "Michal Malohlava"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.16391v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0017_Human-Calibrated_Automated_Testing_and_Validation_of_Generative_Language_Models.pdf"
    },
    {
        "paper_id": 18,
        "title": "Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark,\n  Evaluate Metrics and Strong Baselines",
        "summary": "This paper investigates an intriguing task of Multi-modal Retrieval Augmented\nMulti-modal Generation (M$^2$RAG). This task requires foundation models to\nbrowse multi-modal web pages, with mixed text and images, and generate\nmulti-modal responses for solving user queries, which exhibits better\ninformation density and readability. Given the early researching stage of\nM$^2$RAG task, there is a lack of systematic studies and analysis. To fill this\ngap, we construct a benchmark for M$^2$RAG task, equipped with a suite of\ntext-modal metrics and multi-modal metrics to analyze the capabilities of\nexisting foundation models. Besides, we also propose several effective methods\nfor foundation models to accomplish this task, based on the comprehensive\nevaluation results on our benchmark. Extensive experimental results reveal\nseveral intriguing phenomena worth further research.",
        "published": "2024-11-25T13:20:19Z",
        "link": "http://arxiv.org/abs/2411.16365v1",
        "authors": [
            "Zi-Ao Ma",
            "Tian Lan",
            "Rong-Cheng Tu",
            "Yong Hu",
            "Heyan Huang",
            "Xian-Ling Mao"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.16365v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0018_Multi-modal_Retrieval_Augmented_Multi-modal_Generation_A_Benchmark_Evaluate_Metrics_and_Strong_Baselines.pdf"
    },
    {
        "paper_id": 19,
        "title": "Context Awareness Gate For Retrieval Augmented Generation",
        "summary": "Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems.",
        "published": "2024-11-25T06:48:38Z",
        "link": "http://arxiv.org/abs/2411.16133v1",
        "authors": [
            "Mohammad Hassan Heydari",
            "Arshia Hemmat",
            "Erfan Naman",
            "Afsaneh Fatemi"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.16133v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0019_Context_Awareness_Gate_For_Retrieval_Augmented_Generation.pdf"
    },
    {
        "paper_id": 20,
        "title": "RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large\n  Language Models on Dietary Supplements",
        "summary": "\\textbf{Objective:} We aimed to develop an advanced multi-task large language\nmodel (LLM) framework to extract multiple types of information about dietary\nsupplements (DS) from clinical records.\n  \\textbf{Methods:} We used four core DS information extraction tasks - namely,\nnamed entity recognition (NER: 2,949 clinical sentences), relation extraction\n(RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage\nclassification (UC: 2,460 sentences) as our multitasks. We introduced a novel\nRetrieval-Augmented Multi-task Information Extraction (RAMIE) Framework,\nincluding: 1) employed instruction fine-tuning techniques with task-specific\nprompts, 2) trained LLMs for multiple tasks with improved storage efficiency\nand lower training costs, and 3) incorporated retrieval augmentation generation\n(RAG) techniques by retrieving similar examples from the training set. We\ncompared RAMIE's performance to LLMs with instruction fine-tuning alone and\nconducted an ablation study to assess the contributions of multi-task learning\nand RAG to improved multitasking performance.\n  \\textbf{Results:} With the aid of the RAMIE framework, Llama2-13B achieved an\nF1 score of 87.39 (3.51\\% improvement) on the NER task and demonstrated\noutstanding performance on the RE task with an F1 score of 93.74 (1.15\\%\nimprovement). For the TE task, Llama2-7B scored 79.45 (14.26\\% improvement),\nand MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94\\% improvement) on\nthe UC task. The ablation study revealed that while MTL increased efficiency\nwith a slight trade-off in performance, RAG significantly boosted overall\naccuracy.\n  \\textbf{Conclusion:} This study presents a novel RAMIE framework that\ndemonstrates substantial improvements in multi-task information extraction for\nDS-related data from clinical records. Our framework can potentially be applied\nto other domains.",
        "published": "2024-11-24T03:56:43Z",
        "link": "http://arxiv.org/abs/2411.15700v1",
        "authors": [
            "Zaifu Zhan",
            "Shuang Zhou",
            "Mingchen Li",
            "Rui Zhang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.15700v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0020_RAMIE_Retrieval-Augmented_Multi-task_Information_Extraction_with_Large_Language_Models_on_Dietary_Supplements.pdf"
    },
    {
        "paper_id": 21,
        "title": "Document Haystacks: Vision-Language Reasoning Over Piles of 1000+\n  Documents",
        "summary": "Large multimodal models (LMMs) have achieved impressive progress in\nvision-language understanding, yet they face limitations in real-world\napplications requiring complex reasoning over a large number of images.\nExisting benchmarks for multi-image question-answering are limited in scope,\neach question is paired with only up to 30 images, which does not fully capture\nthe demands of large-scale retrieval tasks encountered in the real-world\nusages. To reduce these gaps, we introduce two document haystack benchmarks,\ndubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on\nlarge-scale visual document retrieval and understanding. Additionally, we\npropose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG)\nframework that leverages a suite of multimodal vision encoders, each optimized\nfor specific strengths, and a dedicated question-document relevance module.\nV-RAG sets a new standard, with a 9% and 11% improvement in Recall@1 on the\nchallenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively,\ncompared to the previous best baseline models. Additionally, integrating V-RAG\nwith LMMs enables them to efficiently operate across thousands of images,\nyielding significant improvements on our DocHaystack and InfoHaystack\nbenchmarks. Our code and datasets are available at\nhttps://github.com/Vision-CAIR/dochaystacks",
        "published": "2024-11-23T18:14:42Z",
        "link": "http://arxiv.org/abs/2411.16740v1",
        "authors": [
            "Jun Chen",
            "Dannong Xu",
            "Junjie Fei",
            "Chun-Mei Feng",
            "Mohamed Elhoseiny"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.16740v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0021_Document_Haystacks_Vision-Language_Reasoning_Over_Piles_of_1000_Documents.pdf"
    },
    {
        "paper_id": 22,
        "title": "From MTEB to MTOB: Retrieval-Augmented Classification for Descriptive\n  Grammars",
        "summary": "Recent advances in language modeling have demonstrated significant\nimprovements in zero-shot capabilities, including in-context learning,\ninstruction following, and machine translation for extremely under-resourced\nlanguages (Tanzer et al., 2024). However, many languages with limited written\nresources rely primarily on formal descriptions of grammar and vocabulary.\n  In this paper, we introduce a set of benchmarks to evaluate how well models\ncan extract and classify information from the complex descriptions found in\nlinguistic grammars. We present a Retrieval-Augmented Generation (RAG)-based\napproach that leverages these descriptions for downstream tasks such as machine\ntranslation. Our benchmarks encompass linguistic descriptions for 248 languages\nacross 142 language families, focusing on typological features from WALS and\nGrambank.\n  This set of benchmarks offers the first comprehensive evaluation of language\nmodels' in-context ability to accurately interpret and extract linguistic\nfeatures, providing a critical resource for scaling NLP to low-resource\nlanguages. The code and data are publicly available at\n\\url{https://github.com/al-the-eigenvalue/RAG-on-grammars}.",
        "published": "2024-11-23T14:47:10Z",
        "link": "http://arxiv.org/abs/2411.15577v1",
        "authors": [
            "Albert Kornilov",
            "Tatiana Shavrina"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.15577v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0022_From_MTEB_to_MTOB_Retrieval-Augmented_Classification_for_Descriptive_Grammars.pdf"
    },
    {
        "paper_id": 23,
        "title": "Multi-Reranker: Maximizing performance of retrieval-augmented generation\n  in the FinanceRAG challenge",
        "summary": "As Large Language Models (LLMs) increasingly address domain-specific\nproblems, their application in the financial sector has expanded rapidly. Tasks\nthat are both highly valuable and time-consuming, such as analyzing financial\nstatements, disclosures, and related documents, are now being effectively\ntackled using LLMs. This paper details the development of a high-performance,\nfinance-specific Retrieval-Augmented Generation (RAG) system for the ACM-ICAIF\n'24 FinanceRAG competition. We optimized performance through ablation studies\non query expansion and corpus refinement during the pre-retrieval phase. To\nenhance retrieval accuracy, we employed multiple reranker models. Notably, we\nintroduced an efficient method for managing long context sizes during the\ngeneration phase, significantly improving response quality without sacrificing\nperformance. We ultimately achieve 2nd place in the FinanceRAG Challenge. Our\nkey contributions include: (1) pre-retrieval ablation analysis, (2) an enhanced\nretrieval algorithm, and (3) a novel approach for long-context management. This\nwork demonstrates the potential of LLMs in effectively processing and analyzing\ncomplex financial data to generate accurate and valuable insights. The source\ncode and further details are available at https://github.com/cv-lee/FinanceRAG.",
        "published": "2024-11-23T09:56:21Z",
        "link": "http://arxiv.org/abs/2411.16732v1",
        "authors": [
            "Joohyun Lee",
            "Minji Roh"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.16732v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0023_Multi-Reranker_Maximizing_performance_of_retrieval-augmented_generation_in_the_FinanceRAG_challenge.pdf"
    },
    {
        "paper_id": 24,
        "title": "Traditional Chinese Medicine Case Analysis System for High-Level\n  Semantic Abstraction: Optimized with Prompt and RAG",
        "summary": "This paper details a technical plan for building a clinical case database for\nTraditional Chinese Medicine (TCM) using web scraping. Leveraging multiple\nplatforms, including 360doc, we gathered over 5,000 TCM clinical cases,\nperformed data cleaning, and structured the dataset with crucial fields such as\npatient details, pathogenesis, syndromes, and annotations. Using the\n$Baidu\\_ERNIE\\_Speed\\_128K$ API, we removed redundant information and generated\nthe final answers through the $DeepSeekv2$ API, outputting results in standard\nJSON format. We optimized data recall with RAG and rerank techniques during\nretrieval and developed a hybrid matching scheme. By combining two-stage\nretrieval method with keyword matching via Jieba, we significantly enhanced the\naccuracy of model outputs.",
        "published": "2024-11-23T08:24:15Z",
        "link": "http://arxiv.org/abs/2411.15491v1",
        "authors": [
            "Peng Xu",
            "Hongjin Wu",
            "Jinle Wang",
            "Rongjia Lin",
            "Liwei Tan"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.15491v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0024_Traditional_Chinese_Medicine_Case_Analysis_System_for_High-Level_Semantic_Abstraction_Optimized_with_Prompt_and_RAG.pdf"
    },
    {
        "paper_id": 25,
        "title": "Improving Factuality of 3D Brain MRI Report Generation with Paired\n  Image-domain Retrieval and Text-domain Augmentation",
        "summary": "Acute ischemic stroke (AIS) requires time-critical management, with hours of\ndelayed intervention leading to an irreversible disability of the patient.\nSince diffusion weighted imaging (DWI) using the magnetic resonance image (MRI)\nplays a crucial role in the detection of AIS, automated prediction of AIS from\nDWI has been a research topic of clinical importance. While text radiology\nreports contain the most relevant clinical information from the image findings,\nthe difficulty of mapping across different modalities has limited the\nfactuality of conventional direct DWI-to-report generation methods. Here, we\npropose paired image-domain retrieval and text-domain augmentation (PIRTA), a\ncross-modal retrieval-augmented generation (RAG) framework for providing\nclinician-interpretative AIS radiology reports with improved factuality. PIRTA\nmitigates the need for learning cross-modal mapping, which poses difficulty in\nimage-to-text generation, by casting the cross-modal mapping problem as an\nin-domain retrieval of similar DWI images that have paired ground-truth text\nradiology reports. By exploiting the retrieved radiology reports to augment the\nreport generation process of the query image, we show by experiments with\nextensive in-house and public datasets that PIRTA can accurately retrieve\nrelevant reports from 3D DWI images. This approach enables the generation of\nradiology reports with significantly higher accuracy compared to direct\nimage-to-text generation using state-of-the-art multimodal language models.",
        "published": "2024-11-23T08:18:55Z",
        "link": "http://arxiv.org/abs/2411.15490v1",
        "authors": [
            "Junhyeok Lee",
            "Yujin Oh",
            "Dahyoun Lee",
            "Hyon Keun Joh",
            "Chul-Ho Sohn",
            "Sung Hyun Baik",
            "Cheol Kyu Jung",
            "Jung Hyun Park",
            "Kyu Sung Choi",
            "Byung-Hoon Kim",
            "Jong Chul Ye"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.15490v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0025_Improving_Factuality_of_3D_Brain_MRI_Report_Generation_with_Paired_Image-domain_Retrieval_and_Text-domain_Augmentation.pdf"
    },
    {
        "paper_id": 26,
        "title": "mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for\n  Knowledge-Based VQA",
        "summary": "Advanced Multimodal Large Language Models (MLLMs) struggle with recent\nKnowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their\nlimited and frozen knowledge scope, often leading to ambiguous and inaccurate\nresponses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally\nintroduced to provide MLLMs with comprehensive and up-to-date knowledge,\neffectively expanding the knowledge scope. However, current mRAG methods have\ninherent drawbacks, including: 1) Performing retrieval even when external\nknowledge is not needed. 2) Lacking of identification of evidence that supports\nthe query. 3) Increasing model complexity due to additional information\nfiltering modules or rules. To address these shortcomings, we propose a novel\ngeneralized framework called \\textbf{m}ultimodal\n\\textbf{R}etrieval-\\textbf{R}eflection-\\textbf{A}ugmented \\textbf{G}eneration\n(mR$^2$AG), which achieves adaptive retrieval and useful information\nlocalization to enable answers through two easy-to-implement reflection\noperations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection\nis designed to distinguish different user queries and avoids redundant\nretrieval calls, and Relevance-Reflection is introduced to guide the MLLM in\nlocating beneficial evidence of the retrieved content and generating answers\naccordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM\nwith efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset\n(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,\nGPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while\nmaintaining the exceptional capabilities of base MLLMs across a wide range of\nVisual-dependent tasks.",
        "published": "2024-11-22T16:15:50Z",
        "link": "http://arxiv.org/abs/2411.15041v1",
        "authors": [
            "Tao Zhang",
            "Ziqi Zhang",
            "Zongyang Ma",
            "Yuxin Chen",
            "Zhongang Qi",
            "Chunfeng Yuan",
            "Bing Li",
            "Junfu Pu",
            "Yuxuan Zhao",
            "Zehua Xie",
            "Jin Ma",
            "Ying Shan",
            "Weiming Hu"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.15041v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0026_mR2AG_Multimodal_Retrieval-Reflection-Augmented_Generation_for_Knowledge-Based_VQA.pdf"
    },
    {
        "paper_id": 27,
        "title": "G-RAG: Knowledge Expansion in Material Science",
        "summary": "In the field of Material Science, effective information retrieval systems are\nessential for facilitating research. Traditional Retrieval-Augmented Generation\n(RAG) approaches in Large Language Models (LLMs) often encounter challenges\nsuch as outdated information, hallucinations, limited interpretability due to\ncontext constraints, and inaccurate retrieval. To address these issues, Graph\nRAG integrates graph databases to enhance the retrieval process. Our proposed\nmethod processes Material Science documents by extracting key entities\n(referred to as MatIDs) from sentences, which are then utilized to query\nexternal Wikipedia knowledge bases (KBs) for additional relevant information.\nWe implement an agent-based parsing technique to achieve a more detailed\nrepresentation of the documents. Our improved version of Graph RAG called G-RAG\nfurther leverages a graph database to capture relationships between these\nentities, improving both retrieval accuracy and contextual understanding. This\nenhanced approach demonstrates significant improvements in performance for\ndomains that require precise information retrieval, such as Material Science.",
        "published": "2024-11-21T21:22:58Z",
        "link": "http://arxiv.org/abs/2411.14592v1",
        "authors": [
            "Radeen Mostafa",
            "Mirza Nihal Baig",
            "Mashaekh Tausif Ehsan",
            "Jakir Hasan"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.14592v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0027_G-RAG_Knowledge_Expansion_in_Material_Science.pdf"
    },
    {
        "paper_id": 28,
        "title": "Towards Knowledge Checking in Retrieval-augmented Generation: A\n  Representation Perspective",
        "summary": "Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing\nthe performance of Large Language Models (LLMs). However, these systems face\nchallenges in effectively integrating external knowledge with the LLM's\ninternal knowledge, often leading to issues with misleading or unhelpful\ninformation. This work aims to provide a systematic study on knowledge checking\nin RAG systems. We conduct a comprehensive analysis of LLM representation\nbehaviors and demonstrate the significance of using representations in\nknowledge checking. Motivated by the findings, we further develop\nrepresentation-based classifiers for knowledge filtering. We show substantial\nimprovements in RAG performance, even when dealing with noisy knowledge\ndatabases. Our study provides new insights into leveraging LLM representations\nfor enhancing the reliability and effectiveness of RAG systems.",
        "published": "2024-11-21T20:39:13Z",
        "link": "http://arxiv.org/abs/2411.14572v1",
        "authors": [
            "Shenglai Zeng",
            "Jiankun Zhang",
            "Bingheng Li",
            "Yuping Lin",
            "Tianqi Zheng",
            "Dante Everaert",
            "Hanqing Lu",
            "Hui Liu",
            "Hui Liu",
            "Yue Xing",
            "Monica Xiao Cheng",
            "Jiliang Tang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.14572v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0028_Towards_Knowledge_Checking_in_Retrieval-augmented_Generation_A_Representation_Perspective.pdf"
    },
    {
        "paper_id": 29,
        "title": "Enhancing LLMs for Power System Simulations: A Feedback-driven\n  Multi-agent Framework",
        "summary": "The integration of experimental technologies with large language models\n(LLMs) is transforming scientific research, positioning AI as a versatile\nresearch assistant rather than a mere problem-solving tool. In the field of\npower systems, however, managing simulations -- one of the essential\nexperimental technologies -- remains a challenge for LLMs due to their limited\ndomain-specific knowledge, restricted reasoning capabilities, and imprecise\nhandling of simulation parameters. To address these limitations, we propose a\nfeedback-driven, multi-agent framework that incorporates three proposed\nmodules: an enhanced retrieval-augmented generation (RAG) module, an improved\nreasoning module, and a dynamic environmental acting module with an\nerror-feedback mechanism. Validated on 69 diverse tasks from Daline and\nMATPOWER, this framework achieves success rates of 93.13% and 96.85%,\nrespectively, significantly outperforming the latest LLMs (ChatGPT 4o and\no1-preview), which achieved a 27.77% success rate on standard simulation tasks\nand 0% on complex tasks. Additionally, our framework also supports rapid,\ncost-effective task execution, completing each simulation in approximately 30\nseconds at an average cost of 0.014 USD for tokens. Overall, this adaptable\nframework lays a foundation for developing intelligent LLM-based assistants for\nhuman researchers, facilitating power system research and beyond.",
        "published": "2024-11-21T19:01:07Z",
        "link": "http://arxiv.org/abs/2411.16707v1",
        "authors": [
            "Mengshuo Jia",
            "Zeyu Cui",
            "Gabriela Hug"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.16707v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0029_Enhancing_LLMs_for_Power_System_Simulations_A_Feedback-driven_Multi-agent_Framework.pdf"
    },
    {
        "paper_id": 30,
        "title": "Efficient Aspect-Based Summarization of Climate Change Reports with\n  Small Language Models",
        "summary": "The use of Natural Language Processing (NLP) for helping decision-makers with\nClimate Change action has recently been highlighted as a use case aligning with\na broader drive towards NLP technologies for social good. In this context,\nAspect-Based Summarization (ABS) systems that extract and summarize relevant\ninformation are particularly useful as they provide stakeholders with a\nconvenient way of finding relevant information in expert-curated reports. In\nthis work, we release a new dataset for ABS of Climate Change reports and we\nemploy different Large Language Models (LLMs) and so-called Small Language\nModels (SLMs) to tackle this problem in an unsupervised way. Considering the\nproblem at hand, we also show how SLMs are not significantly worse for the\nproblem while leading to reduced carbon footprint; we do so by applying for the\nfirst time an existing framework considering both energy efficiency and task\nperformance to the evaluation of zero-shot generative models for ABS. Overall,\nour results show that modern language models, both big and small, can\neffectively tackle ABS for Climate Change reports but more research is needed\nwhen we frame the problem as a Retrieval Augmented Generation (RAG) problem and\nour work and dataset will help foster efforts in this direction.",
        "published": "2024-11-21T16:28:32Z",
        "link": "http://arxiv.org/abs/2411.14272v1",
        "authors": [
            "Iacopo Ghinassi",
            "Leonardo Catalano",
            "Tommaso Colella"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.14272v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0030_Efficient_Aspect-Based_Summarization_of_Climate_Change_Reports_with_Small_Language_Models.pdf"
    },
    {
        "paper_id": 31,
        "title": "Towards Context-Rich Automated Biodiversity Assessments: Deriving\n  AI-Powered Insights from Camera Trap Data",
        "summary": "Camera traps offer enormous new opportunities in ecological studies, but\ncurrent automated image analysis methods often lack the contextual richness\nneeded to support impactful conservation outcomes. Here we present an\nintegrated approach that combines deep learning-based vision and language\nmodels to improve ecological reporting using data from camera traps. We\nintroduce a two-stage system: YOLOv10-X to localise and classify species\n(mammals and birds) within images, and a Phi-3.5-vision-instruct model to read\nYOLOv10-X binding box labels to identify species, overcoming its limitation\nwith hard to classify objects in images. Additionally, Phi-3.5 detects broader\nvariables, such as vegetation type, and time of day, providing rich ecological\nand environmental context to YOLO's species detection output. When combined,\nthis output is processed by the model's natural language system to answer\ncomplex queries, and retrieval-augmented generation (RAG) is employed to enrich\nresponses with external information, like species weight and IUCN status\n(information that cannot be obtained through direct visual analysis). This\ninformation is used to automatically generate structured reports, providing\nbiodiversity stakeholders with deeper insights into, for example, species\nabundance, distribution, animal behaviour, and habitat selection. Our approach\ndelivers contextually rich narratives that aid in wildlife management\ndecisions. By providing contextually rich insights, our approach not only\nreduces manual effort but also supports timely decision-making in conservation,\npotentially shifting efforts from reactive to proactive management.",
        "published": "2024-11-21T15:28:52Z",
        "link": "http://arxiv.org/abs/2411.14219v1",
        "authors": [
            "Paul Fergus",
            "Carl Chalmers",
            "Naomi Matthews",
            "Stuart Nixon",
            "Andre Burger",
            "Oliver Hartley",
            "Chris Sutherland",
            "Xavier Lambin",
            "Steven Longmore",
            "Serge Wich"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.14219v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0031_Towards_Context-Rich_Automated_Biodiversity_Assessments_Deriving_AI-Powered_Insights_from_Camera_Trap_Data.pdf"
    },
    {
        "paper_id": 32,
        "title": "RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented\n  Generation Applications with Agent-based Attacks",
        "summary": "While large language models (LLMs) have achieved notable success in\ngenerative tasks, they still face limitations, such as lacking up-to-date\nknowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)\nenhances LLM performance by integrating external knowledge bases, providing\nadditional context which significantly improves accuracy and knowledge\ncoverage. However, building these external knowledge bases often requires\nsubstantial resources and may involve sensitive information. In this paper, we\npropose an agent-based automated privacy attack called RAG-Thief, which can\nextract a scalable amount of private data from the private database used in RAG\napplications. We conduct a systematic study on the privacy risks associated\nwith RAG applications, revealing that the vulnerability of LLMs makes the\nprivate knowledge bases suffer significant privacy risks. Unlike previous\nmanual attacks which rely on traditional prompt injection techniques, RAG-Thief\nstarts with an initial adversarial query and learns from model responses,\nprogressively generating new queries to extract as many chunks from the\nknowledge base as possible. Experimental results show that our RAG-Thief can\nextract over 70% information from the private knowledge bases within customized\nRAG applications deployed on local machines and real-world platforms, including\nOpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy\nvulnerabilities in current RAG applications and underscore the pressing need\nfor stronger safeguards.",
        "published": "2024-11-21T13:18:03Z",
        "link": "http://arxiv.org/abs/2411.14110v1",
        "authors": [
            "Changyue Jiang",
            "Xudong Pan",
            "Geng Hong",
            "Chenfu Bao",
            "Min Yang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.14110v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0032_RAG-Thief_Scalable_Extraction_of_Private_Data_from_Retrieval-Augmented_Generation_Applications_with_Agent-based_Attacks.pdf"
    },
    {
        "paper_id": 33,
        "title": "FastRAG: Retrieval Augmented Generation for Semi-structured Data",
        "summary": "Efficiently processing and interpreting network data is critical for the\noperation of increasingly complex networks. Recent advances in Large Language\nModels (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved\ndata processing in network management. However, existing RAG methods like\nVectorRAG and GraphRAG struggle with the complexity and implicit nature of\nsemi-structured technical data, leading to inefficiencies in time, cost, and\nretrieval. This paper introduces FastRAG, a novel RAG approach designed for\nsemi-structured data. FastRAG employs schema learning and script learning to\nextract and structure data without needing to submit entire data sources to an\nLLM. It integrates text search with knowledge graph (KG) querying to improve\naccuracy in retrieving context-rich information. Evaluation results demonstrate\nthat FastRAG provides accurate question answering, while improving up to 90% in\ntime and 85% in cost compared to GraphRAG.",
        "published": "2024-11-21T01:00:25Z",
        "link": "http://arxiv.org/abs/2411.13773v1",
        "authors": [
            "Amar Abane",
            "Anis Bekri",
            "Abdella Battou"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.13773v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0033_FastRAG_Retrieval_Augmented_Generation_for_Semi-structured_Data.pdf"
    },
    {
        "paper_id": 34,
        "title": "Retrieval-Augmented Generation for Domain-Specific Question Answering: A\n  Case Study on Pittsburgh and CMU",
        "summary": "We designed a Retrieval-Augmented Generation (RAG) system to provide large\nlanguage models with relevant documents for answering domain-specific questions\nabout Pittsburgh and Carnegie Mellon University (CMU). We extracted over 1,800\nsubpages using a greedy scraping strategy and employed a hybrid annotation\nprocess, combining manual and Mistral-generated question-answer pairs,\nachieving an inter-annotator agreement (IAA) score of 0.7625. Our RAG framework\nintegrates BM25 and FAISS retrievers, enhanced with a reranker for improved\ndocument retrieval accuracy. Experimental results show that the RAG system\nsignificantly outperforms a non-RAG baseline, particularly in time-sensitive\nand complex queries, with an F1 score improvement from 5.45% to 42.21% and\nrecall of 56.18%. This study demonstrates the potential of RAG systems in\nenhancing answer precision and relevance, while identifying areas for further\noptimization in document retrieval and model training.",
        "published": "2024-11-20T20:10:43Z",
        "link": "http://arxiv.org/abs/2411.13691v1",
        "authors": [
            "Haojia Sun",
            "Yaqi Wang",
            "Shuting Zhang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.13691v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0034_Retrieval-Augmented_Generation_for_Domain-Specific_Question_Answering_A_Case_Study_on_Pittsburgh_and_CMU.pdf"
    },
    {
        "paper_id": 35,
        "title": "On the Way to LLM Personalization: Learning to Remember User\n  Conversations",
        "summary": "Large Language Models (LLMs) have quickly become an invaluable assistant for\na variety of tasks. However, their effectiveness is constrained by their\nability to tailor responses to human preferences and behaviors via\npersonalization. Prior work in LLM personalization has largely focused on style\ntransfer or incorporating small factoids about the user, as knowledge injection\nremains an open challenge. In this paper, we explore injecting knowledge of\nprior conversations into LLMs to enable future work on less redundant,\npersonalized conversations. We identify two real-world constraints: (1)\nconversations are sequential in time and must be treated as such during\ntraining, and (2) per-user personalization is only viable in\nparameter-efficient settings. To this aim, we propose PLUM, a pipeline\nperforming data augmentation for up-sampling conversations as question-answer\npairs, that are then used to finetune a low-rank adaptation adapter with a\nweighted cross entropy loss. Even in this first exploration of the problem, we\nperform competitively with baselines such as RAG, attaining an accuracy of\n81.5% across 100 conversations.",
        "published": "2024-11-20T15:45:08Z",
        "link": "http://arxiv.org/abs/2411.13405v1",
        "authors": [
            "Lucie Charlotte Magister",
            "Katherine Metcalf",
            "Yizhe Zhang",
            "Maartje ter Hoeve"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.13405v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0035_On_the_Way_to_LLM_Personalization_Learning_to_Remember_User_Conversations.pdf"
    },
    {
        "paper_id": 36,
        "title": "AIDBench: A benchmark for evaluating the authorship identification\n  capability of large language models",
        "summary": "As large language models (LLMs) rapidly advance and integrate into daily\nlife, the privacy risks they pose are attracting increasing attention. We focus\non a specific privacy risk where LLMs may help identify the authorship of\nanonymous texts, which challenges the effectiveness of anonymity in real-world\nsystems such as anonymous peer review systems. To investigate these risks, we\npresent AIDBench, a new benchmark that incorporates several author\nidentification datasets, including emails, blogs, reviews, articles, and\nresearch papers. AIDBench utilizes two evaluation methods: one-to-one\nauthorship identification, which determines whether two texts are from the same\nauthor; and one-to-many authorship identification, which, given a query text\nand a list of candidate texts, identifies the candidate most likely written by\nthe same author as the query text. We also introduce a Retrieval-Augmented\nGeneration (RAG)-based method to enhance the large-scale authorship\nidentification capabilities of LLMs, particularly when input lengths exceed the\nmodels' context windows, thereby establishing a new baseline for authorship\nidentification using LLMs. Our experiments with AIDBench demonstrate that LLMs\ncan correctly guess authorship at rates well above random chance, revealing new\nprivacy risks posed by these powerful models. The source code and data will be\nmade publicly available after acceptance.",
        "published": "2024-11-20T11:41:08Z",
        "link": "http://arxiv.org/abs/2411.13226v1",
        "authors": [
            "Zichen Wen",
            "Dadi Guo",
            "Huishuai Zhang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.13226v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0036_AIDBench_A_benchmark_for_evaluating_the_authorship_identification_capability_of_large_language_models.pdf"
    },
    {
        "paper_id": 37,
        "title": "Writing Style Matters: An Examination of Bias and Fairness in\n  Information Retrieval Systems",
        "summary": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models.",
        "published": "2024-11-20T10:17:09Z",
        "link": "http://arxiv.org/abs/2411.13173v1",
        "authors": [
            "Hongliu Cao"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.13173v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0037_Writing_Style_Matters_An_Examination_of_Bias_and_Fairness_in_Information_Retrieval_Systems.pdf"
    },
    {
        "paper_id": 38,
        "title": "Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding",
        "summary": "The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development.",
        "published": "2024-11-20T09:59:12Z",
        "link": "http://arxiv.org/abs/2411.13163v1",
        "authors": [
            "Nabeel Seedat",
            "Caterina Tozzi",
            "Andrea Hita Ardiaca",
            "Mihaela van der Schaar",
            "James Weatherall",
            "Adam Taylor"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.13163v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0038_Unlocking_Historical_Clinical_Trial_Data_with_ALIGN_A_Compositional_Large_Language_Model_System_for_Medical_Coding.pdf"
    },
    {
        "paper_id": 39,
        "title": "DMQR-RAG: Diverse Multi-Query Rewriting for RAG",
        "summary": "Large language models often encounter challenges with static knowledge and\nhallucinations, which undermine their reliability. Retrieval-augmented\ngeneration (RAG) mitigates these issues by incorporating external information.\nHowever, user queries frequently contain noise and intent deviations,\nnecessitating query rewriting to improve the relevance of retrieved documents.\nIn this paper, we introduce DMQR-RAG, a Diverse Multi-Query Rewriting framework\ndesigned to improve the performance of both document retrieval and final\nresponses in RAG. Specifically, we investigate how queries with varying\ninformation quantities can retrieve a diverse array of documents, presenting\nfour rewriting strategies that operate at different levels of information to\nenhance the performance of baseline approaches. Additionally, we propose an\nadaptive strategy selection method that minimizes the number of rewrites while\noptimizing overall performance. Our methods have been rigorously validated\nthrough extensive experiments conducted in both academic and industry settings.",
        "published": "2024-11-20T09:43:30Z",
        "link": "http://arxiv.org/abs/2411.13154v1",
        "authors": [
            "Zhicong Li",
            "Jiahao Wang",
            "Zhishu Jiang",
            "Hangyu Mao",
            "Zhongxia Chen",
            "Jiazhen Du",
            "Yuanxing Zhang",
            "Fuzheng Zhang",
            "Di Zhang",
            "Yong Liu"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.13154v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0039_DMQR-RAG_Diverse_Multi-Query_Rewriting_for_RAG.pdf"
    },
    {
        "paper_id": 40,
        "title": "Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension",
        "summary": "Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.",
        "published": "2024-11-20T07:44:34Z",
        "link": "http://arxiv.org/abs/2411.13093v1",
        "authors": [
            "Yongdong Luo",
            "Xiawu Zheng",
            "Xiao Yang",
            "Guilin Li",
            "Haojia Lin",
            "Jinfa Huang",
            "Jiayi Ji",
            "Fei Chao",
            "Jiebo Luo",
            "Rongrong Ji"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.13093v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0040_Video-RAG_Visually-aligned_Retrieval-Augmented_Long_Video_Comprehension.pdf"
    },
    {
        "paper_id": 41,
        "title": "Multimodal large language model for wheat breeding: a new exploration of\n  smart breeding",
        "summary": "UAV remote sensing technology has become a key technology in crop breeding,\nwhich can achieve high-throughput and non-destructive collection of crop\nphenotyping data. However, the multidisciplinary nature of breeding has brought\ntechnical barriers and efficiency challenges to knowledge mining. Therefore, it\nis important to develop a smart breeding goal tool to mine cross-domain\nmultimodal data. Based on different pre-trained open-source multimodal large\nlanguage models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used\nsupervised fine-tuning (SFT), retrieval-augmented generation (RAG), and\nreinforcement learning from human feedback (RLHF) technologies to inject\ncross-domain knowledge into MLLMs, thereby constructing multiple multimodal\nlarge language models for wheat breeding (WBLMs). The above WBLMs were\nevaluated using the newly created evaluation benchmark in this study. The\nresults showed that the WBLM constructed using SFT, RAG and RLHF technologies\nand InternVL2-8B has leading performance. Then, subsequent experiments were\nconducted using the WBLM. Ablation experiments indicated that the combination\nof SFT, RAG, and RLHF technologies can improve the overall generation\nperformance, enhance the generated quality, balance the timeliness and\nadaptability of the generated answer, and reduce hallucinations and biases. The\nWBLM performed best in wheat yield prediction using cross-domain data (remote\nsensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of\n0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate\nprofessional decision support answers for phenotyping estimation, environmental\nstress assessment, target germplasm screening, cultivation technique\nrecommendation, and seed price query tasks.",
        "published": "2024-11-20T04:47:42Z",
        "link": "http://arxiv.org/abs/2411.15203v1",
        "authors": [
            "Guofeng Yang",
            "Yu Li",
            "Yong He",
            "Zhenjiang Zhou",
            "Lingzhen Ye",
            "Hui Fang",
            "Yiqi Luo",
            "Xuping Feng"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.15203v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0041_Multimodal_large_language_model_for_wheat_breeding_a_new_exploration_of_smart_breeding.pdf"
    },
    {
        "paper_id": 42,
        "title": "CodeXEmbed: A Generalist Embedding Model Family for Multiligual and\n  Multi-task Code Retrieval",
        "summary": "Despite the success of text retrieval in many NLP tasks, code retrieval\nremains a largely underexplored area. Most text retrieval systems are tailored\nfor natural language queries, often neglecting the specific challenges of\nretrieving code. This gap leaves existing models unable to effectively capture\nthe diversity of programming languages and tasks across different domains,\nhighlighting the need for more focused research in code retrieval. To address\nthis, we introduce CodeXEmbed, a family of large-scale code embedding models\nranging from 400M to 7B parameters. Our novel training pipeline unifies\nmultiple programming languages and transforms various code-related tasks into a\ncommon retrieval framework, enhancing model generalizability and retrieval\nperformance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval,\noutperforming the previous leading model, Voyage-Code, by over 20% on CoIR\nbenchmark. In addition to excelling in code retrieval, our models demonstrate\ncompetitive performance on the widely adopted BeIR text retrieval benchmark,\noffering versatility across domains. Experimental results demonstrate that\nimproving retrieval performance significantly enhances end-to-end\nRetrieval-Augmented Generation (RAG) performance for code-related tasks.",
        "published": "2024-11-19T16:54:45Z",
        "link": "http://arxiv.org/abs/2411.12644v2",
        "authors": [
            "Ye Liu",
            "Rui Meng",
            "Shafiq Joty",
            "Silvio Savarese",
            "Caiming Xiong",
            "Yingbo Zhou",
            "Semih Yavuz"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.12644v2.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0042_CodeXEmbed_A_Generalist_Embedding_Model_Family_for_Multiligual_and_Multi-task_Code_Retrieval.pdf"
    },
    {
        "paper_id": 43,
        "title": "CUE-M: Contextual Understanding and Enhanced Search with Multimodal\n  Large Language Model",
        "summary": "The integration of Retrieval-Augmented Generation (RAG) with Multimodal Large\nLanguage Models (MLLMs) has expanded the scope of multimodal query resolution.\nHowever, current systems struggle with intent understanding, information\nretrieval, and safety filtering, limiting their effectiveness. This paper\nintroduces Contextual Understanding and Enhanced Search with MLLM (CUE-M), a\nnovel multimodal search pipeline that addresses these challenges through a\nmulti-stage framework comprising image context enrichment, intent refinement,\ncontextual query generation, external API integration, and relevance-based\nfiltering. CUE-M incorporates a robust safety framework combining image-based,\ntext-based, and multimodal classifiers, dynamically adapting to instance- and\ncategory-specific risks. Evaluations on a multimodal Q&A dataset and a public\nsafety benchmark demonstrate that CUE-M outperforms baselines in accuracy,\nknowledge integration, and safety, advancing the capabilities of multimodal\nretrieval systems.",
        "published": "2024-11-19T07:16:48Z",
        "link": "http://arxiv.org/abs/2411.12287v1",
        "authors": [
            "Dongyoung Go",
            "Taesun Whang",
            "Chanhee Lee",
            "Hwayeon Kim",
            "Sunghoon Park",
            "Seunghwan Ji",
            "Dongchan Kim",
            "Young-Bum Kim"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.12287v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0043_CUE-M_Contextual_Understanding_and_Enhanced_Search_with_Multimodal_Large_Language_Model.pdf"
    },
    {
        "paper_id": 44,
        "title": "Large Language Models for Material Property Predictions: elastic\n  constant tensor prediction and materials design",
        "summary": "Efficient and accurate prediction of material properties is critical for\nadvancing materials design and applications. The rapid-evolution of large\nlanguage models (LLMs) presents a new opportunity for material property\npredictions, complementing experimental measurements and multi-scale\ncomputational methods. We focus on predicting the elastic constant tensor, as a\ncase study, and develop domain-specific LLMs for predicting elastic constants\nand for materials discovery. The proposed ElaTBot LLM enables simultaneous\nprediction of elastic constant tensors, bulk modulus at finite temperatures,\nand the generation of new materials with targeted properties. Moreover, the\ncapabilities of ElaTBot are further enhanced by integrating with general LLMs\n(GPT-4o) and Retrieval-Augmented Generation (RAG) for prediction. A specialized\nvariant, ElaTBot-DFT, designed for 0 K elastic constant tensor prediction,\nreduces the prediction errors by 33.1% compared with domain-specific, material\nscience LLMs (Darwin) trained on the same dataset. This natural language-based\napproach lowers the barriers to computational materials science and highlights\nthe broader potential of LLMs for material property predictions and inverse\ndesign.",
        "published": "2024-11-19T07:03:19Z",
        "link": "http://arxiv.org/abs/2411.12280v1",
        "authors": [
            "Siyu Liu",
            "Tongqi Wen",
            "Beilin Ye",
            "Zhuoyuan Li",
            "David J. Srolovitz"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.12280v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0044_Large_Language_Models_for_Material_Property_Predictions_elastic_constant_tensor_prediction_and_materials_design.pdf"
    },
    {
        "paper_id": 45,
        "title": "SayComply: Grounding Field Robotic Tasks in Operational Compliance\n  through Retrieval-Based Language Models",
        "summary": "This paper addresses the problem of task planning for robots that must comply\nwith operational manuals in real-world settings. Task planning under these\nconstraints is essential for enabling autonomous robot operation in domains\nthat require adherence to domain-specific knowledge. Current methods for\ngenerating robot goals and plans rely on common sense knowledge encoded in\nlarge language models. However, these models lack grounding of robot plans to\ndomain-specific knowledge and are not easily transferable between multiple\nsites or customers with different compliance needs. In this work, we present\nSayComply, which enables grounding robotic task planning with operational\ncompliance using retrieval-based language models. We design a hierarchical\ndatabase of operational, environment, and robot embodiment manuals and\nprocedures to enable efficient retrieval of the relevant context under the\nlimited context length of the LLMs. We then design a task planner using a\ntree-based retrieval augmented generation (RAG) technique to generate robot\ntasks that follow user instructions while simultaneously complying with the\ndomain knowledge in the database. We demonstrate the benefits of our approach\nthrough simulations and hardware experiments in real-world scenarios that\nrequire precise context retrieval across various types of context,\noutperforming the standard RAG method. Our approach bridges the gap in\ndeploying robots that consistently adhere to operational protocols, offering a\nscalable and edge-deployable solution for ensuring compliance across varied and\ncomplex real-world environments. Project website: saycomply.github.io.",
        "published": "2024-11-18T06:33:05Z",
        "link": "http://arxiv.org/abs/2411.11323v1",
        "authors": [
            "Muhammad Fadhil Ginting",
            "Dong-Ki Kim",
            "Sung-Kyun Kim",
            "Bandi Jai Krishna",
            "Mykel J. Kochenderfer",
            "Shayegan Omidshafiei",
            "Ali-akbar Agha-mohammadi"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.11323v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0045_SayComply_Grounding_Field_Robotic_Tasks_in_Operational_Compliance_through_Retrieval-Based_Language_Models.pdf"
    },
    {
        "paper_id": 46,
        "title": "On-Board Vision-Language Models for Personalized Autonomous Vehicle\n  Motion Control: System Design and Real-World Validation",
        "summary": "Personalized driving refers to an autonomous vehicle's ability to adapt its\ndriving behavior or control strategies to match individual users' preferences\nand driving styles while maintaining safety and comfort standards. However,\nexisting works either fail to capture every individual preference precisely or\nbecome computationally inefficient as the user base expands. Vision-Language\nModels (VLMs) offer promising solutions to this front through their natural\nlanguage understanding and scene reasoning capabilities. In this work, we\npropose a lightweight yet effective on-board VLM framework that provides\nlow-latency personalized driving performance while maintaining strong reasoning\ncapabilities. Our solution incorporates a Retrieval-Augmented Generation\n(RAG)-based memory module that enables continuous learning of individual\ndriving preferences through human feedback. Through comprehensive real-world\nvehicle deployment and experiments, our system has demonstrated the ability to\nprovide safe, comfortable, and personalized driving experiences across various\nscenarios and significantly reduce takeover rates by up to 76.9%. To the best\nof our knowledge, this work represents the first end-to-end VLM-based motion\ncontrol system in real-world autonomous vehicles.",
        "published": "2024-11-17T23:20:37Z",
        "link": "http://arxiv.org/abs/2411.11913v1",
        "authors": [
            "Can Cui",
            "Zichong Yang",
            "Yupeng Zhou",
            "Juntong Peng",
            "Sung-Yeon Park",
            "Cong Zhang",
            "Yunsheng Ma",
            "Xu Cao",
            "Wenqian Ye",
            "Yiheng Feng",
            "Jitesh Panchal",
            "Lingxi Li",
            "Yaobin Chen",
            "Ziran Wang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.11913v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0046_On-Board_Vision-Language_Models_for_Personalized_Autonomous_Vehicle_Motion_Control_System_Design_and_Real-World_Validation.pdf"
    },
    {
        "paper_id": 47,
        "title": "LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems\n  Anomaly Detection",
        "summary": "Modern industrial infrastructures rely heavily on Cyber-Physical Systems\n(CPS), but these are vulnerable to cyber-attacks with potentially catastrophic\neffects. To reduce these risks, anomaly detection methods based on physical\ninvariants have been developed. However, these methods often require\ndomain-specific expertise to manually define invariants, making them costly and\ndifficult to scale. To address this limitation, we propose a novel approach to\nextract physical invariants from CPS testbeds for anomaly detection. Our\ninsight is that CPS design documentation often contains semantically rich\ndescriptions of physical procedures, which can profile inter-correlated\ndynamics among system components. Leveraging the built-in physics and\nengineering knowledge of recent generative AI models, we aim to automate this\ntraditionally manual process, improving scalability and reducing costs. This\nwork focuses on designing and optimizing a Retrieval-Augmented-Generation (RAG)\nworkflow with a customized prompting system tailored for CPS documentation,\nenabling accurate extraction of semantic information and inference of physical\ninvariants from complex, multimodal content. Then, rather than directly\napplying the inferred invariants for anomaly detection, we introduce an\ninnovative statistics-based learning approach that integrates these invariants\ninto the training dataset. This method addresses limitations such as\nhallucination and concept drift, enhancing the reliability of the model. We\nevaluate our approach on real-world public CPS security dataset which contains\n86 data points and 58 attacking cases. The results show that our approach\nachieves a high precision of 0.923, accurately detecting anomalies while\nminimizing false alarms.",
        "published": "2024-11-17T00:09:04Z",
        "link": "http://arxiv.org/abs/2411.10918v1",
        "authors": [
            "Danial Abshari",
            "Chenglong Fu",
            "Meera Sridhar"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.10918v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0047_LLM-assisted_Physical_Invariant_Extraction_for_Cyber-Physical_Systems_Anomaly_Detection.pdf"
    },
    {
        "paper_id": 48,
        "title": "Empowering Meta-Analysis: Leveraging Large Language Models for\n  Scientific Synthesis",
        "summary": "This study investigates the automation of meta-analysis in scientific\ndocuments using large language models (LLMs). Meta-analysis is a robust\nstatistical method that synthesizes the findings of multiple studies support\narticles to provide a comprehensive understanding. We know that a meta-article\nprovides a structured analysis of several articles. However, conducting\nmeta-analysis by hand is labor-intensive, time-consuming, and susceptible to\nhuman error, highlighting the need for automated pipelines to streamline the\nprocess. Our research introduces a novel approach that fine-tunes the LLM on\nextensive scientific datasets to address challenges in big data handling and\nstructured data extraction. We automate and optimize the meta-analysis process\nby integrating Retrieval Augmented Generation (RAG). Tailored through prompt\nengineering and a new loss metric, Inverse Cosine Distance (ICD), designed for\nfine-tuning on large contextual datasets, LLMs efficiently generate structured\nmeta-analysis content. Human evaluation then assesses relevance and provides\ninformation on model performance in key metrics. This research demonstrates\nthat fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs\ngenerating 87.6% relevant meta-analysis abstracts. The relevance of the\ncontext, based on human evaluation, shows a reduction in irrelevancy from 4.56%\nto 1.9%. These experiments were conducted in a low-resource environment,\nhighlighting the study's contribution to enhancing the efficiency and\nreliability of meta-analysis automation.",
        "published": "2024-11-16T20:18:57Z",
        "link": "http://arxiv.org/abs/2411.10878v1",
        "authors": [
            "Jawad Ibn Ahad",
            "Rafeed Mohammad Sultan",
            "Abraham Kaikobad",
            "Fuad Rahman",
            "Mohammad Ruhul Amin",
            "Nabeel Mohammed",
            "Shafin Rahman"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.10878v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0048_Empowering_Meta-Analysis_Leveraging_Large_Language_Models_for_Scientific_Synthesis.pdf"
    },
    {
        "paper_id": 49,
        "title": "A Novel Approach to Eliminating Hallucinations in Large Language\n  Model-Assisted Causal Discovery",
        "summary": "The increasing use of large language models (LLMs) in causal discovery as a\nsubstitute for human domain experts highlights the need for optimal model\nselection. This paper presents the first hallucination survey of popular LLMs\nfor causal discovery. We show that hallucinations exist when using LLMs in\ncausal discovery so the choice of LLM is important. We propose using Retrieval\nAugmented Generation (RAG) to reduce hallucinations when quality data is\navailable. Additionally, we introduce a novel method employing multiple LLMs\nwith an arbiter in a debate to audit edges in causal graphs, achieving a\ncomparable reduction in hallucinations to RAG.",
        "published": "2024-11-16T03:06:39Z",
        "link": "http://arxiv.org/abs/2411.12759v1",
        "authors": [
            "Grace Sng",
            "Yanming Zhang",
            "Klaus Mueller"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.12759v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0049_A_Novel_Approach_to_Eliminating_Hallucinations_in_Large_Language_Model-Assisted_Causal_Discovery.pdf"
    },
    {
        "paper_id": 50,
        "title": "Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the\n  AutoNuggetizer Framework",
        "summary": "This report provides an initial look at partial results from the TREC 2024\nRetrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation\nas a barrier to continued progress in information access (and more broadly,\nnatural language processing and artificial intelligence), and it is our hope\nthat we can contribute to tackling the many challenges in this space. The\ncentral hypothesis we explore in this work is that the nugget evaluation\nmethodology, originally developed for the TREC Question Answering Track in\n2003, provides a solid foundation for evaluating RAG systems. As such, our\nefforts have focused on \"refactoring\" this methodology, specifically applying\nlarge language models to both automatically create nuggets and to automatically\nassign nuggets to system answers. We call this the AutoNuggetizer framework.\nWithin the TREC setup, we are able to calibrate our fully automatic process\nagainst a manual process whereby nuggets are created by human assessors\nsemi-manually and then assigned manually to system answers. Based on initial\nresults across 21 topics from 45 runs, we observe a strong correlation between\nscores derived from a fully automatic nugget evaluation and a (mostly) manual\nnugget evaluation by human assessors. This suggests that our fully automatic\nevaluation process can be used to guide future iterations of RAG systems.",
        "published": "2024-11-14T17:25:43Z",
        "link": "http://arxiv.org/abs/2411.09607v1",
        "authors": [
            "Ronak Pradeep",
            "Nandan Thakur",
            "Shivani Upadhyay",
            "Daniel Campos",
            "Nick Craswell",
            "Jimmy Lin"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.09607v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0050_Initial_Nugget_Evaluation_Results_for_the_TREC_2024_RAG_Track_with_the_AutoNuggetizer_Framework.pdf"
    },
    {
        "paper_id": 51,
        "title": "Adopting RAG for LLM-Aided Future Vehicle Design",
        "summary": "In this paper, we explore the integration of Large Language Models (LLMs)\nwith Retrieval-Augmented Generation (RAG) to enhance automated design and\nsoftware development in the automotive industry. We present two case studies: a\nstandardization compliance chatbot and a design copilot, both utilizing RAG to\nprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,\nLLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and\nexecution time. Our results demonstrate that while GPT-4 offers superior\nperformance, LLAMA3 and Mistral also show promising capabilities for local\ndeployment, addressing data privacy concerns in automotive applications. This\nstudy highlights the potential of RAG-augmented LLMs in improving design\nworkflows and compliance in automotive engineering.",
        "published": "2024-11-14T17:01:24Z",
        "link": "http://arxiv.org/abs/2411.09590v1",
        "authors": [
            "Vahid Zolfaghari",
            "Nenad Petrovic",
            "Fengjunjie Pan",
            "Krzysztof Lebioda",
            "Alois Knoll"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.09590v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0051_Adopting_RAG_for_LLM-Aided_Future_Vehicle_Design.pdf"
    },
    {
        "paper_id": 52,
        "title": "Harnessing multiple LLMs for Information Retrieval: A case study on Deep\n  Learning methodologies in Biodiversity publications",
        "summary": "Deep Learning (DL) techniques are increasingly applied in scientific studies\nacross various domains to address complex research questions. However, the\nmethodological details of these DL models are often hidden in the unstructured\ntext. As a result, critical information about how these models are designed,\ntrained, and evaluated is challenging to access and comprehend. To address this\nissue, in this work, we use five different open-source Large Language Models\n(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,\nand Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)\napproach to extract and process DL methodological details from scientific\npublications automatically. We built a voting classifier from the outputs of\nfive LLMs to accurately report DL methodological information. We tested our\napproach using biodiversity publications, building upon our previous research.\nTo validate our pipeline, we employed two datasets of DL-related biodiversity\npublications: a curated set of 100 publications from our prior work and a set\nof 364 publications from the Ecological Informatics journal. Our results\ndemonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of\nDL methodological information, achieving an accuracy of 69.5% (417 out of 600\ncomparisons) based solely on textual content from publications. This\nperformance was assessed against human annotators who had access to code,\nfigures, tables, and other supplementary information. Although demonstrated in\nbiodiversity, our methodology is not limited to this field; it can be applied\nacross other scientific domains where detailed methodological reporting is\nessential for advancing knowledge and ensuring reproducibility. This study\npresents a scalable and reliable approach for automating information\nextraction, facilitating better reproducibility and knowledge transfer across\nstudies.",
        "published": "2024-11-14T08:12:36Z",
        "link": "http://arxiv.org/abs/2411.09269v1",
        "authors": [
            "Vamsi Krishna Kommineni",
            "Birgitta König-Ries",
            "Sheeba Samuel"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.09269v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0052_Harnessing_multiple_LLMs_for_Information_Retrieval_A_case_study_on_Deep_Learning_methodologies_in_Biodiversity_publications.pdf"
    },
    {
        "paper_id": 53,
        "title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation\n  Systems for Medical Question Answering",
        "summary": "Retrieval-augmented generation (RAG) has emerged as a promising approach to\nenhance the performance of large language models (LLMs) in knowledge-intensive\ntasks such as those from medical domain. However, the sensitive nature of the\nmedical domain necessitates a completely accurate and trustworthy system. While\nexisting RAG benchmarks primarily focus on the standard retrieve-answer\nsetting, they overlook many practical scenarios that measure crucial aspects of\na reliable medical system. This paper addresses this gap by providing a\ncomprehensive evaluation framework for medical question-answering (QA) systems\nin a RAG setting for these situations, including sufficiency, integration, and\nrobustness. We introduce Medical Retrieval-Augmented Generation Benchmark\n(MedRGB) that provides various supplementary elements to four medical QA\ndatasets for testing LLMs' ability to handle these specific scenarios.\nUtilizing MedRGB, we conduct extensive evaluations of both state-of-the-art\ncommercial LLMs and open-source models across multiple retrieval conditions.\nOur experimental results reveals current models' limited ability to handle\nnoise and misinformation in the retrieved documents. We further analyze the\nLLMs' reasoning processes to provides valuable insights and future directions\nfor developing RAG systems in this critical medical domain.",
        "published": "2024-11-14T06:19:18Z",
        "link": "http://arxiv.org/abs/2411.09213v1",
        "authors": [
            "Nghia Trung Ngo",
            "Chien Van Nguyen",
            "Franck Dernoncourt",
            "Thien Huu Nguyen"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.09213v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0053_Comprehensive_and_Practical_Evaluation_of_Retrieval-Augmented_Generation_Systems_for_Medical_Question_Answering.pdf"
    },
    {
        "paper_id": 54,
        "title": "Practitioners' Discussions on Building LLM-based Applications for\n  Production",
        "summary": "\\textit{Background}: Large language models (LLMs) have become a paramount\ninterest of researchers and practitioners alike, yet a comprehensive overview\nof key considerations for those developing LLM-based systems is lacking. This\nstudy addresses this gap by collecting and mapping the topics practitioners\ndiscuss online, offering practical insights into where priorities lie in\ndeveloping LLM-based applications. \\textit{Method}: We collected 189 videos\nfrom 2022 to 2024 from practitioners actively developing such systems and\ndiscussing various aspects they encounter during development and deployment of\nLLMs in production. We analyzed the transcripts using BERTopic, then manually\nsorted and merged the generated topics into themes, leading to a total of 20\ntopics in 8 themes. \\textit{Results}: The most prevalent topics fall within the\ntheme Design \\& Architecture, with a strong focus on retrieval-augmented\ngeneration (RAG) systems. Other frequently discussed topics include model\ncapabilities and enhancement techniques (e.g., fine-tuning, prompt\nengineering), infrastructure and tooling, and risks and ethical challenges.\n\\textit{Implications}: Our results highlight current discussions and challenges\nin deploying LLMs in production. This way, we provide a systematic overview of\nkey aspects practitioners should be aware of when developing LLM-based\napplications. We further pale off topics of interest for academics where\nfurther research is needed.",
        "published": "2024-11-13T12:44:41Z",
        "link": "http://arxiv.org/abs/2411.08574v1",
        "authors": [
            "Alina Mailach",
            "Sebastian Simon",
            "Johannes Dorn",
            "Norbert Siegmund"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.08574v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0054_Practitioners_Discussions_on_Building_LLM-based_Applications_for_Production.pdf"
    },
    {
        "paper_id": 55,
        "title": "Building Trustworthy AI: Transparent AI Systems via Large Language\n  Models, Ontologies, and Logical Reasoning (TranspNet)",
        "summary": "Growing concerns over the lack of transparency in AI, particularly in\nhigh-stakes fields like healthcare and finance, drive the need for explainable\nand trustworthy systems. While Large Language Models (LLMs) perform\nexceptionally well in generating accurate outputs, their \"black box\" nature\nposes significant challenges to transparency and trust. To address this, the\npaper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.\nBy leveraging domain expert knowledge, retrieval-augmented generation (RAG),\nand formal reasoning frameworks like Answer Set Programming (ASP), TranspNet\nenhances LLM outputs with structured reasoning and verification. This approach\nensures that AI systems deliver not only accurate but also explainable and\ntrustworthy results, meeting regulatory demands for transparency and\naccountability. TranspNet provides a comprehensive solution for developing AI\nsystems that are reliable and interpretable, making it suitable for real-world\napplications where trust is critical.",
        "published": "2024-11-13T09:40:37Z",
        "link": "http://arxiv.org/abs/2411.08469v1",
        "authors": [
            "Fadi Al Machot",
            "Martin Thomas Horsch",
            "Habib Ullah"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.08469v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0055_Building_Trustworthy_AI_Transparent_AI_Systems_via_Large_Language_Models_Ontologies_and_Logical_Reasoning_TranspNet.pdf"
    },
    {
        "paper_id": 56,
        "title": "Towards Evaluating Large Language Models for Graph Query Generation",
        "summary": "Large Language Models (LLMs) are revolutionizing the landscape of Generative\nArtificial Intelligence (GenAI), with innovative LLM-backed solutions emerging\nrapidly. However, when applied to database technologies, specifically query\ngeneration for graph databases and Knowledge Graphs (KGs), LLMs still face\nsignificant challenges. While research on LLM-driven query generation for\nStructured Query Language (SQL) exists, similar systems for graph databases\nremain underdeveloped. This paper presents a comparative study addressing the\nchallenge of generating Cypher queries a powerful language for interacting with\ngraph databases using open-access LLMs. We rigorously evaluate several LLM\nagents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a\nlocally deployed Llama 3.1 8B) using a designed few-shot learning prompt and\nRetrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)\nreasoning. Our empirical analysis of query generation accuracy reveals that\nClaude Sonnet 3.5 outperforms its counterparts in this specific domain.\nFurther, we highlight promising future research directions to address the\nidentified limitations and advance LLM-driven query generation for graph\ndatabases.",
        "published": "2024-11-13T09:11:56Z",
        "link": "http://arxiv.org/abs/2411.08449v2",
        "authors": [
            "Siraj Munir",
            "Alessandro Aldini"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.08449v2.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0056_Towards_Evaluating_Large_Language_Models_for_Graph_Query_Generation.pdf"
    },
    {
        "paper_id": 57,
        "title": "Towards Optimizing a Retrieval Augmented Generation using Large Language\n  Model on Academic Data",
        "summary": "Given the growing trend of many organizations integrating Retrieval Augmented\nGeneration (RAG) into their operations, we assess RAG on domain-specific data\nand test state-of-the-art models across various optimization techniques. We\nincorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble\nRetriever, and In-Context-Learning, to enhance the functionality and\nperformance in the academic domain. We focus on data retrieval, specifically\ntargeting various study programs at a large technical university. We\nadditionally introduce a novel evaluation approach, the RAG Confusion Matrix\ndesigned to assess the effectiveness of various configurations within the RAG\nframework. By exploring the integration of both open-source (e.g., Llama2,\nMistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer\nvaluable insights into the application and optimization of RAG frameworks in\ndomain-specific contexts. Our experiments show a significant performance\nincrease when including multi-query in the retrieval phase.",
        "published": "2024-11-13T08:43:37Z",
        "link": "http://arxiv.org/abs/2411.08438v1",
        "authors": [
            "Anum Afzal",
            "Juraj Vladika",
            "Gentrit Fazlija",
            "Andrei Staradubets",
            "Florian Matthes"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.08438v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0057_Towards_Optimizing_a_Retrieval_Augmented_Generation_using_Large_Language_Model_on_Academic_Data.pdf"
    },
    {
        "paper_id": 58,
        "title": "Refining Translations with LLMs: A Constraint-Aware Iterative Prompting\n  Approach",
        "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmachine translation (MT), even without specific training on the languages in\nquestion. However, translating rare words in low-resource or domain-specific\ncontexts remains challenging for LLMs. To address this issue, we propose a\nmulti-step prompt chain that enhances translation faithfulness by prioritizing\nkey terms crucial for semantic accuracy. Our method first identifies these\nkeywords and retrieves their translations from a bilingual dictionary,\nintegrating them into the LLM's context using Retrieval-Augmented Generation\n(RAG). We further mitigate potential output hallucinations caused by long\nprompts through an iterative self-checking mechanism, where the LLM refines its\ntranslations based on lexical and semantic constraints. Experiments using Llama\nand Qwen as base models on the FLORES-200 and WMT datasets demonstrate\nsignificant improvements over baselines, highlighting the effectiveness of our\napproach in enhancing translation faithfulness and robustness, particularly in\nlow-resource scenarios.",
        "published": "2024-11-13T05:40:24Z",
        "link": "http://arxiv.org/abs/2411.08348v1",
        "authors": [
            "Shangfeng Chen",
            "Xiayang Shi",
            "Pu Li",
            "Yinlin Li",
            "Jingjing Liu"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.08348v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0058_Refining_Translations_with_LLMs_A_Constraint-Aware_Iterative_Prompting_Approach.pdf"
    },
    {
        "paper_id": 59,
        "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the\n  Oracle",
        "summary": "Many existing evaluation benchmarks for Large Language Models (LLMs) quickly\nbecome outdated due to the emergence of new models and training data. These\nbenchmarks also fall short in assessing how LLM performance changes over time,\nas they consist of static questions without a temporal dimension. To address\nthese limitations, we propose using future event prediction as a continuous\nevaluation method to assess LLMs' temporal generalization and forecasting\nabilities. Our benchmark, Daily Oracle, automatically generates question-answer\n(QA) pairs from daily news, challenging LLMs to predict \"future\" event\noutcomes. Our findings reveal that as pre-training data becomes outdated, LLM\nperformance degrades over time. While Retrieval Augmented Generation (RAG) has\nthe potential to enhance prediction accuracy, the performance degradation\npattern persists, highlighting the need for continuous model updates.",
        "published": "2024-11-13T04:20:20Z",
        "link": "http://arxiv.org/abs/2411.08324v1",
        "authors": [
            "Hui Dai",
            "Ryan Teehan",
            "Mengye Ren"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.08324v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0059_Are_LLMs_Prescient_A_Continuous_Evaluation_using_Daily_News_as_the_Oracle.pdf"
    },
    {
        "paper_id": 60,
        "title": "A Large-Scale Study of Relevance Assessments with Large Language Models:\n  An Initial Look",
        "summary": "The application of large language models to provide relevance assessments\npresents exciting opportunities to advance information retrieval, natural\nlanguage processing, and beyond, but to date many unknowns remain. This paper\nreports on the results of a large-scale evaluation (the TREC 2024 RAG Track)\nwhere four different relevance assessment approaches were deployed in situ: the\n\"standard\" fully manual process that NIST has implemented for decades and three\ndifferent alternatives that take advantage of LLMs to different extents using\nthe open-source UMBRELA tool. This setup allows us to correlate system rankings\ninduced by the different approaches to characterize tradeoffs between cost and\nquality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system\nrankings induced by automatically generated relevance assessments from UMBRELA\ncorrelate highly with those induced by fully manual assessments across a\ndiverse set of 77 runs from 19 teams. Our results suggest that automatically\ngenerated UMBRELA judgments can replace fully manual judgments to accurately\ncapture run-level effectiveness. Surprisingly, we find that LLM assistance does\nnot appear to increase correlation with fully manual assessments, suggesting\nthat costs associated with human-in-the-loop processes do not bring obvious\ntangible benefits. Overall, human assessors appear to be stricter than UMBRELA\nin applying relevance criteria. Our work validates the use of LLMs in academic\nTREC-style evaluations and provides the foundation for future studies.",
        "published": "2024-11-13T01:12:35Z",
        "link": "http://arxiv.org/abs/2411.08275v1",
        "authors": [
            "Shivani Upadhyay",
            "Ronak Pradeep",
            "Nandan Thakur",
            "Daniel Campos",
            "Nick Craswell",
            "Ian Soboroff",
            "Hoa Trang Dang",
            "Jimmy Lin"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.08275v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0060_A_Large-Scale_Study_of_Relevance_Assessments_with_Large_Language_Models_An_Initial_Look.pdf"
    },
    {
        "paper_id": 61,
        "title": "Retrieval Augmented Time Series Forecasting",
        "summary": "Retrieval-augmented generation (RAG) is a central component of modern LLM\nsystems, particularly in scenarios where up-to-date information is crucial for\naccurately responding to user queries or when queries exceed the scope of the\ntraining data. The advent of time-series foundation models (TSFM), such as\nChronos, and the need for effective zero-shot forecasting performance across\nvarious time-series domains motivates the question: Do benefits of RAG\nsimilarly carry over to time series forecasting? In this paper, we advocate\nthat the dynamic and event-driven nature of time-series data makes RAG a\ncrucial component of TSFMs and introduce a principled RAG framework for\ntime-series forecasting, called Retrieval Augmented Forecasting (RAF). Within\nRAF, we develop efficient strategies for retrieving related time-series\nexamples and incorporating them into forecast. Through experiments and\nmechanistic studies, we demonstrate that RAF indeed improves the forecasting\naccuracy across diverse time series domains and the improvement is more\nsignificant for larger TSFM sizes.",
        "published": "2024-11-12T23:55:11Z",
        "link": "http://arxiv.org/abs/2411.08249v1",
        "authors": [
            "Kutay Tire",
            "Ege Onur Taga",
            "Muhammed Emrullah Ildiz",
            "Samet Oymak"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.08249v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0061_Retrieval_Augmented_Time_Series_Forecasting.pdf"
    },
    {
        "paper_id": 62,
        "title": "Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\n  Bases and Dual Decoders",
        "summary": "Although people are impressed by the content generation skills of large\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\ngrounding of the content. The correctness and groundedness of the generated\ncontent need to be based on a verified context, such as results from\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\na customized domain is that the generated responses are often incomplete, or\nthe additions are not verified and may even be hallucinated. Prior studies on\nhallucination detection have focused on evaluation metrics, which are not\neasily adaptable to dynamic domains and can be vulnerable to attacks like\njail-breaking. In this work, we propose 1) a post-processing algorithm that\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\ndual-decoder model that fuses RAG context to guide the generation process.",
        "published": "2024-11-12T15:26:17Z",
        "link": "http://arxiv.org/abs/2411.07870v2",
        "authors": [
            "Xiaofeng Zhu",
            "Jaya Krishna Mandivarapu"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.07870v2.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0062_Trustful_LLMs_Customizing_and_Grounding_Text_Generation_with_Knowledge_Bases_and_Dual_Decoders.pdf"
    },
    {
        "paper_id": 63,
        "title": "Query Optimization for Parametric Knowledge Refinement in\n  Retrieval-Augmented Large Language Models",
        "summary": "We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel\napproach designed to bridge the pre-retrieval information gap in\nRetrieval-Augmented Generation (RAG) systems through query optimization\ntailored to meet the specific knowledge requirements of Large Language Models\n(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR\nframework begins by extracting parametric knowledge from LLMs, followed by\nusing a specialized query optimizer for refining these queries. This process\nensures the retrieval of only the most pertinent information essential for\ngenerating accurate responses. Moreover, to enhance flexibility and reduce\ncomputational costs, we propose a trainable scheme for our pipeline that\nutilizes a smaller, tunable model as the query optimizer, which is refined\nthrough knowledge distillation from a larger teacher model. Our evaluations on\nvarious question-answering (QA) datasets and with different retrieval systems\nshow that ERRR consistently outperforms existing baselines, proving to be a\nversatile and cost-effective module for improving the utility and accuracy of\nRAG systems.",
        "published": "2024-11-12T14:12:45Z",
        "link": "http://arxiv.org/abs/2411.07820v2",
        "authors": [
            "Youan Cong",
            "Cheng Wang",
            "Pritom Saha Akash",
            "Kevin Chen-Chuan Chang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.07820v2.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0063_Query_Optimization_for_Parametric_Knowledge_Refinement_in_Retrieval-Augmented_Large_Language_Models.pdf"
    },
    {
        "paper_id": 64,
        "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with\n  ImageRAG",
        "summary": "Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000\n$\\times$ 100,000 pixels or more) poses a significant challenge for current\nRemote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize\nthe UHR image to standard input image size, the extensive spatial and\ncontextual information that UHR images contain will be neglected. Otherwise,\nthe original size of these images often exceeds the token limits of standard\nRSMLLMs, making it difficult to process the entire image and capture long-range\ndependencies to answer the query based on the abundant visual context. In this\npaper, we introduce ImageRAG for RS, a training-free framework to address the\ncomplexities of analyzing UHR remote sensing imagery. By transforming UHR\nremote sensing image analysis task to image's long context selection task, we\ndesign an innovative image contextual retrieval mechanism based on the\nRetrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's\ncore innovation lies in its ability to selectively retrieve and focus on the\nmost relevant portions of the UHR image as visual contexts that pertain to a\ngiven query. Fast path and slow path are proposed in this framework to handle\nthis task efficiently and effectively. ImageRAG allows RSMLLMs to manage\nextensive context and spatial information from UHR RSI, ensuring the analysis\nis both accurate and efficient.",
        "published": "2024-11-12T10:12:12Z",
        "link": "http://arxiv.org/abs/2411.07688v1",
        "authors": [
            "Zilun Zhang",
            "Haozhan Shen",
            "Tiancheng Zhao",
            "Yuhao Wang",
            "Bin Chen",
            "Yuxiang Cai",
            "Yongheng Shang",
            "Jianwei Yin"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.07688v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0064_Enhancing_Ultra_High_Resolution_Remote_Sensing_Imagery_Analysis_with_ImageRAG.pdf"
    },
    {
        "paper_id": 65,
        "title": "Toward Optimal Search and Retrieval for RAG",
        "summary": "Retrieval-augmented generation (RAG) is a promising method for addressing\nsome of the memory-related challenges associated with Large Language Models\n(LLMs). Two separate systems form the RAG pipeline, the retriever and the\nreader, and the impact of each on downstream task performance is not\nwell-understood. Here, we work towards the goal of understanding how retrievers\ncan be optimized for RAG pipelines for common tasks such as Question Answering\n(QA). We conduct experiments focused on the relationship between retrieval and\nRAG performance on QA and attributed QA and unveil a number of insights useful\nto practitioners developing high-performance RAG pipelines. For example,\nlowering search accuracy has minor implications for RAG performance while\npotentially increasing retrieval speed and memory efficiency.",
        "published": "2024-11-11T22:06:51Z",
        "link": "http://arxiv.org/abs/2411.07396v1",
        "authors": [
            "Alexandria Leto",
            "Cecilia Aguerrebere",
            "Ishwar Bhati",
            "Ted Willke",
            "Mariano Tepper",
            "Vy Ai Vo"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.07396v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0065_Toward_Optimal_Search_and_Retrieval_for_RAG.pdf"
    },
    {
        "paper_id": 66,
        "title": "ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\n  We There Yet?",
        "summary": "Hallucinations, the tendency to produce irrelevant/incorrect responses, are\nprevalent concerns in generative AI-based tools like ChatGPT. Although\nhallucinations in ChatGPT are studied for textual responses, it is unknown how\nChatGPT hallucinates for technical texts that contain both textual and\ntechnical terms. We surveyed 47 software engineers and produced a benchmark of\n412 Q&A pairs from the bug reports of two OSS projects. We find that a\nRAG-based ChatGPT (i.e., ChatGPT tuned with the benchmark issue reports) is\n36.4% correct when producing answers to the questions, due to two reasons 1)\nlimitations to understand complex technical contents in code snippets like\nstack traces, and 2) limitations to integrate contexts denoted in the technical\nterms and texts. We present CHIME (ChatGPT Inaccuracy Mitigation Engine) whose\nunderlying principle is that if we can preprocess the technical reports better\nand guide the query validation process in ChatGPT, we can address the observed\nlimitations. CHIME uses context-free grammar (CFG) to parse stack traces in\ntechnical reports. CHIME then verifies and fixes ChatGPT responses by applying\nmetamorphic testing and query transformation. In our benchmark, CHIME shows\n30.3% more correction over ChatGPT responses. In a user study, we find that the\nimproved responses with CHIME are considered more useful than those generated\nfrom ChatGPT without CHIME.",
        "published": "2024-11-11T20:54:54Z",
        "link": "http://arxiv.org/abs/2411.07360v1",
        "authors": [
            "Salma Begum Tamanna",
            "Gias Uddin",
            "Song Wang",
            "Lan Xia",
            "Longyu Zhang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.07360v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0066_ChatGPT_Inaccuracy_Mitigation_during_Technical_Report_Understanding_Are_We_There_Yet.pdf"
    },
    {
        "paper_id": 67,
        "title": "OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model",
        "summary": "OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,\nfinetuned on over 2,000,000 Thai instruction pairs. This report provides an\nengineering perspective on the model's development, capabilities, and\nperformance. We discuss the model's architecture, training process, and key\nfeatures, including multi-turn conversation support, Retrieval Augmented\nGeneration (RAG) compatibility, and tool-calling functionality. Benchmark\nresults demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various\nThai language tasks, outperforming other open-source Thai language models. We\nalso address practical considerations such as GPU memory requirements and\ndeployment strategies.",
        "published": "2024-11-11T18:58:46Z",
        "link": "http://arxiv.org/abs/2411.07238v1",
        "authors": [
            "Sumeth Yuenyong",
            "Kobkrit Viriyayudhakorn",
            "Apivadee Piyatumrong",
            "Jillaphat Jaroenkantasima"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.07238v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0067_OpenThaiGPT_15_A_Thai-Centric_Open_Source_Large_Language_Model.pdf"
    },
    {
        "paper_id": 68,
        "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
        "summary": "Retrieval-augmented generation (RAG) has shown impressive capability in\nproviding reliable answer predictions and addressing hallucination problems. A\ntypical RAG implementation uses powerful retrieval models to extract external\ninformation and large language models (LLMs) to generate answers. In contrast,\nrecent LLM-based retrieval has gained attention for its substantial\nimprovements in information retrieval (IR) due to the LLMs' semantic\nunderstanding capability. However, directly applying LLM to RAG systems\npresents challenges. This may cause feature locality problems as massive\nparametric knowledge can hinder effective usage of global information across\nthe corpus; for example, an LLM-based retriever often inputs document summaries\ninstead of full documents. Moreover, various pre-trained tasks in LLMs\nintroduce variance, further weakening performance as a retriever.\n  To address these issues, we propose a novel two-stage fine-tuning\narchitecture called Invar-RAG. In the retrieval stage, an LLM-based retriever\nis constructed by integrating LoRA-based representation learning to tackle\nfeature locality issues. To enhance retrieval performance, we develop two\npatterns (invariant and variant patterns) and an invariance loss to reduce LLM\nvariance. In the generation stage, a refined fine-tuning method is employed to\nimprove LLM accuracy in generating answers based on retrieved information.\nExperimental results show that Invar-RAG significantly outperforms existing\nbaselines across three open-domain question answering (ODQA) datasets. Code is\navailable in the Supplementary Material for reproducibility.",
        "published": "2024-11-11T14:25:37Z",
        "link": "http://arxiv.org/abs/2411.07021v2",
        "authors": [
            "Ziwei Liu",
            "Liang Zhang",
            "Qian Li",
            "Jianghua Wu",
            "Guangxu Zhu"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.07021v2.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0068_Invar-RAG_Invariant_LLM-aligned_Retrieval_for_Better_Generation.pdf"
    },
    {
        "paper_id": 69,
        "title": "AssistRAG: Boosting the Potential of Large Language Models with an\n  Intelligent Information Assistant",
        "summary": "The emergence of Large Language Models (LLMs) has significantly advanced\nnatural language processing, but these models often generate factually\nincorrect information, known as \"hallucination\". Initial retrieval-augmented\ngeneration (RAG) methods like the \"Retrieve-Read\" framework was inadequate for\ncomplex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised\nFine-Tuning (SFT) methods improved performance but required frequent retraining\nand risked altering foundational LLM capabilities. To cope with these\nchallenges, we propose Assistant-based Retrieval-Augmented Generation\n(AssistRAG), integrating an intelligent information assistant within LLMs. This\nassistant manages memory and knowledge through tool usage, action execution,\nmemory building, and plan specification. Using a two-phase training approach,\nCurriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG\nenhances information retrieval and decision-making. Experiments show AssistRAG\nsignificantly outperforms benchmarks, especially benefiting less advanced LLMs,\nby providing superior reasoning capabilities and accurate responses.",
        "published": "2024-11-11T09:03:52Z",
        "link": "http://arxiv.org/abs/2411.06805v1",
        "authors": [
            "Yujia Zhou",
            "Zheng Liu",
            "Zhicheng Dou"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.06805v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0069_AssistRAG_Boosting_the_Potential_of_Large_Language_Models_with_an_Intelligent_Information_Assistant.pdf"
    },
    {
        "paper_id": 70,
        "title": "Region-Aware Text-to-Image Generation via Hard Binding and Soft\n  Refinement",
        "summary": "Regional prompting, or compositional generation, which enables fine-grained\nspatial control, has gained increasing attention for its practicality in\nreal-world applications. However, previous methods either introduce additional\ntrainable modules, thus only applicable to specific models, or manipulate on\nscore maps within cross-attention layers using attention masks, resulting in\nlimited control strength when the number of regions increases. To handle these\nlimitations, we present RAG, a Regional-Aware text-to-image Generation method\nconditioned on regional descriptions for precise layout composition. RAG\ndecouple the multi-region generation into two sub-tasks, the construction of\nindividual region (Regional Hard Binding) that ensures the regional prompt is\nproperly executed, and the overall detail refinement (Regional Soft Refinement)\nover regions that dismiss the visual boundaries and enhance adjacent\ninteractions. Furthermore, RAG novelly makes repainting feasible, where users\ncan modify specific unsatisfied regions in the last generation while keeping\nall other regions unchanged, without relying on additional inpainting models.\nOur approach is tuning-free and applicable to other frameworks as an\nenhancement to the prompt following property. Quantitative and qualitative\nexperiments demonstrate that RAG achieves superior performance over attribute\nbinding and object relationship than previous tuning-free methods.",
        "published": "2024-11-10T18:45:41Z",
        "link": "http://arxiv.org/abs/2411.06558v2",
        "authors": [
            "Zhennan Chen",
            "Yajie Li",
            "Haofan Wang",
            "Zhibo Chen",
            "Zhengkai Jiang",
            "Jun Li",
            "Qian Wang",
            "Jian Yang",
            "Ying Tai"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.06558v2.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0070_Region-Aware_Text-to-Image_Generation_via_Hard_Binding_and_Soft_Refinement.pdf"
    },
    {
        "paper_id": 71,
        "title": "LProtector: An LLM-driven Vulnerability Detection System",
        "summary": "This paper presents LProtector, an automated vulnerability detection system\nfor C/C++ codebases driven by the large language model (LLM) GPT-4o and\nRetrieval-Augmented Generation (RAG). As software complexity grows, traditional\nmethods face challenges in detecting vulnerabilities effectively. LProtector\nleverages GPT-4o's powerful code comprehension and generation capabilities to\nperform binary classification and identify vulnerabilities within target\ncodebases. We conducted experiments on the Big-Vul dataset, showing that\nLProtector outperforms two state-of-the-art baselines in terms of F1 score,\ndemonstrating the potential of integrating LLMs with vulnerability detection.",
        "published": "2024-11-10T15:21:30Z",
        "link": "http://arxiv.org/abs/2411.06493v2",
        "authors": [
            "Ze Sheng",
            "Fenghua Wu",
            "Xiangwu Zuo",
            "Chao Li",
            "Yuxin Qiao",
            "Lei Hang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.06493v2.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0071_LProtector_An_LLM-driven_Vulnerability_Detection_System.pdf"
    },
    {
        "paper_id": 72,
        "title": "Leveraging Retrieval-Augmented Generation for University Knowledge\n  Retrieval",
        "summary": "This paper introduces an innovative approach using Retrieval-Augmented\nGeneration (RAG) pipelines with Large Language Models (LLMs) to enhance\ninformation retrieval and query response systems for university-related\nquestion answering. By systematically extracting data from the university\nofficial webpage and employing advanced prompt engineering techniques, we\ngenerate accurate, contextually relevant responses to user queries.\n  We developed a comprehensive university benchmark, UniversityQuestionBench\n(UQB), to rigorously evaluate our system performance, based on common key\nmetrics in the filed of RAG pipelines, assessing accuracy and reliability\nthrough various metrics and real-world scenarios. Our experimental results\ndemonstrate significant improvements in the precision and relevance of\ngenerated responses, enhancing user experience and reducing the time required\nto obtain relevant answers. In summary, this paper presents a novel application\nof RAG pipelines and LLMs, supported by a meticulously prepared university\nbenchmark, offering valuable insights into advanced AI techniques for academic\ndata retrieval and setting the stage for future research in this domain.",
        "published": "2024-11-09T17:38:01Z",
        "link": "http://arxiv.org/abs/2411.06237v1",
        "authors": [
            "Arshia Hemmat",
            "Kianoosh Vadaei",
            "Mohammad Hassan Heydari",
            "Afsaneh Fatemi"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.06237v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0072_Leveraging_Retrieval-Augmented_Generation_for_University_Knowledge_Retrieval.pdf"
    },
    {
        "paper_id": 73,
        "title": "Exploring Knowledge Boundaries in Large Language Models for Retrieval\n  Judgment",
        "summary": "Large Language Models (LLMs) are increasingly recognized for their practical\napplications. However, these models often encounter challenges in dynamically\nchanging knowledge, as well as in managing unknown static knowledge.\nRetrieval-Augmented Generation (RAG) tackles this challenge and has shown a\nsignificant impact on LLMs. Actually, we find that the impact of RAG on the\nquestion answering capabilities of LLMs can be categorized into three groups:\nbeneficial, neutral, and harmful. By minimizing retrieval requests that yield\nneutral or harmful results, we can effectively reduce both time and\ncomputational costs, while also improving the overall performance of LLMs. This\ninsight motivates us to differentiate between types of questions using certain\nmetrics as indicators, to decrease the retrieval ratio without compromising\nperformance. In our work, we propose a method that is able to identify\ndifferent types of questions from this view by training a Knowledge Boundary\nModel (KBM). Experiments conducted on 11 English and Chinese datasets\nillustrate that the KBM effectively delineates the knowledge boundary,\nsignificantly decreasing the proportion of retrievals required for optimal\nend-to-end performance. Specifically, we evaluate the effectiveness of KBM in\nthree complex scenarios: dynamic knowledge, long-tail static knowledge, and\nmulti-hop problems, as well as its functionality as an external LLM plug-in.",
        "published": "2024-11-09T15:12:28Z",
        "link": "http://arxiv.org/abs/2411.06207v1",
        "authors": [
            "Zhen Zhang",
            "Xinyu Wang",
            "Yong Jiang",
            "Zhuo Chen",
            "Feiteng Mu",
            "Mengting Hu",
            "Pengjun Xie",
            "Fei Huang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.06207v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0073_Exploring_Knowledge_Boundaries_in_Large_Language_Models_for_Retrieval_Judgment.pdf"
    },
    {
        "paper_id": 74,
        "title": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs",
        "summary": "This paper introduces an innovative semi-supervised learning approach for\ntext classification, addressing the challenge of abundant data but limited\nlabeled examples. Our methodology integrates few-shot learning with\nretrieval-augmented generation (RAG) and conventional statistical clustering,\nenabling effective learning from a minimal number of labeled instances while\ngenerating high-quality labeled data. To the best of our knowledge, we are the\nfirst to incorporate RAG alongside clustering in text data generation. Our\nexperiments on the Reuters and Web of Science datasets demonstrate\nstate-of-the-art performance, with few-shot augmented data alone producing\nresults nearly equivalent to those achieved with fully labeled datasets.\nNotably, accuracies of 95.41\\% and 82.43\\% were achieved for complex text\ndocument classification tasks, where the number of categories can exceed 100.",
        "published": "2024-11-09T13:17:39Z",
        "link": "http://arxiv.org/abs/2411.06175v1",
        "authors": [
            "Shan Zhong",
            "Jiahao Zeng",
            "Yongxin Yu",
            "Bohong Lin"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.06175v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0074_Clustering_Algorithms_and_RAG_Enhancing_Semi-Supervised_Text_Classification_with_Large_LLMs.pdf"
    },
    {
        "paper_id": 75,
        "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems",
        "summary": "Augmenting LLMs with context leads to improved performance across many\napplications. Despite much research on Retrieval Augmented Generation (RAG)\nsystems, an open question is whether errors arise because LLMs fail to utilize\nthe context from retrieval or the context itself is insufficient to answer the\nquery. To shed light on this, we develop a new notion of sufficient context,\nalong with a way to classify instances that have enough information to answer\nthe query. We then use sufficient context to analyze several models and\ndatasets. By stratifying errors based on context sufficiency, we find that\nproprietary LLMs (Gemini, GPT, Claude) excel at answering queries when the\ncontext is sufficient, but often output incorrect answers instead of abstaining\nwhen the context is not. On the other hand, open-source LLMs (Llama, Mistral,\nGemma) hallucinate or abstain often, even with sufficient context. We further\ncategorize cases when the context is useful, and improves accuracy, even though\nit does not fully answer the query and the model errs without the context.\nBuilding on our findings, we explore ways to reduce hallucinations in RAG\nsystems, including a new selective generation method that leverages sufficient\ncontext information for guided abstention. Our method improves the fraction of\ncorrect answers among times where the model responds by 2-10% for Gemini, GPT,\nand Gemma.",
        "published": "2024-11-09T02:13:14Z",
        "link": "http://arxiv.org/abs/2411.06037v1",
        "authors": [
            "Hailey Joren",
            "Jianyi Zhang",
            "Chun-Sung Ferng",
            "Da-Cheng Juan",
            "Ankur Taly",
            "Cyrus Rashtchian"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.06037v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0075_Sufficient_Context_A_New_Lens_on_Retrieval_Augmented_Generation_Systems.pdf"
    },
    {
        "paper_id": 76,
        "title": "Multi-Document Financial Question Answering using LLMs",
        "summary": "We propose two new methods for multi-document financial question answering.\nFirst, a method that uses semantic tagging, and then, queries the index to get\nthe context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that\nuses semantic tagging, and, retrieves knowledge graph triples from a graph\ndatabase, as context. KG_RAG uses knowledge graphs constructed using a small\nmodel that is fine-tuned using knowledge distillation using a large teacher\nmodel. The data consists of 18 10K reports of Apple, Microsoft, Alphabet,\nNVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of\nquestions in the data consists of 111 complex questions including many esoteric\nquestions that are difficult to answer and the answers are not completely\nobvious. As evaluation metrics, we use overall scores as well as segmented\nscores for measurement including the faithfulness, relevance, correctness,\nsimilarity, an LLM based overall score and the rouge scores as well as a\nsimilarity of embeddings. We find that both methods outperform plain RAG\nsignificantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.",
        "published": "2024-11-08T21:03:54Z",
        "link": "http://arxiv.org/abs/2411.07264v1",
        "authors": [
            "Shalin Shah",
            "Srikanth Ryali",
            "Ramasubbu Venkatesh"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.07264v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0076_Multi-Document_Financial_Question_Answering_using_LLMs.pdf"
    },
    {
        "paper_id": 77,
        "title": "Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\n  Bengali Mathematical Olympiad Problem Solving",
        "summary": "We present an innovative approach for solving mathematical problems in\nBengali, developed for the DL Sprint 3.0 BUET CSE Fest 2024 Competition. Our\nmethod uses advanced deep learning models, notably the Qwen 2.5 series, with\nimprovements made through prompt engineering, model quantization, and Tool\nIntegrated Reasoning (TIR) to handle complex calculations. Initially, we\nexplored various model architectures, including fine-tuned Mistral and\nquantized Qwen models, refining them with translation techniques,\nRetrieval-Augmented Generation (RAG), and custom dataset curation. Manual\nhyperparameter tuning optimized parameters like temperature and top-p to\nenhance model adaptability and accuracy. Removal of RAG and parameter\nadjustments further improved robustness. Our approach highlights the potential\nof advanced NLP techniques in solving Bengali mathematical problems.",
        "published": "2024-11-08T19:44:12Z",
        "link": "http://arxiv.org/abs/2411.05934v1",
        "authors": [
            "Saad Tahmid",
            "Sourav Sarker"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.05934v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0077_Qwen25-32B_Leveraging_Self-Consistent_Tool-Integrated_Reasoning_for_Bengali_Mathematical_Olympiad_Problem_Solving.pdf"
    },
    {
        "paper_id": 78,
        "title": "FinDVer: Explainable Claim Verification over Long and Hybrid-Content\n  Financial Documents",
        "summary": "We introduce FinDVer, a comprehensive benchmark specifically designed to\nevaluate the explainable claim verification capabilities of LLMs in the context\nof understanding and analyzing long, hybrid-content financial documents.\nFinDVer contains 2,400 expert-annotated examples, divided into three subsets:\ninformation extraction, numerical reasoning, and knowledge-intensive reasoning,\neach addressing common scenarios encountered in real-world financial contexts.\nWe assess a broad spectrum of LLMs under long-context and RAG settings. Our\nresults show that even the current best-performing system, GPT-4o, still lags\nbehind human experts. We further provide in-depth analysis on long-context and\nRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering\ninsights to drive future advancements. We believe that FinDVer can serve as a\nvaluable benchmark for evaluating LLMs in claim verification over complex,\nexpert-domain documents.",
        "published": "2024-11-08T18:26:17Z",
        "link": "http://arxiv.org/abs/2411.05764v1",
        "authors": [
            "Yilun Zhao",
            "Yitao Long",
            "Yuru Jiang",
            "Chengye Wang",
            "Weiyuan Chen",
            "Hongjun Liu",
            "Yiming Zhang",
            "Xiangru Tang",
            "Chen Zhao",
            "Arman Cohan"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.05764v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0078_FinDVer_Explainable_Claim_Verification_over_Long_and_Hybrid-Content_Financial_Documents.pdf"
    },
    {
        "paper_id": 79,
        "title": "IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge\n  Delivery",
        "summary": "In the rapidly evolving landscape of cyber security, intelligent chatbots are\ngaining prominence. Artificial Intelligence, Machine Learning, and Natural\nLanguage Processing empower these chatbots to handle user inquiries and deliver\nthreat intelligence. This helps cyber security knowledge readily available to\nboth professionals and the public. Traditional rule-based chatbots often lack\nflexibility and struggle to adapt to user interactions. In contrast, Large\nLanguage Model-based chatbots offer contextually relevant information across\nmultiple domains and adapt to evolving conversational contexts. In this work,\nwe develop IntellBot, an advanced cyber security Chatbot built on top of\ncutting-edge technologies like Large Language Models and Langchain alongside a\nRetrieval-Augmented Generation model to deliver superior capabilities. This\nchatbot gathers information from diverse data sources to create a comprehensive\nknowledge base covering known vulnerabilities, recent cyber attacks, and\nemerging threats. It delivers tailored responses, serving as a primary hub for\ncyber security insights. By providing instant access to relevant information\nand resources, this IntellBot enhances threat intelligence, incident response,\nand overall security posture, saving time and empowering users with knowledge\nof cyber security best practices. Moreover, we analyzed the performance of our\ncopilot using a two-stage evaluation strategy. We achieved BERT score above 0.8\nby indirect approach and a cosine similarity score ranging from 0.8 to 1, which\naffirms the accuracy of our copilot. Additionally, we utilized RAGAS to\nevaluate the RAG model, and all evaluation metrics consistently produced scores\nabove 0.77, highlighting the efficacy of our system.",
        "published": "2024-11-08T09:40:53Z",
        "link": "http://arxiv.org/abs/2411.05442v1",
        "authors": [
            "Dincy R. Arikkat",
            "Abhinav M.",
            "Navya Binu",
            "Parvathi M.",
            "Navya Biju",
            "K. S. Arunima",
            "Vinod P.",
            "Rafidha Rehiman K. A.",
            "Mauro Conti"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.05442v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0079_IntellBot_Retrieval_Augmented_LLM_Chatbot_for_Cyber_Threat_Knowledge_Delivery.pdf"
    },
    {
        "paper_id": 80,
        "title": "Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\n  Cluster Diagnosis System and Evaluation Framework",
        "summary": "Recent advancements in Large Language Models (LLMs) and related technologies\nsuch as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have\nenabled the creation of autonomous intelligent systems capable of performing\ncluster diagnostics and troubleshooting. By integrating these technologies with\nself-play methodologies, we have developed an LLM-agent system designed to\nautonomously diagnose and resolve issues within AI clusters. Our innovations\ninclude a knowledge base tailored for cluster diagnostics, enhanced LLM\nalgorithms, practical deployment strategies for agents, and a benchmark\nspecifically designed for evaluating LLM capabilities in this domain. Through\nextensive experimentation across multiple dimensions, we have demonstrated the\nsuperiority of our system in addressing the challenges faced in cluster\ndiagnostics, particularly in detecting and rectifying performance issues more\nefficiently and accurately than traditional methods.",
        "published": "2024-11-08T06:12:56Z",
        "link": "http://arxiv.org/abs/2411.05349v1",
        "authors": [
            "Honghao Shi",
            "Longkai Cheng",
            "Wenli Wu",
            "Yuhang Wang",
            "Xuan Liu",
            "Shaokai Nie",
            "Weixv Wang",
            "Xuebin Min",
            "Chunlei Men",
            "Yonghua Lin"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.05349v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0080_Enhancing_Cluster_Resilience_LLM-agent_Based_Autonomous_Intelligent_Cluster_Diagnosis_System_and_Evaluation_Framework.pdf"
    },
    {
        "paper_id": 81,
        "title": "A Taxonomy of AgentOps for Enabling Observability of Foundation Model\n  based Agents",
        "summary": "The ever-improving quality of LLMs has fueled the growth of a diverse range\nof downstream tasks, leading to an increased demand for AI automation and a\nburgeoning interest in developing foundation model (FM)-based autonomous\nagents. As AI agent systems tackle more complex tasks and evolve, they involve\na wider range of stakeholders, including agent users, agentic system developers\nand deployers, and AI model developers. These systems also integrate multiple\ncomponents such as AI agent workflows, RAG pipelines, prompt management, agent\ncapabilities, and observability features. In this case, obtaining reliable\noutputs and answers from these agents remains challenging, necessitating a\ndependable execution process and end-to-end observability solutions. To build\nreliable AI agents and LLM applications, it is essential to shift towards\ndesigning AgentOps platforms that ensure observability and traceability across\nthe entire development-to-production life-cycle. To this end, we conducted a\nrapid review and identified relevant AgentOps tools from the agentic ecosystem.\nBased on this review, we provide an overview of the essential features of\nAgentOps and propose a comprehensive overview of observability data/traceable\nartifacts across the agent production life-cycle. Our findings provide a\nsystematic overview of the current AgentOps landscape, emphasizing the critical\nrole of observability/traceability in enhancing the reliability of autonomous\nagent systems.",
        "published": "2024-11-08T02:31:03Z",
        "link": "http://arxiv.org/abs/2411.05285v1",
        "authors": [
            "Liming Dong",
            "Qinghua Lu",
            "Liming Zhu"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.05285v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0081_A_Taxonomy_of_AgentOps_for_Enabling_Observability_of_Foundation_Model_based_Agents.pdf"
    },
    {
        "paper_id": 82,
        "title": "Deploying Large Language Models With Retrieval Augmented Generation",
        "summary": "Knowing that the generative capabilities of large language models (LLM) are\nsometimes hampered by tendencies to hallucinate or create non-factual\nresponses, researchers have increasingly focused on methods to ground generated\noutputs in factual data. Retrieval Augmented Generation (RAG) has emerged as a\nkey approach for integrating knowledge from data sources outside of the LLM's\ntraining set, including proprietary and up-to-date information. While many\nresearch papers explore various RAG strategies, their true efficacy is tested\nin real-world applications with actual data. The journey from conceiving an\nidea to actualizing it in the real world is a lengthy process. We present\ninsights from the development and field-testing of a pilot project that\nintegrates LLMs with RAG for information retrieval. Additionally, we examine\nthe impacts on the information value chain, encompassing people, processes, and\ntechnology. Our aim is to identify the opportunities and challenges of\nimplementing this emerging technology, particularly within the context of\nbehavioral research in the information systems (IS) field. The contributions of\nthis work include the development of best practices and recommendations for\nadopting this promising technology while ensuring compliance with industry\nregulations through a proposed AI governance model.",
        "published": "2024-11-07T22:11:51Z",
        "link": "http://arxiv.org/abs/2411.11895v1",
        "authors": [
            "Sonal Prabhune",
            "Donald J. Berndt"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.11895v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0082_Deploying_Large_Language_Models_With_Retrieval_Augmented_Generation.pdf"
    },
    {
        "paper_id": 83,
        "title": "PentestAgent: Incorporating LLM Agents to Automated Penetration Testing",
        "summary": "Penetration testing is a critical technique for identifying security\nvulnerabilities, traditionally performed manually by skilled security\nspecialists. This complex process involves gathering information about the\ntarget system, identifying entry points, exploiting the system, and reporting\nfindings. Despite its effectiveness, manual penetration testing is\ntime-consuming and expensive, often requiring significant expertise and\nresources that many organizations cannot afford. While automated penetration\ntesting methods have been proposed, they often fall short in real-world\napplications due to limitations in flexibility, adaptability, and\nimplementation.\n  Recent advancements in large language models (LLMs) offer new opportunities\nfor enhancing penetration testing through increased intelligence and\nautomation. However, current LLM-based approaches still face significant\nchallenges, including limited penetration testing knowledge and a lack of\ncomprehensive automation capabilities. To address these gaps, we propose\nPentestAgent, a novel LLM-based automated penetration testing framework that\nleverages the power of LLMs and various LLM-based techniques like Retrieval\nAugmented Generation (RAG) to enhance penetration testing knowledge and\nautomate various tasks. Our framework leverages multi-agent collaboration to\nautomate intelligence gathering, vulnerability analysis, and exploitation\nstages, reducing manual intervention. We evaluate PentestAgent using a\ncomprehensive benchmark, demonstrating superior performance in task completion\nand overall efficiency. This work significantly advances the practical\napplicability of automated penetration testing systems.",
        "published": "2024-11-07T21:10:39Z",
        "link": "http://arxiv.org/abs/2411.05185v1",
        "authors": [
            "Xiangmin Shen",
            "Lingzhi Wang",
            "Zhenyuan Li",
            "Yan Chen",
            "Wencheng Zhao",
            "Dawei Sun",
            "Jiashui Wang",
            "Wei Ruan"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.05185v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0083_PentestAgent_Incorporating_LLM_Agents_to_Automated_Penetration_Testing.pdf"
    },
    {
        "paper_id": 84,
        "title": "Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with\n  Retrieval-Augmented Generation",
        "summary": "Current leading Text-To-Audio (TTA) generation models suffer from degraded\nperformance on zero-shot and few-shot settings. It is often challenging to\ngenerate high-quality audio for audio events that are unseen or uncommon in the\ntraining set. Inspired by the success of Retrieval-Augmented Generation (RAG)\nin Large Language Model (LLM)-based knowledge-intensive tasks, we extend the\nTTA process with additional conditioning contexts. We propose Audiobox TTA-RAG,\na novel retrieval-augmented TTA approach based on Audiobox, a conditional\nflow-matching audio generation model. Unlike the vanilla Audiobox TTA solution\nwhich generates audio conditioned on text, we augmented the conditioning input\nwith retrieved audio samples that provide additional acoustic information to\ngenerate the target audio. Our retrieval method does not require the external\ndatabase to have labeled audio, offering more practical use cases. To evaluate\nour proposed method, we curated test sets in zero-shot and few-shot settings.\nOur empirical results show that the proposed model can effectively leverage the\nretrieved audio samples and significantly improve zero-shot and few-shot TTA\nperformance, with large margins on multiple evaluation metrics, while\nmaintaining the ability to generate semantically aligned audio for the\nin-domain setting. In addition, we investigate the effect of different\nretrieval methods and data sources.",
        "published": "2024-11-07T19:50:28Z",
        "link": "http://arxiv.org/abs/2411.05141v1",
        "authors": [
            "Mu Yang",
            "Bowen Shi",
            "Matthew Le",
            "Wei-Ning Hsu",
            "Andros Tjandra"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.05141v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0084_Audiobox_TTA-RAG_Improving_Zero-Shot_and_Few-Shot_Text-To-Audio_with_Retrieval-Augmented_Generation.pdf"
    },
    {
        "paper_id": 85,
        "title": "M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page\n  Multi-document Understanding",
        "summary": "Document visual question answering (DocVQA) pipelines that answer questions\nfrom documents have broad applications. Existing methods focus on handling\nsingle-page documents with multi-modal language models (MLMs), or rely on\ntext-based retrieval-augmented generation (RAG) that uses text extraction tools\nsuch as optical character recognition (OCR). However, there are difficulties in\napplying these methods in real-world scenarios: (a) questions often require\ninformation across different pages or documents, where MLMs cannot handle many\nlong documents; (b) documents often have important information in visual\nelements such as figures, but text extraction tools ignore them. We introduce\nM3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various\ndocument contexts (closed-domain and open-domain), question hops (single-hop\nand multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG\nfinds relevant documents and answers questions using a multi-modal retriever\nand an MLM, so that it can efficiently handle single or many documents while\npreserving visual information. Since previous DocVQA datasets ask questions in\nthe context of a specific document, we also present M3DocVQA, a new benchmark\nfor evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.\nIn three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results\nshow that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance\nthan many strong baselines, including state-of-the-art performance in\nMP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and\nretrieval models. Lastly, we qualitatively show that M3DocRAG can successfully\nhandle various scenarios, such as when relevant information exists across\nmultiple pages and when answer evidence only exists in images.",
        "published": "2024-11-07T18:29:38Z",
        "link": "http://arxiv.org/abs/2411.04952v1",
        "authors": [
            "Jaemin Cho",
            "Debanjan Mahata",
            "Ozan Irsoy",
            "Yujie He",
            "Mohit Bansal"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.04952v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0085_M3DocRAG_Multi-modal_Retrieval_is_What_You_Need_for_Multi-page_Multi-document_Understanding.pdf"
    },
    {
        "paper_id": 86,
        "title": "LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation\n  Combining Hierarchical Agents and RAG",
        "summary": "The increasing use of smart devices has emphasized the critical role of\nmaintenance in production activities. Interactive Electronic Technical Manuals\n(IETMs) are vital tools that support the maintenance of smart equipment.\nHowever, traditional IETMs face challenges such as transitioning from Graphical\nUser Interfaces (GUIs) to natural Language User Interfaces (LUIs) and managing\ncomplex logical relationships. Additionally, they must meet the current demands\nfor higher intelligence. This paper proposes a Maintenance Scheme Generation\nMethod based on Large Language Models (LLM-R). The proposed method includes\nseveral key innovations: We propose the Low Rank Adaptation-Knowledge Retention\n(LORA-KR) loss technology to proportionally adjust mixed maintenance data for\nfine-tuning the LLM. This method prevents knowledge conflicts caused by mixed\ndata, improving the model's adaptability and reasoning ability in specific\nmaintenance domains, Besides, Hierarchical Task-Based Agent and\nInstruction-level Retrieval-Augmented Generation (RAG) technologies are adopted\nto optimize the generation steps and mitigate the phenomenon of hallucination\ncaused by the model's Inability to access contextual information. This\nenhancement improves the model's flexibility and accuracy in handling known or\nunknown maintenance objects and maintenance scheme scenarios. To validate the\nproposed method's effectiveness in maintenance tasks, a maintenance scheme\ndataset was constructed using objects from different fields. The experimental\nresults show that the accuracy of the maintenance schemes generated by the\nproposed method reached 91.59%, indicating which improvement enhances the\nintelligence of maintenance schemes and introduces novel technical approaches\nfor equipment maintenance.",
        "published": "2024-11-07T07:07:34Z",
        "link": "http://arxiv.org/abs/2411.04476v1",
        "authors": [
            "Laifa Tao",
            "Qixuan Huang",
            "Xianjun Wu",
            "Weiwei Zhang",
            "Yunlong Wu",
            "Bin Li",
            "Chen Lu",
            "Xingshuo Hai"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.04476v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0086_LLM-R_A_Framework_for_Domain-Adaptive_Maintenance_Scheme_Generation_Combining_Hierarchical_Agents_and_RAG.pdf"
    },
    {
        "paper_id": 87,
        "title": "ML-Promise: A Multilingual Dataset for Corporate Promise Verification",
        "summary": "Promises made by politicians, corporate leaders, and public figures have a\nsignificant impact on public perception, trust, and institutional reputation.\nHowever, the complexity and volume of such commitments, coupled with\ndifficulties in verifying their fulfillment, necessitate innovative methods for\nassessing their credibility. This paper introduces the concept of Promise\nVerification, a systematic approach involving steps such as promise\nidentification, evidence assessment, and the evaluation of timing for\nverification. We propose the first multilingual dataset, ML-Promise, which\nincludes English, French, Chinese, Japanese, and Korean, aimed at facilitating\nin-depth verification of promises, particularly in the context of\nEnvironmental, Social, and Governance (ESG) reports. Given the growing emphasis\non corporate environmental contributions, this dataset addresses the challenge\nof evaluating corporate promises, especially in light of practices like\ngreenwashing. Our findings also explore textual and image-based baselines, with\npromising results from retrieval-augmented generation (RAG) approaches. This\nwork aims to foster further discourse on the accountability of public\ncommitments across multiple languages and domains.",
        "published": "2024-11-07T06:51:24Z",
        "link": "http://arxiv.org/abs/2411.04473v1",
        "authors": [
            "Yohei Seki",
            "Hakusen Shu",
            "Anaïs Lhuissier",
            "Hanwool Lee",
            "Juyeon Kang",
            "Min-Yuh Day",
            "Chung-Chi Chen"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.04473v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0087_ML-Promise_A_Multilingual_Dataset_for_Corporate_Promise_Verification.pdf"
    },
    {
        "paper_id": 88,
        "title": "GPT-Guided Monte Carlo Tree Search for Symbolic Regression in Financial\n  Fraud Detection",
        "summary": "With the increasing number of financial services available online, the rate\nof financial fraud has also been increasing. The traffic and transaction rates\non the internet have increased considerably, leading to a need for fast\ndecision-making. Financial institutions also have stringent regulations that\noften require transparency and explainability of the decision-making process.\nHowever, most state-of-the-art algorithms currently used in the industry are\nhighly parameterized black-box models that rely on complex computations to\ngenerate a score. These algorithms are inherently slow and lack the\nexplainability and speed of traditional rule-based learners. This work\nintroduces SR-MCTS (Symbolic Regression MCTS), which utilizes a foundational\nGPT model to guide the MCTS, significantly enhancing its convergence speed and\nthe quality of the generated expressions which are further extracted to rules.\nOur experiments show that SR-MCTS can detect fraud more efficiently than widely\nused methods in the industry while providing substantial insights into the\ndecision-making process.",
        "published": "2024-11-07T06:12:38Z",
        "link": "http://arxiv.org/abs/2411.04459v1",
        "authors": [
            "Prashank Kadam"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.04459v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0088_GPT-Guided_Monte_Carlo_Tree_Search_for_Symbolic_Regression_in_Financial_Fraud_Detection.pdf"
    },
    {
        "paper_id": 89,
        "title": "AMSnet-KG: A Netlist Dataset for LLM-based AMS Circuit Auto-Design Using\n  Knowledge Graph RAG",
        "summary": "High-performance analog and mixed-signal (AMS) circuits are mainly\nfull-custom designed, which is time-consuming and labor-intensive. A\nsignificant portion of the effort is experience-driven, which makes the\nautomation of AMS circuit design a formidable challenge. Large language models\n(LLMs) have emerged as powerful tools for Electronic Design Automation (EDA)\napplications, fostering advancements in the automatic design process for\nlarge-scale AMS circuits. However, the absence of high-quality datasets has led\nto issues such as model hallucination, which undermines the robustness of\nautomatically generated circuit designs. To address this issue, this paper\nintroduces AMSnet-KG, a dataset encompassing various AMS circuit schematics and\nnetlists. We construct a knowledge graph with annotations on detailed\nfunctional and performance characteristics. Facilitated by AMSnet-KG, we\npropose an automated AMS circuit generation framework that utilizes the\ncomprehensive knowledge embedded in LLMs. We first formulate a design strategy\n(e.g., circuit architecture using a number of circuit components) based on\nrequired specifications. Next, matched circuit components are retrieved and\nassembled into a complete topology, and transistor sizing is obtained through\nBayesian optimization. Simulation results of the netlist are fed back to the\nLLM for further topology refinement, ensuring the circuit design specifications\nare met. We perform case studies of operational amplifier and comparator design\nto verify the automatic design flow from specifications to netlists with\nminimal human effort. The dataset used in this paper will be open-sourced upon\npublishing of this paper.",
        "published": "2024-11-07T02:49:53Z",
        "link": "http://arxiv.org/abs/2411.13560v1",
        "authors": [
            "Yichen Shi",
            "Zhuofu Tao",
            "Yuhao Gao",
            "Tianjia Zhou",
            "Cheng Chang",
            "Yaxing Wang",
            "Bingyu Chen",
            "Genhao Zhang",
            "Alvin Liu",
            "Zhiping Yu",
            "Ting-Jung Lin",
            "Lei He"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.13560v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0089_AMSnet-KG_A_Netlist_Dataset_for_LLM-based_AMS_Circuit_Auto-Design_Using_Knowledge_Graph_RAG.pdf"
    },
    {
        "paper_id": 90,
        "title": "Enhancing classroom teaching with LLMs and RAG",
        "summary": "Large Language Models have become a valuable source of information for our\ndaily inquiries. However, after training, its data source quickly becomes\nout-of-date, making RAG a useful tool for providing even more recent or\npertinent data. In this work, we investigate how RAG pipelines, with the course\nmaterials serving as a data source, might help students in K-12 education. The\ninitial research utilizes Reddit as a data source for up-to-date cybersecurity\ninformation. Chunk size is evaluated to determine the optimal amount of context\nneeded to generate accurate answers. After running the experiment for different\nchunk sizes, answer correctness was evaluated using RAGAs with average answer\ncorrectness not exceeding 50 percent for any chunk size. This suggests that\nReddit is not a good source to mine for data for questions about cybersecurity\nthreats. The methodology was successful in evaluating the data source, which\nhas implications for its use to evaluate educational resources for\neffectiveness.",
        "published": "2024-11-07T00:39:34Z",
        "link": "http://arxiv.org/abs/2411.04341v1",
        "authors": [
            "Elizabeth A Mullins",
            "Adrian Portillo",
            "Kristalys Ruiz-Rohena",
            "Aritran Piplai"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.04341v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0090_Enhancing_classroom_teaching_with_LLMs_and_RAG.pdf"
    },
    {
        "paper_id": 91,
        "title": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\n  for Design Space Exploration",
        "summary": "GraphRAG addresses significant challenges in Retrieval-Augmented Generation\n(RAG) by leveraging graphs with embedded knowledge to enhance the reasoning\ncapabilities of Large Language Models (LLMs). Despite its promising potential,\nthe GraphRAG community currently lacks a unified framework for fine-grained\ndecomposition of the graph-based knowledge retrieval process. Furthermore,\nthere is no systematic categorization or evaluation of existing solutions\nwithin the retrieval process. In this paper, we present LEGO-GraphRAG, a\nmodular framework that decomposes the retrieval process of GraphRAG into three\ninterconnected modules: subgraph-extraction, path-filtering, and\npath-refinement. We systematically summarize and classify the algorithms and\nneural network (NN) models relevant to each module, providing a clearer\nunderstanding of the design space for GraphRAG instances. Additionally, we\nidentify key design factors, such as Graph Coupling and Computational Cost,\nthat influence the effectiveness of GraphRAG implementations. Through extensive\nempirical studies, we construct high-quality GraphRAG instances using a\nrepresentative selection of solutions and analyze their impact on retrieval and\nreasoning performance. Our findings offer critical insights into optimizing\nGraphRAG instance design, ultimately contributing to the advancement of more\naccurate and contextually relevant LLM applications.",
        "published": "2024-11-06T15:32:28Z",
        "link": "http://arxiv.org/abs/2411.05844v1",
        "authors": [
            "Yukun Cao",
            "Zengyi Gao",
            "Zhiyang Li",
            "Xike Xie",
            "S Kevin Zhou"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.05844v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0091_LEGO-GraphRAG_Modularizing_Graph-based_Retrieval-Augmented_Generation_for_Design_Space_Exploration.pdf"
    },
    {
        "paper_id": 92,
        "title": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in\n  Retrieval-Augmented Generation",
        "summary": "Retrieval-Augmented Generation (RAG) has proven to be an effective method for\nmitigating hallucination issues inherent in large language models (LLMs).\nPrevious approaches typically train retrievers based on semantic similarity,\nlacking optimization for RAG. More recent works have proposed aligning\nretrievers with the preference signals of LLMs. However, these preference\nsignals are often difficult for dense retrievers, which typically have weaker\nlanguage capabilities, to understand and learn effectively. Drawing inspiration\nfrom pedagogical theories like Guided Discovery Learning, we propose a novel\nframework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the\nlanguage capabilities of LLMs to construct examples from a more granular,\ninformation-centric perspective to guide the learning of retrievers.\nSpecifically, our method utilizes LLMs to construct easy-to-understand examples\nfrom samples where the retriever performs poorly, focusing on three learning\nobjectives highly relevant to the RAG scenario: relevance, comprehensiveness,\nand purity. These examples serve as scaffolding to ultimately align the\nretriever with the LLM's preferences. Furthermore, we employ a dual curriculum\nlearning strategy and leverage the reciprocal feedback between LLM and\nretriever to further enhance the performance of the RAG system. A series of\nexperiments demonstrate that our proposed framework enhances the performance of\nRAG systems equipped with different retrievers and is applicable to various\nLLMs.",
        "published": "2024-11-06T14:42:39Z",
        "link": "http://arxiv.org/abs/2411.03957v1",
        "authors": [
            "Yuhang Liu",
            "Xueyu Hu",
            "Shengyu Zhang",
            "Jingyuan Chen",
            "Fan Wu",
            "Fei Wu"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.03957v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0092_Fine-Grained_Guidance_for_Retrievers_Leveraging_LLMs_Feedback_in_Retrieval-Augmented_Generation.pdf"
    },
    {
        "paper_id": 93,
        "title": "RAGulator: Lightweight Out-of-Context Detectors for Grounded Text\n  Generation",
        "summary": "Real-time detection of out-of-context LLM outputs is crucial for enterprises\nlooking to safely adopt RAG applications. In this work, we train lightweight\nmodels to discriminate LLM-generated text that is semantically out-of-context\nfrom retrieved text documents. We preprocess a combination of summarisation and\nsemantic textual similarity datasets to construct training data using minimal\nresources. We find that DeBERTa is not only the best-performing model under\nthis pipeline, but it is also fast and does not require additional text\npreprocessing or feature engineering. While emerging work demonstrates that\ngenerative LLMs can also be fine-tuned and used in complex data pipelines to\nachieve state-of-the-art performance, we note that speed and resource limits\nare important considerations for on-premise deployment.",
        "published": "2024-11-06T13:51:42Z",
        "link": "http://arxiv.org/abs/2411.03920v1",
        "authors": [
            "Ian Poey",
            "Jiajun Liu",
            "Qishuai Zhong",
            "Adrien Chenailler"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.03920v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0093_RAGulator_Lightweight_Out-of-Context_Detectors_for_Grounded_Text_Generation.pdf"
    },
    {
        "paper_id": 94,
        "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge\n  Reasoning and Text Generation",
        "summary": "This study aims to optimize the existing retrieval-augmented generation model\n(RAG) by introducing a graph structure to improve the performance of the model\nin dealing with complex knowledge reasoning tasks. The traditional RAG model\nhas the problem of insufficient processing efficiency when facing complex graph\nstructure information (such as knowledge graphs, hierarchical relationships,\netc.), which affects the quality and consistency of the generated results. This\nstudy proposes a scheme to process graph structure data by combining graph\nneural network (GNN), so that the model can capture the complex relationship\nbetween entities, thereby improving the knowledge consistency and reasoning\nability of the generated text. The experiment used the Natural Questions (NQ)\ndataset and compared it with multiple existing generation models. The results\nshow that the graph-based RAG model proposed in this paper is superior to the\ntraditional generation model in terms of quality, knowledge consistency, and\nreasoning ability, especially when dealing with tasks that require\nmulti-dimensional reasoning. Through the combination of the enhancement of the\nretrieval module and the graph neural network, the model in this study can\nbetter handle complex knowledge background information and has broad potential\nvalue in multiple practical application scenarios.",
        "published": "2024-11-06T00:23:55Z",
        "link": "http://arxiv.org/abs/2411.03572v1",
        "authors": [
            "Yuxin Dong",
            "Shuo Wang",
            "Hongye Zheng",
            "Jiajing Chen",
            "Zhenhong Zhang",
            "Chihang Wang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.03572v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0094_Advanced_RAG_Models_with_Graph_Structures_Optimizing_Complex_Knowledge_Reasoning_and_Text_Generation.pdf"
    },
    {
        "paper_id": 95,
        "title": "Long Context RAG Performance of Large Language Models",
        "summary": "Retrieval Augmented Generation (RAG) has emerged as a crucial technique for\nenhancing the accuracy of Large Language Models (LLMs) by incorporating\nexternal information. With the advent of LLMs that support increasingly longer\ncontext lengths, there is a growing interest in understanding how these models\nperform in RAG scenarios. Can these new long context models improve RAG\nperformance? This paper presents a comprehensive study of the impact of\nincreased context length on RAG performance across 20 popular open source and\ncommercial LLMs. We ran RAG workflows while varying the total context length\nfrom 2,000 to 128,000 tokens (and 2 million tokens when possible) on three\ndomain-specific datasets, and report key insights on the benefits and\nlimitations of long context in RAG applications. Our findings reveal that while\nretrieving more documents can improve performance, only a handful of the most\nrecent state of the art LLMs can maintain consistent accuracy at long context\nabove 64k tokens. We also identify distinct failure modes in long context\nscenarios, suggesting areas for future research.",
        "published": "2024-11-05T22:37:43Z",
        "link": "http://arxiv.org/abs/2411.03538v1",
        "authors": [
            "Quinn Leng",
            "Jacob Portes",
            "Sam Havens",
            "Matei Zaharia",
            "Michael Carbin"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.03538v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0095_Long_Context_RAG_Performance_of_Large_Language_Models.pdf"
    },
    {
        "paper_id": 96,
        "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\n  in RAG Systems",
        "summary": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\nmajor source of external knowledge used in RAG systems, and many commercial\nsystems such as ChatGPT and Perplexity have used Web search engines as their\nmajor retrieval systems. Typically, such RAG systems retrieve search results,\ndownload HTML sources of the results, and then extract plain texts from the\nHTML sources. Plain text documents or chunks are fed into the LLMs to augment\nthe generation. However, much of the structural and semantic information\ninherent in HTML, such as headings and table structures, is lost during this\nplain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,\nwhich uses HTML instead of plain text as the format of retrieved knowledge in\nRAG. We believe HTML is better than plain text in modeling knowledge in\nexternal documents, and most LLMs possess robust capacities to understand HTML.\nHowever, utilizing HTML presents new challenges. HTML contains additional\ncontent such as tags, JavaScript, and CSS specifications, which bring extra\ninput tokens and noise to the RAG system. To address this issue, we propose\nHTML cleaning, compression, and pruning strategies, to shorten the HTML while\nminimizing the loss of information. Specifically, we design a two-step\nblock-tree-based pruning method that prunes useless HTML blocks and keeps only\nthe relevant part of the HTML. Experiments on six QA datasets confirm the\nsuperiority of using HTML in RAG systems.",
        "published": "2024-11-05T09:58:36Z",
        "link": "http://arxiv.org/abs/2411.02959v1",
        "authors": [
            "Jiejun Tan",
            "Zhicheng Dou",
            "Wen Wang",
            "Mang Wang",
            "Weipeng Chen",
            "Ji-Rong Wen"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.02959v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0096_HtmlRAG_HTML_is_Better_Than_Plain_Text_for_Modeling_Retrieved_Knowledge_in_RAG_Systems.pdf"
    },
    {
        "paper_id": 97,
        "title": "WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African\n  clean water access, sanitation and hygiene",
        "summary": "This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate\nrural African communities on clean water access, sanitation, and hygiene (WASH)\nprinciples. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach\nto address the limitations of previous approaches with limited reach or missing\ncontextualization. The paper details the development process, employing Design\nScience Research Methodology. The evaluation consisted of two phases: content\nvalidation by four WASH experts and community validation by potential users.\nContent validation confirmed WASHtsApp's ability to provide accurate and\nrelevant WASH-related information. Community validation indicated high user\nacceptance and perceived usefulness of the chatbot. The paper concludes by\ndiscussing the potential for further development, including incorporating local\nlanguages and user data analysis for targeted interventions. It also proposes\nfuture research cycles focused on wider deployment and leveraging user data for\neducational purposes.",
        "published": "2024-11-05T06:44:15Z",
        "link": "http://arxiv.org/abs/2411.02850v1",
        "authors": [
            "Simon Kloker",
            "Alex Cedric Luyima",
            "Matthew Bazanya"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.02850v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0097_WASHtsApp_--_A_RAG-powered_WhatsApp_Chatbot_for_supporting_rural_African_clean_water_access_sanitation_and_hygiene.pdf"
    },
    {
        "paper_id": 98,
        "title": "PersianRAG: A Retrieval-Augmented Generation System for Persian Language",
        "summary": "Retrieval augmented generation (RAG) models, which integrate large-scale\npre-trained generative models with external retrieval mechanisms, have shown\nsignificant success in various natural language processing (NLP) tasks.\nHowever, applying RAG models in Persian language as a low-resource language,\nposes distinct challenges. These challenges primarily involve the\npreprocessing, embedding, retrieval, prompt construction, language modeling,\nand response evaluation of the system. In this paper, we address the challenges\ntowards implementing a real-world RAG system for Persian language called\nPersianRAG. We propose novel solutions to overcome these obstacles and evaluate\nour approach using several Persian benchmark datasets. Our experimental results\ndemonstrate the capability of the PersianRAG framework to enhance question\nanswering task in Persian.",
        "published": "2024-11-05T06:11:17Z",
        "link": "http://arxiv.org/abs/2411.02832v2",
        "authors": [
            "Hossein Hosseini",
            "Mohammad Sobhan Zare",
            "Amir Hossein Mohammadi",
            "Arefeh Kazemi",
            "Zahra Zojaji",
            "Mohammad Ali Nematbakhsh"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.02832v2.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0098_PersianRAG_A_Retrieval-Augmented_Generation_System_for_Persian_Language.pdf"
    },
    {
        "paper_id": 99,
        "title": "Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare\n  Disease Knowledge",
        "summary": "Rare diseases present unique challenges in healthcare, often suffering from\ndelayed diagnosis and fragmented information landscapes. The scarcity of\nreliable knowledge in these conditions poses a distinct challenge for Large\nLanguage Models (LLMs) in supporting clinical management and delivering precise\npatient information underscoring the need for focused training on these 'zebra'\ncases. We present Zebra-Llama, a specialized context-aware language model with\nhigh precision Retrieval Augmented Generation (RAG) capability, focusing on\nEhlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000\nindividuals, exemplifies the complexities of rare diseases with its diverse\nsymptoms, multiple subtypes, and evolving diagnostic criteria. By implementing\na novel context-aware fine-tuning methodology trained on questions derived from\nmedical literature, patient experiences, and clinical resources, along with\nexpertly curated responses, Zebra-Llama demonstrates unprecedented capabilities\nin handling EDS-related queries. On a test set of real-world questions\ncollected from EDS patients and clinicians, medical experts evaluated the\nresponses generated by both models, revealing Zebra-Llama's substantial\nimprovements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.\n70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation\nreliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama\nnot only provides more accessible and reliable EDS information but also\nestablishes a framework for developing specialized AI solutions for other rare\nconditions. This work represents a crucial step towards democratizing\nexpert-level knowledge in rare disease management, potentially transforming how\nhealthcare providers and patients navigate the complex landscape of rare\ndiseases.",
        "published": "2024-11-04T22:45:52Z",
        "link": "http://arxiv.org/abs/2411.02657v1",
        "authors": [
            "Karthik Soman",
            "Andrew Langdon",
            "Catalina Villouta",
            "Chinmay Agrawal",
            "Lashaw Salta",
            "Braian Peetoom",
            "Gianmarco Bellucci",
            "Orion J Buske"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.02657v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0099_Zebra-Llama_A_Context-Aware_Large_Language_Model_for_Democratizing_Rare_Disease_Knowledge.pdf"
    },
    {
        "paper_id": 100,
        "title": "TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context\n  Support for Network",
        "summary": "The telecommunications industry's rapid evolution demands intelligent systems\ncapable of managing complex networks and adapting to emerging technologies.\nWhile large language models (LLMs) show promise in addressing these challenges,\ntheir deployment in telecom environments faces significant constraints due to\nedge device limitations and inconsistent documentation. To bridge this gap, we\npresent TeleOracle, a telecom-specialized retrieval-augmented generation (RAG)\nsystem built on the Phi-2 small language model (SLM). To improve context\nretrieval, TeleOracle employs a two-stage retriever that incorporates semantic\nchunking and hybrid keyword and semantic search. Additionally, we expand the\ncontext window during inference to enhance the model's performance on\nopen-ended queries. We also employ low-rank adaption for efficient fine-tuning.\nA thorough analysis of the model's performance indicates that our RAG framework\nis effective in aligning Phi-2 to the telecom domain in a downstream question\nand answer (QnA) task, achieving a 30% improvement in accuracy over the base\nPhi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our\nmodel not only performs on par with the much larger LLMs but also achieves a\nhigher faithfulness score, indicating higher adherence to the retrieved\ncontext.",
        "published": "2024-11-04T21:12:08Z",
        "link": "http://arxiv.org/abs/2411.02617v1",
        "authors": [
            "Nouf Alabbasi",
            "Omar Erak",
            "Omar Alhussein",
            "Ismail Lotfi",
            "Sami Muhaidat",
            "Merouane Debbah"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.02617v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0100_TeleOracle_Fine-Tuned_Retrieval-Augmented_Generation_with_Long-Context_Support_for_Network.pdf"
    },
    {
        "paper_id": 101,
        "title": "Social-RAG: Retrieving from Group Interactions to Socially Ground\n  Proactive AI Generation to Group Preferences",
        "summary": "AI agents are increasingly tasked with making proactive suggestions in online\nspaces where groups collaborate, but can be unhelpful or even annoying, due to\nnot fitting the group's preferences or behaving in socially inappropriate ways.\nFortunately, group spaces have a rich history of prior social interactions and\naffordances for social feedback to support creating agents that align to a\ngroup's interests and norms. We present Social-RAG, a workflow for grounding\nagents to social information about a group, which retrieves from prior group\ninteractions, selects relevant social signals, and then feeds the context into\na large language model to generate messages to the group. We implement this\ninto PaperPing, our system that posts academic paper recommendations in group\nchat, leveraging social signals determined from formative studies with 39\nresearchers. From a three-month deployment in 18 channels, we observed\nPaperPing posted relevant messages in groups without disrupting their existing\nsocial practices, fostering group common ground.",
        "published": "2024-11-04T18:21:53Z",
        "link": "http://arxiv.org/abs/2411.02353v1",
        "authors": [
            "Ruotong Wang",
            "Xinyi Zhou",
            "Lin Qiu",
            "Joseph Chee Chang",
            "Jonathan Bragg",
            "Amy X. Zhang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.02353v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0101_Social-RAG_Retrieving_from_Group_Interactions_to_Socially_Ground_Proactive_AI_Generation_to_Group_Preferences.pdf"
    },
    {
        "paper_id": 102,
        "title": "QCG-Rerank: Chunks Graph Rerank with Query Expansion in\n  Retrieval-Augmented LLMs for Tourism Domain",
        "summary": "Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in\nLarge Language Models (LLMs) by integrating information retrieval techniques.\nHowever, in the tourism domain, since the query is usually brief and the\ncontent in the database is diverse, existing RAG may contain a significant\namount of irrelevant or contradictory information contents after retrieval. To\naddress this challenge, we propose the QCG-Rerank model. This model first\nperforms an initial retrieval to obtain candidate chunks and then enhances\nsemantics by extracting critical information to expand the original query.\nNext, we utilize the expanded query and candidate chunks to calculate\nsimilarity scores as the initial transition probability and construct the\nchunks graph. Subsequently, We iteratively compute the transition probabilities\nbased on an initial estimate until convergence. The chunks with the highest\nscore are selected and input into the LLMs to generate responses. We evaluate\nthe model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets.\nThe experimental results demonstrate the effectiveness and superiority of the\nQCG-Rerank method.",
        "published": "2024-11-04T08:15:22Z",
        "link": "http://arxiv.org/abs/2411.08724v1",
        "authors": [
            "Qikai Wei",
            "Mingzhi Yang",
            "Chunlong Han",
            "Jingfu Wei",
            "Minghao Zhang",
            "Feifei Shi",
            "Huansheng Ning"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.08724v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0102_QCG-Rerank_Chunks_Graph_Rerank_with_Query_Expansion_in_Retrieval-Augmented_LLMs_for_Tourism_Domain.pdf"
    },
    {
        "paper_id": 103,
        "title": "Can Language Models Enable In-Context Database?",
        "summary": "Large language models (LLMs) are emerging as few-shot learners capable of\nhandling a variety of tasks, including comprehension, planning, reasoning,\nquestion answering, arithmetic calculations, and more. At the core of these\ncapabilities is LLMs' proficiency in representing and understanding structural\nor semi-structural data, such as tables and graphs. Numerous studies have\ndemonstrated that reasoning on tabular data or graphs is not only feasible for\nLLMs but also gives a promising research direction which treats these data as\nin-context data. The lightweight and human readable characteristics of\nin-context database can potentially make it an alternative for the traditional\ndatabase in typical RAG (Retrieval Augmented Generation) settings. However,\nalmost all current work focuses on static in-context data, which does not allow\ndynamic update. In this paper, to enable dynamic database update, delta\nencoding of database is proposed. We explore how data stored in traditional\nRDBMS can be encoded as in-context text and evaluate LLMs' proficiency for CRUD\n(Create, Read, Update and Delete) operations on in-context databases. A\nbenchmark named InConDB is presented and extensive experiments are conducted to\nshow the performance of different language models in enabling in-context\ndatabase by varying the database encoding method, prompting method, operation\ntype and input data distribution, revealing both the proficiency and\nlimitations.",
        "published": "2024-11-04T05:25:39Z",
        "link": "http://arxiv.org/abs/2411.01807v1",
        "authors": [
            "Yu Pan",
            "Hongfeng Yu",
            "Tianjiao Zhao",
            "Jianxin Sun"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.01807v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0103_Can_Language_Models_Enable_In-Context_Database.pdf"
    },
    {
        "paper_id": 104,
        "title": "RAGViz: Diagnose and Visualize Retrieval-Augmented Generation",
        "summary": "Retrieval-augmented generation (RAG) combines knowledge from domain-specific\nsources into large language models to ground answer generation. Current RAG\nsystems lack customizable visibility on the context documents and the model's\nattentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool\nthat visualizes the attentiveness of the generated tokens in retrieved\ndocuments. With a built-in user interface, retrieval index, and Large Language\nModel (LLM) backbone, RAGViz provides two main functionalities: (1) token and\ndocument-level attention visualization, and (2) generation comparison upon\ncontext document addition and removal. As an open-source toolkit, RAGViz can be\neasily hosted with a custom embedding model and HuggingFace-supported LLM\nbackbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,\nmemory-efficient LLM inference tool, and custom context snippet method, RAGViz\noperates efficiently with a median query time of about 5 seconds on a moderate\nGPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo\nvideo of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.",
        "published": "2024-11-04T02:30:05Z",
        "link": "http://arxiv.org/abs/2411.01751v1",
        "authors": [
            "Tevin Wang",
            "Jingyuan He",
            "Chenyan Xiong"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.01751v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0104_RAGViz_Diagnose_and_Visualize_Retrieval-Augmented_Generation.pdf"
    },
    {
        "paper_id": 105,
        "title": "RuAG: Learned-rule-augmented Generation for Large Language Models",
        "summary": "In-context learning (ICL) and Retrieval-Augmented Generation (RAG) have\ngained attention for their ability to enhance LLMs' reasoning by incorporating\nexternal knowledge but suffer from limited contextual window size, leading to\ninsufficient information injection. To this end, we propose a novel framework,\nRuAG, to automatically distill large volumes of offline data into interpretable\nfirst-order logic rules, which are injected into LLMs to boost their reasoning\ncapabilities. Our method begins by formulating the search process relying on\nLLMs' commonsense, where LLMs automatically define head and body predicates.\nThen, RuAG applies Monte Carlo Tree Search (MCTS) to address the combinational\nsearching space and efficiently discover logic rules from data. The resulting\nlogic rules are translated into natural language, allowing targeted knowledge\ninjection and seamless integration into LLM prompts for LLM's downstream task\nreasoning. We evaluate our framework on public and private industrial tasks,\nincluding natural language processing, time-series, decision-making, and\nindustrial tasks, demonstrating its effectiveness in enhancing LLM's capability\nover diverse tasks.",
        "published": "2024-11-04T00:01:34Z",
        "link": "http://arxiv.org/abs/2411.03349v1",
        "authors": [
            "Yudi Zhang",
            "Pei Xiao",
            "Lu Wang",
            "Chaoyun Zhang",
            "Meng Fang",
            "Yali Du",
            "Yevgeniy Puzyrev",
            "Randolph Yao",
            "Si Qin",
            "Qingwei Lin",
            "Mykola Pechenizkiy",
            "Dongmei Zhang",
            "Saravan Rajmohan",
            "Qi Zhang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.03349v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0105_RuAG_Learned-rule-augmented_Generation_for_Large_Language_Models.pdf"
    },
    {
        "paper_id": 106,
        "title": "Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors",
        "summary": "Despite significant advancements, large language models (LLMs) still struggle\nwith providing accurate answers when lacking domain-specific or up-to-date\nknowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by\nincorporating external knowledge bases, but it also introduces new attack\nsurfaces. In this paper, we investigate data extraction attacks targeting the\nknowledge databases of RAG systems. We demonstrate that previous attacks on RAG\nlargely depend on the instruction-following capabilities of LLMs, and that\nsimple fine-tuning can reduce the success rate of such attacks to nearly zero.\nThis makes these attacks impractical since fine-tuning is a common practice\nwhen deploying LLMs in specific domains. To further reveal the vulnerability,\nwe propose to backdoor RAG, where a small portion of poisoned data is injected\nduring the fine-tuning phase to create a backdoor within the LLM. When this\ncompromised LLM is integrated into a RAG system, attackers can exploit specific\ntriggers in prompts to manipulate the LLM to leak documents from the retrieval\ndatabase. By carefully designing the poisoned data, we achieve both verbatim\nand paraphrased document extraction. We show that with only 3\\% poisoned data,\nour method achieves an average success rate of 79.7\\% in verbatim extraction on\nLlama2-7B, with a ROUGE-L score of 64.21, and a 68.6\\% average success rate in\nparaphrased extraction, with an average ROUGE score of 52.6 across four\ndatasets. These results underscore the privacy risks associated with the supply\nchain when deploying RAG systems.",
        "published": "2024-11-03T22:27:40Z",
        "link": "http://arxiv.org/abs/2411.01705v1",
        "authors": [
            "Yuefeng Peng",
            "Junda Wang",
            "Hong Yu",
            "Amir Houmansadr"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.01705v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0106_Data_Extraction_Attacks_in_Retrieval-Augmented_Generation_via_Backdoors.pdf"
    },
    {
        "paper_id": 107,
        "title": "AttackQA: Development and Adoption of a Dataset for Assisting\n  Cybersecurity Operations using Fine-tuned and Open-Source LLMs",
        "summary": "Retrieval-augmented generation (RAG) on specialized domain datasets has shown\nimproved performance when large language models (LLMs) are fine-tuned for\ngenerating responses to user queries. In this study, we develop a cybersecurity\nquestion-answering (Q\\&A) dataset, called AttackQA, and employ it to build a\nRAG-based Q\\&A system designed for analysts in security operations centers. The\ndataset comprises 25,335 Q\\&A pairs, accompanied by rationales to facilitate\nfine-tuning and evaluation. 80\\% of the dataset was generated with help of a\nlightweight open-source LLM (LLama 3 8B), which produced over 1100 tokens per\nsecond with full 16-bit precision on SambaNova System's SN40L specialized\nhardware. To ensure dataset quality, we fine-tuned LLama 3 70B to detect and\nreject low-quality Q\\&A pairs. In using the dataset for RAG, we demonstrate\nthat fine-tuning open-source embeddings and LLMs can yield superior accuracy\ncompared to OpenAI's state-of-the-art proprietary embedding and LLM (GPT-4o).\nFurthermore, we use Llama 3.1 405B as a judge to evaluate answer correctness,\nenabling the creation of a fully open-source, high-speed RAG and evaluation\npipeline with a benchmark for model accuracy.",
        "published": "2024-11-01T23:03:40Z",
        "link": "http://arxiv.org/abs/2411.01073v1",
        "authors": [
            "Varun Badrinath Krishna"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.01073v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0107_AttackQA_Development_and_Adoption_of_a_Dataset_for_Assisting_Cybersecurity_Operations_using_Fine-tuned_and_Open-Source_LLMs.pdf"
    },
    {
        "paper_id": 108,
        "title": "Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM\n  Generation Output",
        "summary": "We present a light-weight approach for detecting nonfactual outputs from\nretrieval-augmented generation (RAG). Given a context and putative output, we\ncompute a factuality score that can be thresholded to yield a binary decision\nto check the results of LLM-based question-answering, summarization, or other\nsystems. Unlike factuality checkers that themselves rely on LLMs, we use\ncompact, open-source natural language inference (NLI) models that yield a\nfreely accessible solution with low latency and low cost at run-time, and no\nneed for LLM fine-tuning. The approach also enables downstream mitigation and\ncorrection of hallucinations, by tracing them back to specific context chunks.\nOur experiments show high area under the ROC curve (AUC) across a wide range of\nrelevant open source datasets, indicating the effectiveness of our method for\nfact-checking RAG output.",
        "published": "2024-11-01T20:44:59Z",
        "link": "http://arxiv.org/abs/2411.01022v1",
        "authors": [
            "Hithesh Sankararaman",
            "Mohammed Nasheed Yasin",
            "Tanner Sorensen",
            "Alessandro Di Bari",
            "Andreas Stolcke"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.01022v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0108_Provenance_A_Light-weight_Fact-checker_for_Retrieval_Augmented_LLM_Generation_Output.pdf"
    },
    {
        "paper_id": 109,
        "title": "CORAG: A Cost-Constrained Retrieval Optimization System for\n  Retrieval-Augmented Generation",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable generation\ncapabilities but often struggle to access up-to-date information, which can\nlead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this\nissue by incorporating knowledge from external databases, enabling more\naccurate and relevant responses. Due to the context window constraints of LLMs,\nit is impractical to input the entire external database context directly into\nthe model. Instead, only the most relevant information, referred to as chunks,\nis selectively retrieved. However, current RAG research faces three key\nchallenges. First, existing solutions often select each chunk independently,\noverlooking potential correlations among them. Second, in practice the utility\nof chunks is non-monotonic, meaning that adding more chunks can decrease\noverall utility. Traditional methods emphasize maximizing the number of\nincluded chunks, which can inadvertently compromise performance. Third, each\ntype of user query possesses unique characteristics that require tailored\nhandling, an aspect that current approaches do not fully consider. To overcome\nthese challenges, we propose a cost constrained retrieval optimization system\nCORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search\n(MCTS) based policy framework to find optimal chunk combinations sequentially,\nallowing for a comprehensive consideration of correlations among chunks.\nAdditionally, rather than viewing budget exhaustion as a termination condition,\nwe integrate budget constraints into the optimization of chunk combinations,\neffectively addressing the non-monotonicity of chunk utility.",
        "published": "2024-11-01T17:11:16Z",
        "link": "http://arxiv.org/abs/2411.00744v1",
        "authors": [
            "Ziting Wang",
            "Haitao Yuan",
            "Wei Dong",
            "Gao Cong",
            "Feifei Li"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.00744v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0109_CORAG_A_Cost-Constrained_Retrieval_Optimization_System_for_Retrieval-Augmented_Generation.pdf"
    },
    {
        "paper_id": 110,
        "title": "Towards Multi-Source Retrieval-Augmented Generation via Synergizing\n  Reasoning and Preference-Driven Retrieval",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a reliable external\nknowledge augmentation technique to mitigate hallucination issues and\nparameterized knowledge limitations in Large Language Models (LLMs). Existing\nAdaptive RAG (ARAG) systems struggle to effectively explore multiple retrieval\nsources due to their inability to select the right source at the right time. To\naddress this, we propose a multi-source ARAG framework, termed MSPR, which\nsynergizes reasoning and preference-driven retrieval to adaptive decide \"when\nand what to retrieve\" and \"which retrieval source to use\". To better adapt to\nretrieval sources of differing characteristics, we also employ retrieval action\nadjustment and answer feedback strategy. They enable our framework to fully\nexplore the high-quality primary source while supplementing it with secondary\nsources at the right time. Extensive and multi-dimensional experiments\nconducted on three datasets demonstrate the superiority and effectiveness of\nMSPR.",
        "published": "2024-11-01T15:50:58Z",
        "link": "http://arxiv.org/abs/2411.00689v1",
        "authors": [
            "Qingfei Zhao",
            "Ruobing Wang",
            "Xin Wang",
            "Daren Zha",
            "Nan Mu"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.00689v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0110_Towards_Multi-Source_Retrieval-Augmented_Generation_via_Synergizing_Reasoning_and_Preference-Driven_Retrieval.pdf"
    },
    {
        "paper_id": 111,
        "title": "Rationale-Guided Retrieval Augmented Generation for Medical Question\n  Answering",
        "summary": "Large language models (LLM) hold significant potential for applications in\nbiomedicine, but they struggle with hallucinations and outdated knowledge.\nWhile retrieval-augmented generation (RAG) is generally employed to address\nthese issues, it also has its own set of challenges: (1) LLMs are vulnerable to\nirrelevant or incorrect context, (2) medical queries are often not\nwell-targeted for helpful information, and (3) retrievers are prone to bias\ntoward the specific source corpus they were trained on. In this study, we\npresent RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the\nreliability of RAG in biomedical contexts. RAG$^2$ incorporates three key\ninnovations: a small filtering model trained on perplexity-based labels of\nrationales, which selectively augments informative snippets of documents while\nfiltering out distractors; LLM-generated rationales as queries to improve the\nutility of retrieved snippets; a structure designed to retrieve snippets evenly\nfrom a comprehensive set of four biomedical corpora, effectively mitigating\nretriever bias. Our experiments demonstrate that RAG$^2$ improves the\nstate-of-the-art LLMs of varying sizes, with improvements of up to 6.1\\%, and\nit outperforms the previous best medical RAG model by up to 5.6\\% across three\nmedical question-answering benchmarks. Our code is available at\nhttps://github.com/dmis-lab/RAG2.",
        "published": "2024-11-01T01:40:23Z",
        "link": "http://arxiv.org/abs/2411.00300v1",
        "authors": [
            "Jiwoong Sohn",
            "Yein Park",
            "Chanwoong Yoon",
            "Sihyeon Park",
            "Hyeon Hwang",
            "Mujeen Sung",
            "Hyunjae Kim",
            "Jaewoo Kang"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.00300v1.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0111_Rationale-Guided_Retrieval_Augmented_Generation_for_Medical_Question_Answering.pdf"
    },
    {
        "paper_id": 112,
        "title": "LLM-Ref: Enhancing Reference Handling in Technical Writing with Large\n  Language Models",
        "summary": "Large Language Models (LLMs) excel in data synthesis but can be inaccurate in\ndomain-specific tasks, which retrieval-augmented generation (RAG) systems\naddress by leveraging user-provided data. However, RAGs require optimization in\nboth retrieval and generation stages, which can affect output quality. In this\npaper, we present LLM-Ref, a writing assistant tool that aids researchers in\nwriting articles from multiple source documents with enhanced reference\nsynthesis and handling capabilities. Unlike traditional RAG systems that use\nchunking and indexing, our tool retrieves and generates content directly from\ntext paragraphs. This method facilitates direct reference extraction from the\ngenerated outputs, a feature unique to our tool. Additionally, our tool employs\niterative response generation, effectively managing lengthy contexts within the\nlanguage model's constraints. Compared to baseline RAG-based systems, our\napproach achieves a $3.25\\times$ to $6.26\\times$ increase in Ragas score, a\ncomprehensive metric that provides a holistic view of a RAG system's ability to\nproduce accurate, relevant, and contextually appropriate responses. This\nimprovement shows our method enhances the accuracy and contextual relevance of\nwriting assistance tools.",
        "published": "2024-11-01T01:11:58Z",
        "link": "http://arxiv.org/abs/2411.00294v2",
        "authors": [
            "Kazi Ahmed Asif Fuad",
            "Lizhong Chen"
        ],
        "pdf_link": "http://arxiv.org/pdf/2411.00294v2.pdf",
        "pdf_path": "/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0112_LLM-Ref_Enhancing_Reference_Handling_in_Technical_Writing_with_Large_Language_Models.pdf"
    }
]