{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries, Set keys, Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.api.arxiv import load_data\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "from google.generativeai import GenerativeModel\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "# Count chunks per paper\n",
    "from collections import Counter\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from data/RAG/text/top_5_rag_papers.json\n"
     ]
    }
   ],
   "source": [
    "top_5_rag_papers = load_data(file_path=\"data/RAG/text/top_5_rag_papers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'text': 'Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation Robin D. Pesl1, Jerin G. Mathew2, Massimo Mecella2, and Marco Aiello1 1University of Stuttgart, Stuttgart, Germany 2Sapienza Universit di Roma, Rome, Italy Abstract Integrating multiple (sub-)systems is essential to create ad- vanced Information Systems (ISs). Difficulties mainly arise when integrat- ing dynamic environments across the IS lifecycle, e.g., services not yet existent at design time. A traditional approach is a registry that provides the API documentation of the systems endpoints. Large Language Mod- els (LLMs) have shown to be capable of automatically creating system integrations (e.g., as service composition) based on this documentation but require concise input due to input token limitations, especially regard- ing comprehensive API descriptions. Currently, it is unknown how best to preprocess these API descriptions. Within this work, we (i) analyze the usage of Retrieval Augmented Generation (RAG) for endpoint discovery and the chunking, i.e., preprocessing, of state-of-practice OpenAPIs to reduce the input token length while preserving the most relevant informa- tion. To further reduce the input token length for the composition prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that only receives a summary of the most relevant endpoints and retrieves specification details on demand. We evaluate RAG for endpoint discovery using the RestBench benchmark, first, for the different chunking possibil- ities and parameters measuring the endpoint retrieval recall, precision, and F1 score. Then, we assess the Discovery Agent using the same test set. With our prototype, we demonstrate how to successfully employ RAG for endpoint discovery to reduce the token count. While revealing high values for recall, precision, and F1, further research is necessary to retrieve all requisite endpoints. Our experiments show that for preprocessing, LLM-based and format-specific approaches outperform na ve chunking methods. Relying on an agent further enhances these results as the agent splits the tasks into multiple fine granular subtasks, improving the overall RAG performance in the token count, precision, and F1 score. Keywords: Retrieval augmented generation Large language models OpenAPI Endpoint discovery RestBench. OpenAPI is the state-of-practice for describing interfaces for integrating Informa- tion Systems (ISs). It contains formal elements like paths and natural language 29 Nov 20242 Pesl et al. constituents like descriptions. For integrating these systems automatically, au- tomated service composition using Large Language Models (LLMs) has been proposed [ 27,28,29]. These approaches exploit the capabilities of LLMs to process formal and natural language input, combining them with the inherent nature of automated service composition of decoupling and independent lifecycle manage- ment. While prohibiting any manual modeling effort by relying on already broadly available OpenAPIs, the approaches face the challenge of limited input token length [29]. This bounds the quantity and extent of the input service description. Even for proprietary models with a large input token context, e.g., OpenAIs GPT4 with a context size of 128,000 tokens [ 23], an economic constraint emerges as these models are paid by input and output token count. Therefore, a smaller prompt length is beneficial to (i) insert further service documentation and (ii) reduce costs for proprietary models. To address these challenges, Retrieval Augmented Generation (RAG) [ 16] has emerged as a promising resort. In this approach, the external information is collected in a database, typically structured as a set of documents or document chunks. The primary goal is retrieving only a small subset of the most relevant documents or document chunks, which is then inserted into the prompt [ 16]. How to optimally apply RAG for endpoint discovery in IS is open to investigation, leading to the following research questions: RQ1.How best to preprocess, i.e., chunk, OpenAPIs for RAG endpoint discovery? RQ2.Can LLM agents be employed to reduce token count further and improve To answer RQ1, we develop an OpenAPI RAG system that takes as input service descriptions. We apply different token-based and LLM-based chunking strategies to split the documentation and evaluate them based on retrieval quality. The token-based strategies process the document using a classical parser and then split the parts, e.g., endpoints, into equal-sized chunks. The LLM-based strategies let an LLM create a description, i.e., a summary or a question, for each endpoint and then use these descriptions for similarity matching. We employ mainstream open-source and proprietary embedding models for similarity matching, which can create an embedding vector for an input. The similarity between two inputs can then be determined by comparing their embedding vectors using, e.g., the cosine similarity. We evaluate the OpenAPI RAG and the different chunking strategies by relying on the already available RestBench benchmark [ 35] for LLMs agents, measuring recall, precision, and F1 score for each chunking strategy. The benchmark consists of the OpenAPI descriptions of Spotify and TMDB and corresponding queries, each with a set of endpoints as the sample solution. To address RQ2, we propose an LLM agent called Discovery Agent . As LLM agents allow the usage of external tools, we first investigate using one tool that simply inputs the results of the RAG to the prompt. Then, we experiment with using two tools: the first tool filters and enters the LLM endpoint summaries to the prompt using RAG, while the second tool allows the retrieval of the endpoint details on demand. We resort to the same RestBench benchmark for evaluation and measure recall, precision, F1 score, and additional token count.Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 3 As the chunking strategy, we rely on the LLM-based summary strategy with OpenAIs text-embedding-3-large embedding model [24]. The remainder of the paper is structured as follows. First, we provide an overview of related works regarding service discovery and LLMs in Section 2. Then, we present how to use RAG for endpoint discovery and the OpenAPI chunking strategies in Section 3. We evaluate and discuss the RAG and the different chunking strategies in Section 4. Finally, we conclude with Section 5. Regarding endpointdiscovery,we provide abrief overview ofthe essential concepts of the various service discovery approaches. Additionally, we provide relevant insights into LLMs and the novel approach of integrating LLMs with tools, known as LLM agents, and how they relate to our approach. The most common service discovery implementation is a service registry, which collects information about available services and offers search facilities. This service registry is usually backed by a component residing at the middleware or application level [ 15]. It is characterized by the syntax used to describe the services and their invocation and the expressive power of the available query language. The typical integration model is a pull model where service consumers search for the required services. Less is a push model as used in the UPnP protocol, where service providers regularly advertise their services [32]. In the early days of XML-based services, the infrastructure for service discov- ery was the Universal Description, Discovery, and Integration (UDDI) specifi- cation [7]. UDDI had a global incarnation called the UDDI Business Registry (UBR), intended to offer an Internet-wide repository of available web services and promoted by IBM, Microsoft, and SAP. Unfortunately, UBR never gained widespread adoption and was short-lived (2000-2006). Significant research in the early days focused on enhancing service discovery on UDDI, improving search capabilities, and creating federated registries, e.g., [ 3,4,11]. Alternatively, WS-Discovery is a multicast protocol that finds web services on a local network. Nowadays, OpenAPI is the de facto standard for describing services. While not offering a discovery protocol and mechanism, given its popularity, OpenAPI would also benefit from discovery [ 34]. So, additional infrastructure for discovery has been proposed, such as centralized repositories (SwaggerHub or Apiary), service registry integration (Consul, Eureka), API Gateways (Kong, Apigee), or Populating registries of services requires effort from service providers, which often hinders the success of such approaches, especially if the service provider is expected to provide extensive additional information beyond the service endpoints. This additional effort has often been the reason for the failure of some of these technologies, most notably UBR. Approaches confined to specific applications,4 Pesl et al. domains, or enterprises have been more successful, e.g., Eureka. Developed by Netflix as part of its microservices architecture [ 36], Eureka helps clients find service instances described by host IP, port, health indicator URL, and home page. Developers can add additional data to the registry for extra use cases. extensive modeling, e.g., as semantic annotations. Hence, our approach relies on already broadly available state-of-practice OpenAPI specification and their 2.2 Large Language Models (NLP) and machine learning field [ 1,2,13]. Often containing billions of parameters, these models are trained on extensive text corpora to generate and manipulate human-like text [ 30]. They are primarily based on an encoder-decoder architecture tasks using decoder-only models such as GPT [ 31]. Usually, the input is a natural language task called prompt, which first needs to be translated to a sequence of input tokens. The model processes this prompt and returns an output token sequence, which can then be translated back to a natural language answer. As these models can, in general, capture intricate linguistic nuances and semantic contexts, they can be applied to a wide range of tasks, e.g., in software engineering [ 10]. LLMs can be used to create integration based on endpoint documentation automatically [ 27,28,29]. Yet, these face strict input token limitations, e.g., 128,000 tokens for current OpenAI models [23,29]. With this paper, we analyze how RAG can be used to preprocess API documentation to mitigate this issue. Another approach is encoder-only models such as BERT [ 8], often referred to as embedding models. They allow condensing the contextual meaning of a text into a dense vector, termed embedding. Using similarity metrics such as dot product, cosine similarity, or Euclidean distance allows for assessing the similarity of two input texts. Embedding models are usually used for the similarity search in RAG systems [6], which we also do in our implementation. LLMs have shown remarkable capabilities in solving complex tasks by decompos- ing them in a step-by-step fashion [ 38] or by exploring multiple solution paths simultaneously [ 41]. Typically, these plans are generated iteratively by using the history of the previously generated steps to guide the generation of the next step. Additionally, recent studies have shown the potential of providing LLMs access to external tools to boost their reasoning capabilities and add further knowledge. This approach consists of prompting the LLM to interact with external tools to solve tasks, thus offloading computations from the LLM to specialized functions. Notable examples of such tools include web browsers [ 20], calculators [ 5], andAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 5 Python interpreters [ 12]. In practice, a tool is usually a Python function, which can be called during the interaction with the LLM. The LLM agent paradigm [ 19,22,40] combines the concepts of (i) external tool usage, (ii) the planning capabilities of LLMs, and adds a shared (iii) memory, to solve complex tasks. Given an input task, an LLM agent uses its reasoning capabilities to decompose the task into a set of simpler subtasks. For each subtask, the LLM finds and interacts with the set of tools to solve the subtask. Then, based on the outcome of the current task and the history of previously executed subtasks, the LLM agent generates a new subtask and repeats the steps mentioned above or terminates if the original task is solved. To instruct the processing, the outcome of the tool invocations and the history of the subtasks are stored in the memory, typically consisting in the LLM agents own context. Within this work, we apply the LLM agent paradigm to create the Discovery Agent as an LLM agent for endpoint discovery. A critical challenge for LLM agents is the accessibility to a set of common APIs and tasks for their evaluation, e.g., tested using benchmarks like API Bank [17] or RestBench [ 35]. API Bank is a benchmark to evaluate the tool use of an LLM consisting of a set of APIs exposed through a search engine. Unfortunately, the available code of the benchmark is incomplete. The RestBench benchmark contains a collection of tasks and endpoints expressed using the OpenAPI specification of Spotify and TMDB [ 35]. As the currently most extensive available benchmark, we employ RestBench to validate our results. OpenAPIs within LLM agents have been used in RestGPT [ 35] and Chain of Tools [ 33]. The former combines multiple LLM agents to solve complex tasks by interacting with a set of tools exposed using the OpenAPI specification. The latter solves an input query by framing the problem as a code generation task and interacts with the set of tools to generate Python code to solve the query. In contrast, our Discovery Agent does not directly interact with the endpoints stated in the OpenAPIs. Instead, it filters and returns matching endpoints that can be used for subsequent processing. Even when considering the similarity to the tool selection within LLM agents, the task of selecting a set of tools from a larger pool to solve a specific problem remains relatively underexplored [ 42]. Existing research primarily focuses on the a priori selection of human-curated tools [ 25], heuristic-based methods for tool selection [ 18], choosing the relevant tool by scoring each query against every tool based semantic retrieval using a combination of different vector databases [ 42]. With our work, we contribute the analysis of preprocessing OpenAPIs into this We first introduce the general architecture to employ RAG for endpoint discovery. As state-of-practice for service documentation, we then investigate how to chunk OpenAPIs as preprocessing for RAG.6 Pesl et al. 3.1 RAG for Endpoint Discovery RAG comprises a preprocessing step ahead of the answer generation of an LLM to a semantic search based on some knowledge sources. Usually, the semantic search is done by embedding similarity, and the data from the knowledge sources is reduced to small chunks to allow fine-grained information retrieval [16]. Figure1. RAG for Endpoint Discovery Figure 1 depicts the application of RAG for endpoint discovery, i.e., the OpenAPI RAG. Initially, the chunking strategy determines how the chunks are created from the OpenAPIs, i.e., how many chunks are created and what they contain. Each chunk has an embedding as metadata for similarity search in addition to its content. The chunking strategy specifies which data is used as input to the embedding model to create the embedding. This input does not have to match the chunk content, e.g., it can be a summary instead of the entire content. The chunks are finally stored in the chunk database. For retrieval, the user submits in 1a natural language query qto the chunk retriever, which converts qinto the embedding eusing the same embedding model as for the chunk creation. In 2, the chunk retriever queries the chunk database using e. The chunk database compares eusing a similarity metric with the embeddings of the service chunks contained in the database. The results are the top kmost similar chunks according to the metric, which are then returned to the chunk retriever in 3. Finally, in 4, the chunk retriever forwards the retrieved results to the user, who can add them to their prompt either manually or automatically through integration into their tooling. The benefit of employing RAG is the insertion of only the gist of the available information, which allows picking more and only the most relevant information for the fix LLM context size. A drawback is that, based on the retrieval algorithm, not all relevant information may be retrieved. Further, fixing kreveals the advantage of controlling the result size. An alternative would be to return all chunks about a certain similarity threshold, introducing the question about the optimal cutoff. Figure 2 shows how the Discovery Agent extends on the RAG from Figure 1 shown in yellow hued. Instead of passing qto the RAG, the user submits it in1to the Discovery Agent, which then iteratively decomposes qinto a set of fine-grained tasks in 2. Breaking down the query into smaller, more manageable tasks can potentially fill the gap between the coarse semantics of the query and the specificities in the services documentation. In 3, the Discovery AgentAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 7 Figure2. Overview of the Discovery Agent Approach for Endpoint Discovery submits each task to the RAG to retrieve the set of relevant chunks to solve the current task specifically. Finally, in 4, the Discovery Agent collects the retrieval results of each individual task, filters them, and repeats 2ifqneeds further processing or returns the results to the user in 5. 3.2 OpenAPI Chunking Strategies A critical step in the RAG workflow is creating the chunks for the chunk database. registries can contain tens of thousands of services, each containing multiple potentially lengthy endpoints due to detailed descriptions or extensive input and output schemas. So, a single service might not fit into the context size of the embedding model or even exceed the limit of the LLM that further processes the output of the RAG system. In addition, service documentation can also feature additional metadata that, while valuable for understanding service details, is not necessarily relevant for composing services to solve a query. Table 1. Implemented Chunking Strategies Category Splitting Refinement Meta-Parameters Token-based No split Token chunking m(model), s(chunk size), l(overlap) Endpoint split Token chunking m(model), s(chunk size), l(overlap) Endpoint split Remove examples m(model) Endpoint split Relevant fields m(model) JSON split Token chunking m(model), s(chunk size), l(overlap) LLM-based Endpoint split Query m(model) Endpoint split Summary m(model) To determine advantageous chunking strategies, we employ the seven different and output is a list of chunks. The chunking strategies can be categorized into token-based andLLM-based strategies. Each strategy consists of a splitting method, which dissects the OpenAPI specification into a list of intermediate chunks, and another refinement step, which converts the intermediate chunks to the final list of chunks. In addition, there is the meta-parameter for the used embedding model m. For the token chunking refinement step, there is also the8 Pesl et al. chunk size sin tokens and their overlap l, i.e., how many tokens two consecutive chunks share, in tokens. For the token-based approaches, we consider three main splitting methods. Theno split method returns a single intermediate chunk for each OpenAPI containing the whole specification. The endpoint split divides the OpenAPI into one chunk per endpoint. The JSON split is a built-in LlamaIndex3splitting strategy tailored to files in the JSON format. This strategy parses the JSON file and traverses it using depth-first search, collecting leaf nodes, i.e., key-value pair where the value is a primitive type, e.g., strings, numbers, etc.. During this traversal, the parser concatenates keys and values into single lines of text to create a comprehensive textual representation of each leaf node. For the refinement, we implemented token chunking ,remove example , and relevant field . Thetoken chunking splits each intermediate chunk into a list of fixed-size chunks of stokens respecting an overlap of ltokens with the previous node. The remove example removes the requestBody and recursively all examples fields for each endpoint as these are typically lengthy but contribute little information. The relevant field extracts representative fields, i.e., service title, service description, endpoint verb, endpoint path, and endpoint description, which contribute much information but few tokens. For the LLM-based processing strategies, we apply the endpoint split and a summary (similar to [ 21]) andqueryapproach for refinement. In the summary approach, we prompt an LLM to generate a summary for each OpenAPI endpoint. For thequeryapproach, we instruct the LLM to generate a possible query matching the OpenAPI endpoint, as this might be closer to a possible input query than the summary. For both approaches, we only consider the LLM output for the embedding creation. The chunk content remains the original OpenAPI endpoint information. The no split and JSON split splitting methods can only be used with token chunking since all other refinement strategies rely on exactly one endpoint as an intermediate chunk. To evaluate the OpenAPI RAG and the Discovery Agent, we implement it as a fully operational prototype. Then, we employ the RestBench [ 35] benchmark to validate it in a real-world setting. We implement the OpenAPI RAG and Discovery Agent approaches as open- source prototypes based on the LlamaIndex library.3For the prototypes, we rely solely on OpenAPIs as the state-of-practice for service descriptions. All sources and results are available online.4 4https://doi.org/10.18419/darus-4605Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 9 For the OpenAPI RAG, we focus on the components presented in Figure 1. At the first start, the system loads the OpenAPIs and applies a chunking strategy to create chunks and their embeddings for their later retrieval. The chunks contain thereby the information from the OpenAPIs, e.g., a whole endpoint or a part of it. A chunk embedding does not necessarily have to match the chunks content; for example, the content can be the endpoint, and the embedding is created using a natural language summary of the endpoint. Thus, the matching is performed based on the embedding, and the result returned is the chunks content, which can include additional information not required for the matching process. As the service database, we use FAISS, which allows the storage and the similarity search of chunks [ 9]. We use a so-called QueryEngine from LlamaIndex for the chunk retriever, which allows us to query a chunk database based on textual We realize the Discovery Agent from Figure 2 using a LlamaIndex Ope- nAIAgent, which implements the LLM agent pattern for OpenAIs LLMs. An OpenAIAgent takes a list of tools, i.e., Python functions with a name and a description as parameters, and interacts with these using the OpenAI API. We implement two strategies for the tools. In the first strategy ( Query), we use the OpenAPI RAG as input for a LlamaIndex QueryEngineTool, which allows the agent to interact with the RAG on demand. This has the advantage of being a simple, straightforward implementation but may increase the token count as the results of the RAG are fed into the chat history, which is transferred to the LLM for the reasoning on this data. The second strategy ( Summary ) uses a RAG with chunks of the endpoints verb, path, and summary as contents and for their embeddings. We create the summary by instructing an LLM to create it based on the endpoint information, i.e., as in the summary chunking strategy. This should reduce the token count, as the chunks are much smaller, as not all endpoint details are returned and processed. To account for the same function as the first approach with the OpenAPI RAG and provide all information, we introduce a second tool, which takes the endpoint verb and path as input parameters and returns the whole endpoint information. The complete data is only inserted into the history for indispensable endpoints. To enable measuring the retrieved endpoints, we attach the endpoint infor- mation, i.e., verb and path, to each chunk as metadata. For the endpoint split splitting strategies, we take the information from the endpoint. For the other strategies, we first attach a list of all endpoints to the nodes before splitting and then filter on the endpoint paths in the final chunks after splitting. So, for each chunk, we know to which endpoint or endpoints it relates to. 4.2 Dataset and Metrics We evaluate our approach using the RestBench benchmark, covering the Spotify and TMDB OpenAPI specifications [ 35]. With 40 endpoints for Spotify and 54 for TMDB, this benchmark is much more complex than usual Service-Oriented Com- puting (SOC) case studies containing usually just three to seven endpoints [28].10 Pesl et al. Nevertheless, a holistic benchmark covering various domains is still missing (see RestBench contains 57 queries for Spotify and 100 for TMDB. For each of these queries, a solution set of endpoints is given, i.e., one to four endpoints that must be called to fulfill the query. For example, one query is Who directed the top-1 rated movie? The solution contains the endpoints GET /movie/top_rated As embedding models, we employ OpenAIs text-embedding-3-large [24] as one of the currently leading proprietary models. As open-source models, we utilize BAAI/ bge-small-en-v1.5 [39], which is relatively small while still producing reasonable results, allowing the model to be executed on commonly available hardware like laptops, and Nvidias NV-Embed-v1 [14] as one of the leading open-source models. For the LLM, we use OpenAIs gpt-4o-2024-05-13 . We evaluate the quality of the retrieved information in terms of accuracy and the token count of the returned result. We measure the accuracy using standard information retrieval metrics, namely recall =TP To correctly solve the query q, a service discovery approachs recall should ideally be maximal. However, this could result in retrieving several irrelevant services, turning into a drop in precision. The F1 score represents a balance between recall and precision. Table 2 shows the RestBench results for the OpenAPI RAG on the Spotify API. In recall, the JSON split method performs exceptionally well, especially with a high chunk size s, as this approach densely packs the information from the JSON into the chunks by removing all formatting. For precision and F1, the endpoint splitting approaches perform best because each chunk corresponds to precisely one endpoint. Differences between the models are minor, except that thebge-small-en-v1.5 performs worse for the no split approach. We also tested s= 2048and s= 4096, which are not reported here for space reasons. We show s= 1024because it is the default chunk size of LlamaIndex and s= 8191 because it is the maximum input token count for the OpenAI model. It is worth mentioning that with an increasing chunk size, the token size of the returned result also increases. Generally, a higher recall seems to correlate with a higher token count, e.g., no splitting with s= 1024, and l= 0has 4717tokens output on average. In contrast, the JSON split has 10056with the same parameters, but this needs further analysis. Due to length limitations, we cannot show the token count comparison and other values for top khere. We also tested top k= 5and topk= 20. Recall increases with a higher top k, but precision drops. Additional data is available in the complementary material.4 Table 3 presents the OpenAPI RAG RestBench results for the TMDB API. The TMDB OpenAPI is more complex in length and extent than the Spotify OpenAPI. In this case, the endpoint split-based approaches performs best inAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 11 Table 2. Results for the OpenAPI RAG for top k= 10with the Spotify API. The bge-small-en-v1.5 and NV-Embed-v1 , respectively. TC is for token chunking, RE for remove examples, and RF for relevant fields. Recall, precision, and F1 are in percent. The highest values per column are marked in bold. Category Splitting Refinement s lOSNOSNOSN Character No TC 1024 0674062161219261829 No TC 8191 0887191777121313 Endpoint TC 1024 0707576191920293131 Endpoint TC 8191 0737576191920293031 JSON TC 1024 08184859810171519 JSON TC 8191 09795100555101010 LLM Endpoint Query N/AN/A715758181515292324 Table 3. Results for the RAG for top k= 10with the TMDB API. Schema as in Category Splitting Refinement s lOSNOSNOSN Character No TC 1024 0114733173821511 No TC 8191 0301715195923811 Endpoint TC 1024 0404046201518272126 Endpoint TC 8191 0664759191214291923 JSON TC 1024 0444446181216261924 JSON TC 8191 0616050886141411 LLM Endpoint Query N/AN/A566546131510202417 Endpoint Summary N/AN/A69596516131529222412 Pesl et al. Table 4. Results of the Discovery Agent experiments. We set topk= 10and use OpenAIs text-embedding-3-large as the embedding model. Spotify and TMDB are the two test sets from the RestGPT benchmark. RAG are the results for the summary chunking strategy from the Tables 2 and 3. Query is the standard LlamaIndex QueryEngineTool to retrieve data from a RAG system. The summary is our approach with a QueryEngineTool for summaries and a details-on-demand fetcher. Accuracy values are in percent. #Token is the number of tokens per query averaged over all queries in the test set. The best value per row is marked in bold. Accuracy Recall71.92 63.70 66.4469.33 43.11 46.67 Precision 18.4267.3970.29 15.60 45.9750.97 #Token Prompt4233.658606.873125.21 41001.46 65699.75 4544.57 Completion 0.00262.30 256.26 0.00242.65 231.73 Total4233.658869.183411.47 41001.46 65942.40 4776.30 precision and F1. The no split approaches achieve high values in precision due to their low value of true positives. Overall, the endpoint split tends to outperform no splitting. The JSON splitting benefits Spotify as the endpoints are already very dense, i.e., the endpoints do not contain examples, and schemas are only referenced. Therefore, many endpoints can be condensed into one chunk. This approach performs much worse for the lengthier endpoints in the TMDB API. The summary refinement outperforms the query refinement, leading to the Discovery Agent. We present the RestBench results of the Discovery Agent in Table 4. For accuracy, we measure recall, precision, and F1 equally to the OpenAPI RAG experiments. For the token count, we measure the actual tokens sent from the agent to the LLM from the agent as prompt, the tokens received as completion , and their sum astotal. For the RAG approach, we accumulate the tokens of the retrieved The results show that both agent approaches improve precision and F1 but reduce recall. The Query approach increases the tokens in the prompt. Contrarily, in the total token count. The completion token count is by a magnitude smaller than the prompt token count for the agent approaches, which is relevant as completion tokens are usually more expensive than prompt tokens. No LLM is invoked in the RAG approach, so the completion tokens are zero.Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 13 We demonstrated the effectiveness of the OpenAPI RAG and the Discovery Agent using our implementation. They are able to retrieve large portions of relevant data while not revealing all relevant information in all cases. To address RQ1, we implemented the OpenAPI RAG to apply RAG for endpoint discovery with seven chunking strategies and numerous parameter combinations. We showed its effectiveness using the RestBench benchmark. Overall, the ability to adequately reduce the token size to fit into the LLM context size while maintaining most of the relevant information is exhibited by the prototype. Regarding the chunking strategies, endpoint split-based chunking strategies achieve favorable accuracies. Limitations are primarily that the RAG results may not contain all relevant information, and the precision is low due to the retrieval of exactly kchunks. Additional research is needed to improve the retrieval performance further and prove the results in a generalized setting across For RQ2, we introduced the Discovery Agent, which transfers the LLM agent pattern to endpoint discovery. Especially using Summary approach, the Discovery Agent showed strong improvement over the OpenAPI RAG in terms of precision, F1, and token count. Further research is needed to improve the decline in recall due to the processing through the LLM. While we rely on the research benchmark RestBench for our results, which covers two extensive OpenAPIs, queries, and ground truth, it is still limited to these two services. OpenAPI RAG systems in practice may operate on much larger datasets. For the data processing, we rely on standard RAG implementations like LlamaIndex, which are already designed to operate on large amounts of data. The performance evaluation, especially in larger real-world scenarios, remains open for future research. The applicability of the OpenAPI RAG depends on the availability of service documentation. We try to mitigate this issue by relying on widely adopted OpenAPI specifications, but this might not be valid for all domains. A solution to consider is automatically generating service documentation using an LLM. Another factor influencing the discovery is the quality of the OpenAPIs. The discovery may fail if no descriptions, meaningful naming, or erroneous information is given. This is not an issue of the approach, as a human developer would face the same problem, but it highlights the importance of high-quality documentation. strategies, e.g., CRAFT [ 42], could be added to the OpenAPI RAG. These could improve retrieval performance by combining multiple strategies or by creating a custom chunking strategy for a specific kind of service documentation. Another advancement could also be creating a custom embedding model tailored explicitly to service descriptions and service description chunks. This model may also be trained for one specific chunking strategy or intended use case. Additionally, the RAG output may be trimmed to boost precision. This could be done by, e.g., employing a similarity threshold.14 Pesl et al. The presented Discovery Agent could be further improved to handle whole service compositions. In this case, the agent would be extended by an additional component for the service composition, and the user would only submit their ser- vice composition task to the agent to retrieve the executable service composition Besides capabilities of the RAG system, resource consumption is a major issue in LLM-based systems. The OpenAPI RAG only uses embedding models. These are much more efficient than LLMs, resulting in costs in fractions of a cent per query. In contrast, the Discovery Agent requires significantly more resources, i.e., running RestBench in our experiments resulted in about $50 of API fees. Further work is needed to reduce this resource footprint. different ISs. With the application of automated LLM-based service composition approaches, the LLM input context limitations have become prominent, as the the preselection of relevant information. To address this issue, we proposed an OpenAPI RAG, which facilitates semantic search based on state-of-the-practice OpenAPIs and reduces the input token size. Further, we show an advanced integration through a Discovery Agent, which can retrieve service details on demand to reduce the input token count further. Our evaluation based on the RestBench benchmark shows that our approach is viable and performing. Limitations are especially in the restriction of RestBench to two services of the entertainment domain. We will address this in an extended version of this work. Further improvements are in optimizing the implementation and extending the agent for additional tasks, e.g., whole service compositions. We leave this for Acknowledgments. This work was partially funded by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) project Software-Defined Car (SofD- Car) (19S21002). The authors acknowledge support by the state of Baden-W rttemberg Disclosure of Interests. The authors Pesl and Aiello are listed as inventors of a patent [27], which covers automated service composition using LLMs for the automotive 1.Achiam, J., et al.: GPT-4 technical report (2023), https://arxiv.org/abs/2303. 2.AI@Meta: Llama 3 model card (2024), https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.mdAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 15 3.Baresi, L., Miraz, M.: A distributed approach for the federation of heterogeneous registries. In: ICSOC 2006. pp. 240 251. Springer (2006). https://doi.org/10. 4.Bohn, H., Golatowski, F., Timmermann, D.: Dynamic device and service discovery extensions for WS-BPEL. In: ICSSSM 2008. pp. 1 6. IEEE (2008). https://doi. 5.Cobbe, K., et al.: Training verifiers to solve math word problems (2021), https: 6.Cuconasu, F., et al.: The power of noise: Redefining retrieval for RAG systems. In: SIGIR. vol. 47, pp. 719 729 (2024). https://doi.org/10.1145/3626772.3657834 7.Curbera, F., et al.: Unraveling the web services web: an introduction to SOAP, WSDL, and UDDI. IEEE Internet Computing 6(2), 86 93 (2002). https://doi. 8.Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: NAACL-HLT 2019. pp. 9. Douze, M., et al.: The Faiss library (2024), https://arxiv.org/abs/2401.08281 10.Fan, A., et al.: Large language models for software engineering: Survey and open 11.Fikouras, I., Freiter, E.: Service discovery and orchestration for distributed service repositories. In: ICSOC 2003. pp. 59 74. Springer (2003). https://doi.org/10. 12. Gao, L., et al.: Pal: Program-aided language models. In: International Conference on Machine Learning. pp. 10764 10799. PMLR (2023) 13.Kim, M., Stennett, T., Shah, D., Sinha, S., Orso, A.: Leveraging large language models to improve REST API testing. In: ICSE. vol. 44, pp. 37 41 (2024). https: 14.Lee, C., et al.: Nv-embed: Improved techniques for training llms as generalist embedding models (2024), https://arxiv.org/abs/2405.17428 15.Lemos, A.L., Daniel, F., Benatallah, B.: Web service composition: A survey of techniques and tools. ACM Comput. Surv. 48(3) (dec 2015). https://doi.org/10. 16.Lewis, P., et al.: Retrieval-augmented generation for knowledge-intensive NLP tasks. In: NeurIPS. vol. 33, pp. 9459 9474. Curran Associates (2020) 17.Li, M., et al.: API-Bank: A comprehensive benchmark for tool-augmented LLMs. In: EMNLP. Association for Computational Linguistics (2023). https://doi.org/ 18.Liang, Y., et al.: Taskmatrix.AI: Completing tasks by connecting foundation models with millions of APIs. Intelligent Computing 3, 0063 (2024). https://doi.org/ 19.Mialon, G., et al.: Augmented language models: a survey (2023), https://arxiv. 20.Nakano, R., et al.: WebGPT: Browser-assisted question-answering with human 21.Nogueira, R., Yang, W., Lin, J., Cho, K.: Document expansion by query prediction 22.OpenAI: Function calling and other API updates (Jun 2024), https://openai.com/ index/function-calling-and-other-api-updates/ , last accessed 2024-07-18 23.OpenAI: GPT-4 Turbo in the OpenAI API. https://help.openai.com/en/ articles/8555510-gpt-4-turbo-in-the-openai-api (2024), last accessed 2024- 11-1916 Pesl et al. 24.OpenAI: New embedding models and API updates (Jan 2024), https://openai. com/blog/new-embedding-models-and-api-updates , last accessed 2024-07-18 25.Parisi, A., Zhao, Y., Fiedel, N.: Talm: Tool augmented language models (2022), 26.Patil, S.G., Zhang, T., Wang, X., Gonzalez, J.E.: Gorilla: Large language model connected with massive APIs (2023), https://arxiv.org/abs/2305.15334 27.Pesl, R.D., Klein, K., Aiello, M.: Verfahren zur Nutzung von unbekannten neuen Systemdiensten in einer Fahrzeuganwendung (2024), Patent DE102024108126A1 28.Pesl, R.D., St tzner, M., Georgievski, I., Aiello, M.: Uncovering LLMs for service- composition: Challenges and opportunities. In: ICSOC 2023 WS. Springer (2024). 29.Pesl, R.D., et al.: Compositio Prompto: An architecture to employ large language models in automated service computing. In: ICSOC 2024. Springer (2024) 30.Radford, A., Wu, J., Amodei, D., Amodei, D., Clark, J., Brundage, M., Sutskever, I.: Better language models and their implications. OpenAI blog 1(2) (2019), https: //openai.com/index/better-language-models/ , last accessed 2024-11-28 31.Radford, A., et al.: Improving language understanding by generative pre-training 32.Santana, J.M.S., Petrova, M., Mahonen, P.: UPnP service discovery for heteroge- neous networks. In: IEEE PIMRC. vol. 17, pp. 1 5. IEEE (2006) 33.Shi, Z., et al.: Chain of tools: Large language model is an automatic multi-tool 34.Soki, A.T., Siqueira, F.: Discovery of RESTful Web services based on the OpenAPI 3.0 standard with semantic annotations. In: AINA. pp. 22 34. Springer (2024). 35.Song, Y., et al.: RestGPT: Connecting large language models with real-world applications via restful APIs (2023), https://arxiv.org/abs/2306.06624 36.Th nes, J.: Microservices. IEEE software 32(1), 116 116 (2015). https://doi.org/ 37. Vaswani, A., et al.: Attention is all you need. NeurIPS 30(2017) 38.Wei, J., et al.: Chain-of-thought prompting elicits reasoning in large language models. NeurIPS 35, 24824 24837 (2022) 39.Xiao, S., Liu, Z., Zhang, P., Muennighoff, N.: C-pack: Packaged resources to advance general chinese embedding (2023), https://arxiv.org/abs/2309.07597 40.Yao, S., et al.: React: Synergizing reasoning and acting in language models (2023), 41.Yao, S., et al.: Tree of thoughts: Deliberate problem solving with large language 42.Yuan, L., et al.: CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets (2024), https://arxiv.org/abs/2309.17428'},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'text': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet? Abstract Hallucinations, the tendency to produce irrelevan- t/incorrect responses, are prevalent concerns in generative AI- based tools like ChatGPT. Although hallucinations in ChatGPT are studied for textual responses, it is unknown how ChatGPT hallucinates for technical texts that contain both textual and technical terms. We surveyed 47 software engineers and produced a benchmark of 412 Q&A pairs from the bug reports of two OSS projects. We find that a RAG-based ChatGPT (i.e., ChatGPT tuned with the benchmark issue reports) is 36.4% correct when producing answers to the questions, due to two reasons 1) limita- tions to understand complex technical contents in code snippets like stack traces, and 2) limitations to integrate contexts denoted in the technical terms and texts. We present CHIME ( ChatGPT Inaccuracy Mitigation Engine) whose underlying principle is that if we can preprocess the technical reports better and guide the query validation process in ChatGPT, we can address the observed limitations. CHIME uses context-free grammar (CFG) to parse stack traces in technical reports. CHIME then verifies and fixes ChatGPT responses by applying metamorphic testing and query transformation. In our benchmark, CHIME shows 30.3% more correction over ChatGPT responses. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT Index Terms ChatGPT, Hallucination, Software Issue Reports The reliability of LLMs is often questioned due to their tendency to produce nonsensical or incorrect outputs, a phe- nomenon commonly referred to as hallucination [1], [2], [3]. Like any LLM, ChatGPT can also suffer from hallucination issues like inconsistency in responses [4], [5] or factual inaccuracies. These problems can arise even when the model is provided with the context as a document/paragraph. While progress is made to assess hallucinations in textual data [6], [7], we are not aware of how hallucinations can be detected and mitigated for software technical reports that contain both textual and technical terms (e.g., crash dumps, code snippets, This paper studies the detection and mitigation of ChatGPT inaccuracies in technical reports. We pick software bug reports for our study, because bug reports often contain a blend of descriptive text, technical terminology, code references, and snippets of crash/system dumps [8]. These documents are crucial for tracking and resolving software issues but can be overwhelming due to their volume and complexity [9]. An AIchatbot, trained to understand these reports, may streamline the process by extracting information. But for that, first we need to ensure that the responses from the chatbot are correct. In the first phase of our study , we conducted a survey of 47 software engineers to understand the types of questions they ask while exploring bug reports and for which they wish for an automated Q&A tool like a chatbot. We found that developers ask diverse questions during bug exploration, which we could group into five types: 1) issue analytics, 2) issue trends, 3) issue summary, 4) issue labeling, and 5) issue backlogs. Based on the survey findings, we produced a benchmark of 412 Q&A pairs by consulting our industry partner (with whom we conducted regular bi-weekly sessions) and the literature. The Q&A pairs are collected by analyzing the issue reports of two popular open-source software (OSS). In the second phase of our study , we tuned ChatGPT with issue reports from the two studied OSS based on the Retrieval Augmented Generation (RAG) techniques [10], [11], [12], [13]. We then evaluated the correctness of ChatGPT re- sponses against our benchmark. Each question was asked and compared automatically to its expected answer. Correctness was assessed as the ratio of questions whose answers were found as correct. We found that our RAG-based ChatGPT was correct in only 36.4% cases. For the rest of the questions, it hallucinated by producing incorrect or irrelevant answers. We manually examined each hallucination case and identified two limitations in ChatGPT to process technical documents like bug reports: 1) limitations to understand complex technical contents in code snippets like stack traces (e.g., when a partial code snippets/crash dump is provided and the question is about determining the cause of the crash by assessing both the crash dump and the textual contents), and 2) limitations to integrate contexts denoted in the technical terms and texts (e.g., when ChatGPT was required to assess the relationships among multiple metadata and the technical terms). In the third phase of our study , we designed CHIME (ChatGPT Inaccuracy Mitigation Engine) to address the above two limitations. The underlying principle in CHIME is that (1) if we can preprocess the technical reports better and store information relevant to an issue report as a combination of metadata and actual contents and (2) then guide the query validation process in ChatGPT with guided iterative prompting approaches, we can address the observed limitations. The 11 Nov 2024TABLE I: Demography of Survey Participants Current Role 0-3 4-5 6-10 11-15 16-20 Total Developer 13 20 4 2 - 39 QA Engineer 1 1 - - - 2 Project Manager - 1 - - - 1 Other 2 - 2 - 1 5 Total 16 22 6 2 1 47 (a)Resolution Triaging Reporting Management89.36% Fig. 1: (a) Frequency & (b) Reasons of Bug Report Exploration usage of metadata is found to improve LLM search capabilities [14]. For us, such metadata could be generated by organizing the mix of textual and technical contents into a structured form. A challenge was how to separate the textual and technical contents and process the code terms within a crash dump and then organize those within a structure. We introduce a novel context-free grammar (CFG) in CHIME to efficiently parse stack traces in technical reports. As for the second principle (i.e., guided prompting for verification), we extended recent similar work on textual content. CHIME verifies and fixes ChatGPT responses by using query transformation [15] and by extending CoVe [16] with metamorphic testing (MT) [17]. CoVe is a zero-shot iterative prompting-based query verifi- cation technique. We evaluated CoVes response by further mutating the question using MT, because CoVe may discard correct responses or promote incorrect responses. We evaluated CHIME using our benchmark. CHIME shows on average 30.3% improvement over ChatGPT responses by offering more correct answers. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT without CHIME. Our replication package (https://bit.ly/4fyaMIP) contains all the data and code developed in the study. II. R ELATED WORK A. Hallucinations in Large Language Models Extensive studies in the literature identified the causes of hallucinations as sub optimal training, inference [18], [19], [20], [21], [22], and insufficient/low-quality data [23], [24], [25]. Techniques such as bidirectional auto-regressive models [26] and attention-sharpening mechanisms [27] have been developed to address training-related hallucinations. Inference issues, primarily due to decoding strategies, often result in in- accurate outputs. Strategies like factual-nucleus sampling [28]and in-context pretraining [29] are implemented to mitigate these inaccuracies. Challenges posed by flawed data sources introduce biases and inaccuracies into models, stemming from misinformation, duplication biases, and social biases in the training datasets. Mitigating data biases involves manual dataset creation [30], integrating high-quality sources such as the Pile [31], and up-sampling factual data [32]. Furthermore, knowledge editing [33], [34] and Retrieval-Augmented Gen- eration (RAG) [35], [11], [12], [13] are employed to bridge knowledge gaps, utilizing external sources for more accurate Our study utilizes RAG-based ChatGPT for technical bug report understanding. We enhance ChatGPTs knowledge base by integrating it with a database of bug reports through RAG methods. We then develop CHIME, which refines both the preprocessing of input data and the validation of RAG-based B. LLMs for Software Engineering In recent years, the application of LLMs has been widely utilized in Software Engineering (SE) tasks, ranging from code analysis to bug detection [36], [37], [38], [39], [40], [41], [42], [43], [44]. Encoder-only models like BERT [45] and its derivatives, including CodeBERT [46] and GraphCodeBERT [47], excel in processing code. While encoder-decoder models, such as T5 and PLBART excel in understanding semantics of code for tasks like code summarization [48], [49]. Decoder- only models like the GPT series and specialized versions like CodeGPT and Codex generate direct responses from prompts. However, the specific challenge of mitigating inaccuracies in software technical reports remains unexplored. Addressing this gap, we introduce CHIME to reduce inaccuracies in ChatGPT- generated responses during bug report exploration. III. AI C HATBOT NEEDS FOR BUGREPORT EXPLORATION To evaluate a chatbot on software technical documents, we needed a benchmark, which at the time of our study was not available. We thus adopted a systematic approach to create such a benchmark. First, we conducted a survey of software developers to produce a catalog of questions that they ask during bug reports. Second, we used the catalog to produce our benchmark (see Section IV). This section discusses the survey, which answers the following research question (RQ): RQ1. What types of questions would software practitioners like to ask a chatbot during bug report exploration? We employed the snowball sampling approach [50] to recruit participants, resulting in 47 software practitioners. The majority of respondents (83%) held developer roles. The largest proportion of participants (55%) reported having 4- 5 years of experience in the field. Table I illustrates the distribution of participants across their roles in the software industry and years of experience. We ensured that all se- lected participants regularly use Issue Tracking Systems (ITS), such as Jira, GitHub, or in-house systems for several keyTABLE III: Key Questions (KQ) of the Survey. 1 How would you like to utilize a chatbot during bug 2 Would you want to use the chatbot for T#? C 3 Rate the usefulness of T# for your work C 4 Rate the potential usefulness of the following bench- mark questions of T#C TABLE II: Identified Task Types for Bug Report Exploration T# Task Title Task Description T1 Issue Analytics Explores complex details within/across bug reports, including technical jargon, error codes, and contextual nuances, etc. T2 Issue Trends Patterns and trends in bug occurrences. T3 Issue Summaries Summaries of key topics discussed in an issue or across multiple issues. T4 Issue Labeling Inquires labels for bug reports to help orga- nize and categorize them effectively. T5 Issue Backlog Explores whether the issue remains open for purposes. Around 89.36% of respondents used ITS for issue resolution addressing and for solving bugs; followed by issue triaging which involves prioritizing and assigning issues. Project Management and documenting issues for records or stakeholder communication were also noted. A vast majority of respondents (70.21%) reported engaging with issue reports daily. Figure 1a presents the distribution of issue report inter- action frequency and Figure 1b illustrates the survey responses for the primary reason for issue report exploration. Before the main survey, we consulted two software profes- sionals from a reputed software company to get insights on the potential tasks that an AI chatbot can support during bug report understanding. Each had 16 and 35 years of experience respectively. We finally settled on five distinct tasks as shown in Table II). For our survey, we formulated questions to address During the survey, participants were prompted to envision an AI chatbot with comprehensive access to bug reports. Subsequently, they were asked whether they would like to utilize the chatbot for the specified tasks (T#) and to rate the perceived usefulness of each task for their work. We condensed the key survey questions into Table III for clarity. All survey questions can be found in our online appendix. C. Understanding Preferences of Software Practitioners on Identified Task Types (RQ1) In Table III, with Key Question 1 (KQ1), we tried to identify the desired chatbot roles during bug report explorations. With KQ2-4, we examined how participants perceived the useful- ness of each task type. The questions are summarized in TableIV. For each question, we show quantitative evidence from the survey about what the participants thought of the usefulness 1) KQ1. Desired Chatbot Roles: We asked KQ1 as an open- ended question to check whether the participants considered the same five task types that we identified in Table II. As such, we did not show them KQ2-4 until they answered KQ1. Participants desired a tool whose capabilities closely aligned with our predefined tasks (T1 T5). They emphasized the chat- bots potential to analyze issues (T1), such as extracting pivotal details like exceptions or log entries from similar bug reports. According to R43: Queries for potential duplicate issues could be really helpful. The capability to analyze trends (T2) within reported issues to uncover recurring problems was seen as crucial. Participants also noted the importance of the chatbots ability to summarize (T3) and categorize (T4) issues. The survey responses indicated a significant interest in features that would allow users to query backlogged items (T5), as R43 stated: Queries for issues with very little recent activity and no clear resolution would be super helpful. Apart from these tasks, participants also expressed desires for additional capabilities such as sorting issues based on priority, severity, or difficulty and forecasting resolution times using historical data. We leave support for those tasks as our future work. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 2: (a) Interest for T1 (b) Usefulness Perception of T1 2) KQ2-4. Issue Analytics (T1): The survey results reveal a strong preference for the chatbots analytical capabilities, particularly in the context of analyzing multiple issues, with more than 80% of participants expressing interest in utilizing the chatbot for detecting similar or duplicate issues and finding it useful; while 72.34% participants value the chatbots utility in analyzing individual issues as illustrated in Figure 2. In Table IV, we show 10 questions under T1 that each par- ticipant assessed. On average, 73.62% of participants marked these useful. The capability to identify and summarize stack traces (Q1.1) within the single issue analysis domain was highly valued, evidenced by a utility score of 78.72% and remark from respondent R11, Summary of stack trace is a good idea . In comparison, the importance of determining the environment linked to an exception (Q1.2) was rated lower, at 57.45%. For the analysis of multiple issues, the ability to find similar issues (Q1.3) was highly valued at 82.98%, as quoted by R09, They are all extremely time-consuming when done manually. A chatbot will definitely help with this. 0 20% 40% 60% 0No Maybe Yes 0 20% 40% 60% 80% 0Not Useful Neutral Useful Fig. 3: (a) Interest for T2 (b) Usefulness Perception of T2TABLE IV: Perceived Usefulness of Benchmark Questions Presented in the Survey. Not Useful Neutral Useful Q# Question Perceived Usefulness of T# (KQ4) T1 - Issue Analytics Extracting Information from Issue Details or Find Similarities Among Issues Q1.1 Is there a stack trace provided in issue 123, and can you summarize it? Q1.2 Where in the code does the exception in issue 123 occur? Q1.3 What is the exception reported in issue 123? Q1.4 How many tests failed as reported in issue 123? Q1.5 Which environment is associated with the exception reported in issue 123? Q1.6 Are there any issues similar to issue 123? Q1.7 Find duplicate reports of the X error (or other) in A module Q1.8 Find all similar issues related to X failures] Q1.9 Identify any performance degradation issues reported on last month Q1.10 Has there been a report of a crash on a X machine running the A Test recently? T2 - Issue Trend Detect and Analyze the Trends and Patterns among Issues Q2.1 What are the frequently encountered errors in the nightly builds? Q2.2 What are the recurring themes in bug reports post the latest OS update? T3 - Issue Summary Obtain a Comprehensive Overview of Reports Selected by Different Criterion Q3.1 List all issues related to an X feature and their current status Q3.2 Generate a report detailing the distribution of issues across different project modules Q3.3 Compile a summary of unresolved issues not older than 60 days Q3.4 Can you generate a summary of all issues tagged as bug in the last 30 days? Q3.5 Create a summary of user-reported issues versus internally identified issues T4 - Issue Label Provide Suggestions for Categorizing and Tagging Issues with Appropriate Labels Q4.1 Suggest existing labels to tag issue 123 Q4.2 Can you recommend labels for performance-related issues? T5 - Issue Backlog Analyze Unresolved Issues Reported but not yet Addressed Q5.1 Are there any long-standing issues that have been consistently postponed? Q5.2 Find issues that have not been assigned to any milestone but are older than 60 days Q5.3 List issues that have missed two or more release cycles Q5.4 Identify issues with no activity in the last 30 days. 3) KQ2-4. Issue Trend (T2): 87.2% of participants ex- pressed interest in using this feature and 85.1% found the corresponding questions on identifying and analyzing trends within bug reports useful (see Figure 3). In Table IV, we show two questions that we asked under this task. Both received a favorable response. Respondent R31 noted, By focusing on recurring errors and themes, these questions provide valuable insights that can guide decision-making, resource allocation, and issue resolution efforts. When participants were asked about the utility of chatbots in identifying frequently encoun- tered errors in the nightly builds of their development envi- ronment (Q2.1), e.g., in CI/CD pipelines, 76.60% perceived this functionality as useful. 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 4: (a) Interest for T3 (b) Usefulness Perception of T3 4) KQ2-4. Issue Summary (T3): 80.6% of participants were keen on a chatbot to produce summaries of issues and 70.2% considered the asked questions useful for efficiently understanding and resolving software issues. In Table IV, regarding the chatbots ability to report on how issues are distributed across different project modules (Q3.1), 76.6% found this function useful. On the other hand, the feature for distinguishing between user-reported and internallyidentified issues (Q3.2) was seen as useful by 51.06% of participants, indicating a notable but more moderate interest in differentiating the sources of issues. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 5: (a) Interest for T4 (b) Usefulness Perception of T4 5) KQ2-4. Issue Labeling (T4): 70.2% of participants are interested in leveraging chatbots for the task of issue labeling (see Figure 5). However, its worth noting that this task re- ceived the lowest percentage of perceived usefulness (61.70%) compared to others. Regarding the chatbots ability to suggest appropriate labels for an issue (Q4.1) and to recommend labels for performance-related issues (Q4.2), about 68% of respondents considered these features to be useful. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 6: (a) Interest for T5 (b) Usefulness Perception of T5 6) KQ2-4. Issue Backlog (T5): The management of Issue Backlogs is an essential aspect of software development. R16 highlighted the challenge: Sometimes change of prioritiespushes issue out of find and stay unresolved for days. So it is good to find out long-running or inactive issues. Despite its importance, this task garnered the least interest (63.8%) among all tasks for potential chatbot utilization. Notably, T5 records the highest percentage of No responses (17.02%) regarding interest and Not Useful perceptions (19.15%). 70.2% of respondents see value in identifying long-standing, postponed issues (Q5.1) but interest slightly drops to 61.70% for detecting issues with no recent activity over the last 30 Summary of RQ1. When examining software practition- ers preferences for AI chatbot capabilities in exploring bug reports, the identification of similar issues and the analysis of recurring error trends were highly favored. In contrast, capabilities related to categorizing issues and handling pending bugs were deemed less critical. IV. A B ENCHMARK OF Q&A P AIRS TO EVALUATE AI CHATBOTS FOR BUGREPORT EXPLORATION In Table IV, we showed a catalog of 23 questions that we validated with our survey participants and for which they wished for chatbot support. Each question is a template, which can be used to produce multiple similar questions.Based on the question templates in Table IV, we produced a total of 412 questions from the issue reports of two popular OSS repos, OpenJ9 and ElasticSearch. OpenJ9 was chosen due to its alignment with our industrial partner. ElasticSearch [51], [52], [53] is frequently referenced in academic studies. We then produced an answer to each question by assessing the two OSS repos and by consulting among the authors. Four authors (the first two and last two) engaged in many hours of discussions that spanned over six months (both in- person and over formal presentations). The last two authors are also among the maintainers of OpenJ9. Given the benchmark was created via mutual discussion, we did not compute any standard agreement analysis metrics. We created the benchmark by selecting 80 complex issues (40 from each repository). Following Deeksha et al. [9], we define an issue as complex if it is excessively long and/or it has stack traces. We picked issues within the last year of our analysis because those are likely to be explored more by developers. We sorted issues by length and selected 40 issues with stack traces (per repo). Following the standard chatbot evaluation process, we contained three types of answers: binary [54], factual [55], [56], and summary [57]. The binary (i.e., Query Type = Y/N) queries have ques- tions with answers as Y/N. These are designed to assess the chatbots accuracy in identifying clear-cut, definitive binary decisions based on information available in bug reports; such as the presence of a particular error code or the applicability of a specific scenario. To verify a chatbot response for these queries, we simply need to check for Y/N in their responses and match those against the benchmark answer.Q1. Type: Y/N. Source: ElasticSearch Question: Is there any issue similar to issue 100071? The factual (i.e., Query Type = Factual) queries assessed the chatbots ability to extract concrete information from bug reports, such as identifying, retrieving, and presenting specific details from the dataset, such as error messages, stack traces, configuration settings, etc. Like binary queries, this method also allows for a straightforward assessment of the chatbots accuracy, and thus direct matching can be used for verification. Q2. Type: Factual. Source: ElasticSearch Question: What existing label is recommended for issues that need immediate triaging? The summary-based (i.e., Query Type = Summary) queries challenge the chatbot to engage in deeper analysis and syn- thesis of data. These queries require the chatbot to identify patterns and even to propose potential solutions based on the analysis of multiple data points. Since these queries demand a synthesis of information and provide insights or summaries, we need a similarity analysis between a response and the expected answer for verification. Q3. Type: Summary. Source: ElasticSearch Question: Summarize similarities between issues 103072 Expected Answer: Issues 103072 and 103344 both involve test failures within the LearningToRankRescorerIT class. The root cause of these failures stems from a named object notfound exception and xcontent parse exception, resulting in ... TABLE V: Distribution of Benchmark Questions over Survey- identified Tasks from Table IV. T# Y/N Fact Summarization T1 - Issue Anlys(S)48 140 24 212 T1 - Issue Anlys(M)12 20 8 40 T2 - Issue Trend16 16 8 40 T3 - Issue Summary8 32 40 T4 - Issue Labeling12 20 8 40 T5 - Issue Backlog12 24 440 Table V shows the distribution of question types Yes/No, Fact, and Summarization across OpenJ9 and ElasticSearch, totaling 206 questions per project. OpenJ9 and ElasticSearch have a similar overall structure, with a strong emphasis on factual questions (114 for OpenJ9, 114 for ElasticSearch), followed by binary (yes/no) and summarization questions. Our online appendix contains details about each of the 412 questions and how each question maps to our catalog of 23 survey questions.V. E FFECTIVENESS OF CHATGPT ON THE BENCHMARK In this section, we answer the following research question: RQ2. How effective is a RAG-enhanced ChatGPT to answer to the benchmark questions while exploring the corre- The RAG architecture combines ChatGPT with an external knowledge retriever to provide responses to queries. This framework utilizes external database sources, primarily issue reports with structural data and metadata fetched by the GitHub API. Figure 7 illustrates the pipeline for this. It functions by first retrieving pertinent information from the database based on the input query. This step is crucial as it aligns the models focus with the most relevant data. Then, the augmented data from the retrieval step are combined with the inherent generative capabilities of ChatGPT to help ChatGPT provide high-quality responses. We used ChatGPT 3.5-turbo within LangChain framework [58] to implement this pipeline. We used a temperature setting of 0. A temperature value above 0 produces slightly different answers to a prompt across multiple runs, which is unnecessary when we expect consistent answers from ChatGPT. We ran it multiple times on our benchmark dataset to ensure that the answers were indeed consistent across multiple runs. Fig. 7: Pipeline of the RAG-based ChatGPT We measure the effectiveness of the above RAG-based ChatGPT on our benchmark by calculating correctness (C): C=Number of Correct Responses Total Number of Queries 100% (1) Correctness analysis involved comparing the generated re- sponses against the predefined correct answer for a query in the benchmark. We adopted the following automated approaches to measure the correctness of the responses for the three types of queries in our benchmark (i.e., Y/N, Fact, and Summaries). For evaluating yes/no responses, we used a zero-shot clas- sification approach, enabling us to automatically determine if detailed answers from the chatbot align with a simple Yes or No expectation. For evaluating factual query responses, we combine two approaches: direct comparison of extracted information (such as issue numbers) and semantic similarity assessment for non-listed facts. First, we extract and compare key factual elements. If the response and expectation directly match or share common elements, the correctness is assessed accordingly. For responses without explicit factual elements, we utilize the SentenceTransformer library, employing the all- MiniLM-L6-v2 model [59] to encode the actual and expected answers into embeddings. Subsequently, we gauge the seman- tic similarity between these embeddings by computing thecosine similarity [60] and assessing how closely the actual answer aligns with the expected fact, considering nuances in phrasing and context. For evaluating Summarization queries, we compute the semantic similarity like before between the actual summary provided by ChatGPT and the expected sum- mary. Based on empirical observations (see Section VIII-B), we used a similarity threshold of 0.7. The RAG-based pipeline achieved 36.4% accuracy in our benchmark (see Table VI). We manually assessed each of the 262 incorrect answers to determine the causes of its incorrectness. Given that ChatGPT is a black-box model, our assessment is based on the nature of the questions asked and the provided answers. We observed two limitations in L1. Limitations in Understanding Complex Technical Content L2. Contextual Understanding Challenges (57.3% cases). TABLE VI: Correctness of RAG Based ChatGPT T# Y/N Fact Sum Total Total 49.0% 30.7% 36.9% 36.4% T1 - Issue Anlys(S) 41.7% 32.1% 33.3% 34.4% T1 - Issue Anlys(M) 50.0% 15.0% 0.0% 22.5% T2 - Issue Trend 50.0% 12.5% 37.5% 32.5% T3 - Issue Summary 0.0% 75.0% 43.8% 50.0% T4 - Issue Labeling 58.3% 40.0% 62.5% 50.0% T5 - Issue Backlog 66.7% 25.0% 25.0% 37.5% The challenge in understanding technical contents (L1) occurred when ChatGPT faced challenges in deeply under- standing and accurately processing highly technical content, particularly when it involved intricate programming/technical concepts. For example, when asked to provide the root location of the exception encountered in issue 18151; it gives a non- useful answer; even though the issue provides a stack trace. Q4. Asking ChatGPT - OpenJ9 Question: Where in code exception of issue 18151 occur? Incorrect Answer: The exception of issue 18151 occurs in the body of the issue. Again, for asking to find the line number where the error occurs, ChatGPT just gives an irrelevant answer by showing Q5. Asking ChatGPT - OpenJ9 Question: Which line in CharacterDataLatin1 class trig- gered ArrayIndexOutOfBoundsException in issue 17063? Irrelevant Answer: The line of code that is The Contextual Understanding Challenges category (i.e., L2) shows a fundamental difficulty in ChatGPT to processand respond to queries about technical issues. This category primarily involves the models struggles with: Integrating and interpreting the context in which queries are made. Some context is explicitly stated within the query or the referenced issue, such as a specific error message or stack trace. Other times, the context is implicit, requiring the model to infer based on its broader knowledge or related data points. Handling ambiguous or insufficiently detailed queries necessitates the chatbot to fill in the gaps with assumptions or inferred knowledge. Not all contextual in- formation holds equal relevance to a given query, requiring the chatbot to prioritize the most pertinent context based on the nuances of the query. Adapting to the technical conventions of specific domains. Technical domains often have their own conventions for documentation, communication, and issue tracking. For instance, understanding that a particular label in an issue tracking system denotes the responsible team, requires domain-specific knowledge that the AI must possess. On asking to find a responsible team for an issue, ChatGPT searches on the assignee list, but the team details are on the issue labels. Due to this lack of contextual information, it fails to answer the question. Q6. Asking ChatGPT - ElasticSearch Question: Which team is responsible for issue 104160? Incorrect Answer: The team responsible for issue 104160 is not specified in the database. Even when the relevant context is identified, retrieving and applying it accurately to generate a response is challenging. This includes understanding the specific ways in which information is structured or presented within data sources and how it relates to the users query. Effectively bridging this gap is crucial for generating accurate and contextually Summary of RQ2. A RAG-based ChatGPT showed an average correctness of 36.4% on our benchmark. The pipeline encountered challenges in comprehending com- plex technical content and grasping contextual nuances, leading to inaccuracies in its responses. VI. CHIME: C HATGPT I NACCURACY MITIGATION Our observations in Section V of ChatGPT limitations contributed to the design of CHIME, as a suite of techniques to detect and fix incorrectness in ChatGPT responses. The underlying principle of CHIME is that by offering ChatGPT with a more structured representation of bug reports and by applying a systematic approach to assess ChatGPT responses, we can address the two limitations we observed in Section V-B. A more structured representation of bug reports can be achieved if we can process the different technical and textual terms properly and store those in a structured way, e.g., in a database with metadata offering more information about thoseterms. A systematic approach to verify the responses can be achieved by applying/adapting the techniques of guided itera- tive prompting of LLM responses that are used in the literature for textual content. As such, we designed to preprocess the inputs (both the bug report and the query) and to verify the CHIME treats ChatGPT as an API, where the inputs (ques- tions) and outputs (answers) are processed for inaccuracy detection and mitigation. We can use another LLM as an API in CHIME and apply all the techniques we developed. Doing so would simply require changing the API endpoints to point to the other LLM within the LangChain toolkit. CHIME will need to be updated significantly while using multi-modal LLMs, e.g., to process/validate modalities other than texts, etc. Fig. 8: The Architecture of CHIME In Figure 8, we show the architectural diagram of CHIME. We apply an Issue Preprocessor component to parse crash dumps and other technical terms. Issue Preprocessor employs Context-Free Grammars (CFGs) to interpret complex technical data, addressing ChatGPTs limitations in understanding com- plex technical content. To address the second limitation (i.e., Contextual understanding challenges), we introduce two more components in CHIME. First, we apply Query Preprocessor component to decipher users intents. Second, we designed the Response Validator component, which evaluates the accuracy in a response using a combination of two techniques: Chain of Verification (CoVe) [16] and Metamorphic Testing (MT) [17]. A query goes through each of these components. We discuss We developed a CFG as shown in Listing 1 which sup- ports parsing stack traces to the level of individual code elements. Stack traces can also contain auxiliary information like timestamps and memory addresses (e.g., crash dumps). We preprocessed these auxiliary information and applied our CFG to parse the code elements in a Java stack trace. Finally, our issue preprocessor stored the code blocks in the database by separating the code blocks from the textual contents. The issue preprocessor saves the following information by parsing an issue report: title, body, stack trace, and other metadata(creation date, status, etc). When ChatGPT, enhanced with Issue Preprocessor, is presented with the query regarding the type of exception in issue 18151 (Q4) or 17063 (Q5), it correctly identifies the location of the exception detailed in Listing 1: CFG Grammar for Stack Traces 1 Root ::= StackTraceElems 2 StackTraceElems ::= StackTraceElem StackTraceElems | 3 StackTraceElem ::= ExceptionElems |CodeDetails 4 ExceptionElems ::= ExceptionElem ExceptionElems | 5 ExceptionElem ::= ExceptionType |ExceptionMessage 6 CodeDetails ::= ClassElem |MethodElem |FileElem | Asking CHIME with Q4 from Section V. OpenJ9 Question: Where in the code exception of issue 18151? Correct Answer: The exception in issue 18151 can be triggered at line 98 in the JvmErgonomics.java file. Asking CHIME with Q5 from Section V. OpenJ9 Question: Which code in CharacterDataLatin1 triggered the ArrayIndexOutOfBoundsException in issue 17063? Correct Answer: Line 72 in the CharacterDataLatin1 A user query may not provide enough details, leading ChatGPT to generate responses based on faulty assumptions or context. We leveraged the query transformation algorithm from [15] that strategically rephrases and modifies a query. Each query goes through this component for error correction as follows. We prompt ChatGPT with instructions on how to transform a query based on the question type. For example, for a Yes/No query type, we instruct ChatGPT as follows Change is/are/have there issues to check if there are any issues with the provided condition. For a summarization type question, the instruction is Summarize the contents from issue title, exceptions, body, and labels. We then provide some example transformations (in a few-shot setting) to clarify the instruction, like the following. Original Query: Are issue 18102 and 18669 similar? Transformed Query: Compare the exceptions, stack traces, and descriptions of issues 18102 and 18669 to Finally, GPT answers based on the transformed query. For example, when presented with the query, Q6 from Section V, the Query Preprocessor directs the question with additional in- structions: Summarize the team responsible for issue 104160 based on the assignee and labels. This instruction results in generating a more precise SQL query to our database (we used LangChain SQL engine + RAG ChatGPT pipeline).Asking CHIME with Q6 from Section V. ElasticSearch Question: Which team is responsible for issue 104160? Correct Answer: The team responsible for issue 104160 is the ML team, with no specific assignee and labels indicating it is related to ML. The third component of CHIME is added to validate ini- tial responses from RAG-based ChatGPT. We employed two strategies sequentially, CoVe [16] and Metamorphic Testing (MT) [17]. Drawing inspiration from investigative interroga- tion techniques, CoVes methodology is designed to test the consistency and authenticity of ChatGPTs responses through a series of challenge questions. CoVes process can be unfolded into three distinct stages 1) Upon receipt of a query, ChatGPT formulates an initial response based on its interpretation of the query within the context of available data. 2) CoVe then autonomously generates a set of follow-up questions related to the initial response. CoVe dynamically determines the number of follow-up questions. These questions aim to clarify its preceding statements. 3) The subsequent responses are ana- lyzed for consistency with the initial response. Any observed contradictions signal the need for a review and adjustment of the initial response, leading to the refinement or generation of a new, more accurate answer. Listing 2: How CoVe validates in CHIME Initial Question: Is issue 18102 and 18669 similar? Response: No, issue 18102 and 18669 are not similar. Follow-up Questions Generated by CoVe 1. Can issue 18102 and 18669 be considered similar based on their descriptions and reported problems? 2. Have there been any interactions or dependencies between the teams working on issue 18102 and 18669 that could 3. Are the issues 18102 and 18669 related to the same error? ChatGPT Responses to Follow-up Questions 1. Yes, based on the descriptions and reported problems, they can be considered similar as both involve 2. Yes, there have been interactions between the teams working on issue 18102 and 18669. 3. No, issues 18102 and 18669 are not related to same error. Final Response: Yes, issue 18102 and 18669 are similar. However, CoVe can discard correct responses (if not found in responses to challenge questions) or promote incorrect responses (if responses are found both in base and challenge questions). We thus refine CoVe with MT as follows. Using CoVe to verify ChatGPTs initial responses, we get an intermediate response which is then passed for consistency verification to MT. Central to MT is the concept of metamor- phic relations (MRs), which involve applying transformationsto input queries to anticipate changes in output. In CHIME, this approach is leveraged to introduce nuanced variations to the queries for testing the consistency of its responses under diverse conditions. A specific implementation of MRs in CHIME involves Sentence-Level Mutation, where equivalent questions are generated to test. This process includes 1) using predefined MRs to subtly alter the phrasing or focus of the original queries. In our implementation, MT generates three mutated questions each time in CHIME. For instance, the original query might be What causes error X in module Y?, and its mutated equivalent could be Why does error X occur in module Y?. 2) The responses ChatGPT provides for both the original and mutated queries are compared to assess consistency with MR expectations. If an MR implies that adding specific details should refine the response, the eval- uation checks for this level of refinement. After the refinement via MR, CHIME generates the final response for the users. The user in CHIME does not see the mutated questions from CoVe or the mutated questions from MRs. Instead, the user only sees the final response from CHIME. However, CHIME can print the mutated questions to the user if needed. Listing 3: How CoVe + MT validates in CHIME Initial Question: Does Elasticsearch require the UseG1GC option to be present during issue 18151 startup stage? Initial ChatGPT Response: No, Elasticsearch does not require the UseG1GC option to be present during its startup stage in issue 18151. Follow-up Questions Generated by CoVE : 1. Is Elasticsearch the software mentioned in the response? 2. Is the UseG1GC option not required? 3. Is issue 18151 not requiring the UseG1GC option? ChatGPT Responses to Follow-up Questions : 1. Yes, Elasticsearch is mentioned in the response. 2. No, the UseG1GC option is not required. 3. No, issue 18151 is not requiring the UseG1GC option. CoVE : No, the UseG1GC option is not required during Mutated Questions Generated by MT : 1. Is the UseG1GC option necessary for Elasticsearch to be present during its startup stage in issue 18151? 2. Must the UseG1GC option be included during Elasticsearch s startup stage for issue 18151? 3. Is it required to have the UseG1GC option present during Elasticsearchs startup stage for issue 18151? ChatGPT Responses to Mutated Questions : 1. No, the UseG1GC option is not necessary for Elasticsearch s to be present during its startup stage in issue 18151. 2. Yes, the UseG1GC option must be included during Elasticsearchs startup stage for issue 18151. 3. Yes, it is required to have the UseG1GC option present during Elasticsearchs startup stage for issue 18151. Final Response: Yes, it is required to have the UseG1GC option during Elasticsearchs startup stage .VII. E FFECTIVENESS OF CHIME We evaluate CHIME by answering three RQs: RQ3. How well can CHIME fix inaccuracies in ChatGPT RQ4. How well do individual components in CHIME per- RQ5. Would responses from CHIME be favored like those from ChatGPT when both are correct? RQ3 and RQ4 investigate whether CHIME and its components can fix inaccuracies in ChatGPT while we use our benchmark. Similar to RQ2, we use the correctness metric from Equation 1 to answer RQ3 and RQ4. RQ5 assesses the usability of CHIME when it is used by developers instead of a RAG-based ChatGPT. We conduct a user study to answer RQ5. TABLE VII: Correctness of CHIME. Column Improv shows percent improvement over RAG-based ChatGPT T# Y/N Fact Sum Total Improv Total 80.0% 61.4% 65.5% 66.7% +30.3% T1 - Issue Anlys(S) 83.3% 67.1% 66.7% 70.8% +36.3% T1 - Issue Anlys(M) 58.3% 30.0% 50.0% 42.5% +20.0% T2 - Issue Trend 68.8% 43.8% 50.0% 55.0% +22.5% T3 - Issue Summary 0.0% 87.5% 68.8% 72.5% +22.5% T4 - Issue Labeling 83.3% 60.0% 87.5% 72.5% +22.5% T5 - Issue Backlog 100.0% 58.3% 50.0% 70.0% +32.5% A. How well can CHIME fix ChatGPT inaccuracies? (RQ3) Table VII presents the assessments of the correctness of CHIME in our benchmark by offering overall results, across the three types of queries and also across the five task types in our benchmark. Overall, CHIME offers around 30.3% improvement over the RAG-based ChatGPT pipeline from Section V. The improvement is consistent across all five task types, with issue analytics and backlog tasks benefiting the most from CHIME. CHIME showcases enhancements over ChatGPT across all tasks for both OpenJ9 and ElasticSearch: 29.6% and 31.1% improvement over ChatGPT for OpenJ9 and ElasticSearch respectively. The detailed result for each project is provided in our online appendix. We manually assessed the responses where CHIME was inaccurate and observed three main reasons as follows. Query-Directed Retrieval Failure (60.6%) : CHIME relies on its ability to query a database of stored data and generate SQL queries based on the provided questions. However, when user or verifying queries lack clarity, the query fails to provide clear instructions for formulating SQL queries. In such cases, CHIME produces incorrect or irrelevant responses. Logical Inference Errors (27%) : This pertains to cases where CHIME fails to accurately apply logical inference principles. It occurs when CHIME incorrectly deduces information from the data or makes faulty assumptions during reasoning. Semantic Discrepancy (10.9%) : CHIME relies on similarity scores to match user queries with existing data or responses. However, discrepancies in semantic similarity assessments can lead to incorrect matches or associations.TABLE VIII: Correctness of components of CHIME. Column Improv shows percent improvement over RAG-based Chat- Component Y/N Fact Sum Total Improv Issue Preprocessor 59.0% 43.9% 46.4% 48.1% +11.7% Query Preprocessor 57.0% 35.1% 45.2% 42.5% +6.1% Response Validator 55.0% 36.0% 47.6% 43.0% +6.6% CoVe 58.0% 28.9% 38.1% 37.9% +1.5% MT 69.0% 37.7% 53.6% 48.5% +12.1% Summary of RQ3. CHIME achieves an average correct- ness of 66.7% and an improvement of 30.3% over a RAG- based ChatGPT on our benchmark of bug report questions. B. How do individual components in CHIME perform? (RQ4) We ran each component of CHIME individually and deter- mined the contribution of the component within the pipeline. In Table VIII, we show the performance of each component per query type and also show whether the component offered an improvement over a RAG-based ChatGPT. We discuss how we ran each component while analyzing the results below. Issue Preprocessor. In our CHIME pipeline, we kept this component and removed the other two components (i.e., Query Processor and Response Validator). Hence, issue reports are preprocessed by this component and then stored in the database. From here, we utilize RAG-based ChatGPT for Q&A. The integration of the Issue Processor enhances the accuracy of the baseline GPT model by 11.7% on average for both projects. This improvement is particularly notable in technical question comprehension and analysis tasks. Query Preprocessor. Similar to the above setup, we only kept this component and removed the other two components in our CHIME pipeline (i.e., Issue Preprocessor and Response Val- idator). On average, this process demonstrates an improvement of 6.1% over a RAG-based ChatGPT. Response Validator. We used it to validate responses from an RAG-based ChatGPT. Overall, this component contributed to a 6.6% improvement over a RAG-based ChatGPT. The bottom two rows in Table VIII further illustrate the performance of the two modules in the Response Validator, i.e., CoVe and MT. Interestingly, MT as an individual module worked even better than the Response Validator component. MT offered a 12.1% improvement over RAG-ChatGPT while CoVe offered a 1.5% improvement. However, we kept the combinations of CoVe and MT in the response validator, because CoVe + MT may become more useful for other repos where responses may need a sequence of challenges via both CoVe and MT. For example, when the responses from CoVe contain references to the fact (but with incorrect summarization), MT can double- check those facts via follow-up mutated questions. As we can see from Tables VII and VIII, CHIME as an end-to-end pipeline offers 30.3% improvement over RAG-based ChatGPT, while none of the individual components in CHIME could offer more than 12% improvement over RAG-based ChatGPT. This means that the ensemble of all the components in CHIMEs pipeline helped the fixing of ones mistake by others. For instance, when a user queries List all pending issues rather than simply providing the count of pending issues, the transformed query from Query Preprocessor prompts the system to generate a list of issue numbers, which increases its accuracy. Summary of RQ4. Each component in CHIME can offer an improvement over a RAG-based ChatGPT by correcting the inaccuracies in ChatGPT responses. The components work best when they are all put together in CHIME as an C. Would responses from CHIME favored like those from ChatGPT when both are correct? (RQ5) A comparative study was conducted to assess the practical efficacy of CHIME, involving 31 participants. The majority (93%) had 0-5 years of experience in the software industry, with 57% being software developers and 33% researchers. 1) Survey Setup: Participants were presented with two random questions from each task in our benchmark dataset. The questions include a summarization of the failure of an issue, similarities between multiple issues, recurring themes in a component, pending issues, identification of error-prone components, unresolved or blocker issues, guidelines for label- ing, and label suggestions. For these questions, responses from both CHIME and ChatGPT, along with links to associated bug reports, were provided for evaluation. To ensure a fair comparison, only questions with correct responses from both systems were selected. Participants were then asked to rate the correctness and perceived usefulness of the responses in addressing software bug-related queries. The survey questions are provided in our online appendix. 2) Survey Result: Participant feedback in Table IX indi- cates that CHIME was the preferred choice for the majority of tasks when the answers were correct and selected more frequently for 6 out of 10 questions. It was favored in issue analysis (T1) with a 79% participant preference. On average, for this task, 63.6% of participants found the responses to be comprehensive and covering all necessary aspects, 45.1% felt that they provided additional information helpful for a better understanding of the problem, and 33.3% thought the responses were clear and easy to follow. Their preference also extended to issue summarization (T3) with a 65% preference and to issue labeling (T4), with a 63% preference. However, for the issue trending task (T2), there was a slight preference for ChatGPT. Nonetheless, for two questions, participants seemed undecided, indicating a comparable level of usefulness between CHIME and ChatGPT when the responses were correct.0.60.65 0.70.75 0.80.85 0.90.9510%20%30%40%50%60%70%80%90% ThresholdGPT Issue Pre. Query Pre. RV Fig. 9: Impact of Threshold on Similarity Analysis. Here, Issue Pre. = Issue Preprocessor, Query Pre. = Query Preprocessor, RV = Response validator TABLE IX: Selection Preference of CHIME and ChatGPT Provided Correct Responses across Tasks T1 - Issue Analytics21% 79% T2 - Issue Trend58% 42% T3 - Issue Summary35% 65% T4 - Issue Labeling37% 63% T5 - Issue Backlog48% 52% Summary of RQ5. In a comparative study with 31 partici- pants, CHIME responses were preferred over a stand-alone ChatGPT for the majority of tasks when both provided correct answers. This preference was particularly evident for tasks related to issue analysis, summarization, and A. Accuracy of our CFG We evaluated the CFG-based stack trace parsing by as- sessing the 80 issue reports that we used to create our benchmark dataset. The CFG is designed to identify key elements, such as exception types, messages, and code details (e.g., class/method/file names, etc.). For each stack trace, we checked whether the parser found all key elements as expected. We used three metrics to compute accuracy: precision, recall, and F1-score. Precision is the ratio of correctly identified elements to the total elements identified by the parser. Recall is the ratio of correctly identified elements to the total actual elements in the stack trace. F1-score ( F1) is the harmonicmean of precision and recall. We manually created a list of the expected elements for accurate comparison for each of the 80 issue reports. We observed an average precision of 0.99 and recall of 0.91 (F1-score = 0.93). The few errors in parsing were mainly due to the limitations in our regular expressions used in the CFG parser, and the variations in stack trace formats across issue reports. Our replication package contains the details of B. Threats to Validity Concerns regarding construct validity arise from the bench- marks design. However, we derived our benchmark queries from survey responses. The selection of issues from OpenJ9 and ElasticSearch may affect the generalizability of the find- ings across various software engineering contexts. The partic- ipant pool in surveys might not comprehensively represent the diverse perspectives in the broader software engineering com- munity. Finally, the methodology used for evaluating CHIME could affect the accuracy and objectivity of our effectiveness assessment and may introduce methodological bias . However, we have analyzed the accuracy rates of summary queries across various similarity threshold values of CHIME, ranging from 0.60 to 0.95, and chose the threshold of 0.7 as it gives the highest accuracy (see Figure 9), an optimal trade-off between capturing relevant information and minimizing false positives. We have introduced CHIME to mitigate the inaccuracy of ChatGPT response during bug report exploration. CHIME demonstrates 30.3% improvements over ChatGPT in terms of providing more correct responses for bug exploration tasks. Our industrial partner is working on deploying CHIME as a Slack bot. The conceptualization of CHIME originated from an internal demo of a similar chatbot created by our partner one year ago. Their initial chatbot lacked the required accuracy, which we sought to address by developing CHIME. To further motivate the need for such a chatbot beyond our industrial partner, we conducted a survey of 47 software practitioners (see Section III). The survey findings highlight the necessity of such chatbots in the real world. Feedback from 31 industry participants, presented in Section VII-C shows that CHIME is preferred for its ability to analyze, summarise, and label issues. Our industrial partner was involved in the design and evalu- ation of CHIME, which was crucial for advancing CHIME from the proof-of-concept stage to the current deployment stage within the company. Like any innovation, we expect to improve CHIME in an agile manner, i.e., based on user In the future, we will also focus on expanding CHIMEs grasp of more technical terminologies and other documents. To handle other documents, in CHIME we will improve the issue preprocessor module e.g., to separate code and textual contents, and to adapt the CFG to handle code snippet- s/traces/crash dumps in those documents or using a static partial program analyzer to handle code examples in API documentation. We expect that the other modules in CHIME can be used with minimal changes.DATA AVAILABILITY The code and data used for this study can be found here: [1] Z. Ji et al. , Survey of hallucination in natural language generation, ACM Computing Surveys , vol. 55, no. 12, pp. 1 38, 2023. [2] K. Filippova, Controlled hallucinations: Learning to generate faithfully from noisy data, in Findings of the Association for Computational Linguistics: EMNLP 2020 , T. Cohn, Y . He, and Y . Liu, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 864 870. [3] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, ArXiv , vol. abs/2311.05232, 2023. [Online]. Available: [4] M. Jang, D. S. Kwon, and T. Lukasiewicz, BECEL: Benchmark for consistency evaluation of language models, in Proceedings of the 29th International Conference on Computational Linguistics , N. Calzolari, C.-R. Huang, H. Kim, J. Pustejovsky, L. Wanner, K.-S. Choi, P.-M. Ryu, H.-H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim, Y . Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S.-H. Na, Eds. Gyeongju, Republic of Korea: International Committee on Computational Linguistics, Oct. 2022, pp. 3680 3696. [Online]. [5] M. Jang and T. Lukasiewicz, Consistency analysis of chatgpt, in Conference on Empirical Methods in Natural Language Processing , 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID: [6] R. Cohen, M. Hamri, M. Geva, and A. Globerson, LM vs LM: Detecting factual errors via cross examination, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 12 621 12 640. [Online]. Available: https://aclanthology.org/2023.emnlp-main. [7] B. A. Galitsky, Truth-o-meter: Collaborating with llm in fighting its [8] OpenJ9. (2024, February) Issue 18151: To accept openjdk option +useg1gc to enable startup of elasticsearch application]. [Online]. [9] D. Arya, W. Wang, J. L. C. Guo, and J. Cheng, Analysis and detection of information types of open source software issue discussions, inProceedings of the 41st International Conference on Software Engineering , ser. ICSE 19. IEEE Press, 2019, p. 454 464. [Online]. [10] Z. Jiang, F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang, J. Callan, and G. Neubig, Active retrieval augmented generation, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 7969 7992. [Online]. Available: https: [11] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K uttler, M. Lewis, W.-t. Yih, T. Rockt aschel, S. Riedel, and D. Kiela, Retrieval-augmented generation for knowledge-intensive nlp tasks, in Proceedings of the 34th International Conference on Neural Information Processing Systems , ser. NIPS20. Red Hook, NY , USA: Curran [12] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang, Realm: retrieval-augmented language model pre-training, in Proceedings of the 37th International Conference on Machine Learning , ser. ICML20. [13] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, Retrieval augmentation reduces hallucination in conversation, in Findings of the Association for Computational Linguistics: EMNLP 2021 , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3784 3803. [Online]. Available: https: [14] K. Beelen and D. van Strien, Metadata might make language models better, 2022. [Online]. Available: https://arxiv.org/abs/2211.10086[15] X. Ma, Y . Gong, P. He, hai zhao, and N. Duan, Query rewriting in retrieval-augmented large language models, in The 2023 Conference on Empirical Methods in Natural Language Processing , 2023. [Online]. [16] S. Z. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. E. Weston, Chain-of-verification reduces hallucination in large language models, 2024. [Online]. Available: [17] T. Chen, F. Kuo, H. Liu, P. Poon, D. Towey, T. Tse, and Z. Zhou, Metamorphic testing: A review of challenges and opportunities, ACM Computing Surveys , vol. 51, no. 1, pp. 1 27, 2018. [18] C. Wang and R. Sennrich, On exposure bias, hallucination and domain shift in neural machine translation, 07 2020. [19] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang, Exposing attention glitches with flip-flop language modeling, in Thirty-seventh Conference on Neural Information Processing Systems , 2023. [Online]. [20] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, Deduplicating training data makes language models better, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 8424 8445. [Online]. [21] N. Dziri, A. Madotto, O. Za ane, and A. J. Bose, Neural path hunter: Reducing hallucination in dialogue systems via path grounding, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 2197 2214. [Online]. [22] H.-S. Chang and A. McCallum, Softmax bottleneck makes language models unable to represent multi-mode word distributions, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 8048 8073. [Online]. [23] B. Dhingra, M. Faruqui, A. P. Parikh, M.-W. Chang, D. Das, and W. W. Cohen, Handling divergent reference texts when evaluating table-to-text generation, ArXiv , vol. abs/1906.01081, 2019. [Online]. [24] S. Lin, J. Hilton, and O. Evans, TruthfulQA: Measuring how models mimic human falsehoods, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 3214 3252. [25] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee, A. Roberts, T. B. Brown, D. X. Song, U. Erlingsson, A. Oprea, and C. Raffel, Extracting training data from large language models, in USENIX Security Symposium , 2020. [Online]. Available: [26] Z. Li, S. Zhang, H. Zhao, Y . Yang, and D. Yang, Batgpt: A bidirectional autoregressive talker from generative pre-trained transformer, ArXiv preprint , vol. abs/2307.00360, 2023. [27] Y . Liu, Y . Yao, J.-F. Ton, X. Zhang, R. Guo, H. Cheng, Y . Klochkov, M. F. Taufiq, and H. Li, Trustworthy llms: A survey and guideline for evaluating large language models alignment, ArXiv preprint , vol. [28] N. Lee, W. Ping, P. Xu, M. Patwary, M. Shoeybi, and B. Catanzaro, Factuality enhanced language models for open-ended text generation, ArXiv , vol. abs/2206.04624, 2022. [Online]. Available: https://api. [29] W. Shi, X. Han, M. Lewis, Y . Tsvetkov, L. Zettlemoyer, and S. W.-t. Yih, Trusting your evidence: Hallucinate less with context-aware decoding, ArXiv preprint , vol. abs/2305.14739, 2023. [30] A. Radford et al. , Language models are unsupervised multitask learn- ers, OpenAI blog , vol. 1, no. 8, p. 9, 2019. [31] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al. , The pile: An 800gb dataset of diverse text for language modeling, arXiv preprint arXiv:2101.00027 , 2021.[32] H. Touvron et al. , Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2306.00186 , 2023. [33] A. Sinitsin, V . Plokhotnyuk, D. Pyrkin, S. Popov, and A. Babenko, Editable neural networks, in 8th International Conference on Learning Representations, ICLR 2020 . Addis Ababa, Ethiopia: OpenReview.net, [34] Y . Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, Editing large language models: Problems, methods, and opportunities, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 10 222 10 240. [Online]. Available: [35] Z. Jiang et al. , Active retrieval augmented generation, arXiv preprint [36] D. Sobania, M. Briesch, C. Hanna, and J. Petke, An analysis of the automatic bug fixing performance of chatgpt, in 2023 IEEE/ACM International Workshop on Automated Program Repair (APR) . Los Alamitos, CA, USA: IEEE Computer Society, may 2023, pp. 23 30. [Online]. Available: https://doi.ieeecomputersociety.org/10. [37] A. Radford and K. Narasimhan, Improving language understanding by generative pre-training, 2018. [Online]. Available: https://api. [38] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. GONG, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. LIU, CodeXGLUE: A machine learning benchmark dataset for code understanding and generation, inThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1) , 2021. [Online]. Available: [39] J. Lanchantin et al. , Learning to reason and memorize with self-notes, arXiv preprint arXiv:2305.00833 , 2023. [40] B. Peng et al. , Check your facts and try again: Improving large language models with external knowledge and automated feedback, arXiv preprint arXiv:2302.12813 , 2023. [41] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, Retrieval augmentation reduces hallucination in conversation, in Findings of the Association for Computational Linguistics: EMNLP 2021 , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3784 3803. [Online]. Available: https: [42] A. Madaan et al. , Self-refine: Iterative refinement with self-feedback, arXiv preprint arXiv:2303.17651 , 2023. [43] C. Malaviya et al. , Quest: A retrieval dataset of entity-seeking queries with implicit set operations, arXiv preprint arXiv:2305.11694 , 2023. [44] P. Manakul et al. , Selfcheckgpt: Zero-resource black-box hallucina- tion detection for generative large language models, arXiv preprint [45] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in North American Chapter of the Association for Computational Linguistics , 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID: [46] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, CodeBERT: A pre-trained model for programming and natural languages, in Findings of the Association for Computational Linguistics: EMNLP 2020 , T. Cohn, Y . He, and Y . Liu, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 1536 1547. [Online]. Available: https://aclanthology.org/2020.findings-emnlp.139[47] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, J. Yin, D. Jiang, and M. Zhou, Graphcodebert: Pre-training code representations with data flow, ArXiv , vol. abs/2009.08366, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID: [48] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, Unified pre-training for program understanding and generation, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani- Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y . Zhou, Eds. Online: Association for Computational Linguistics, Jun. 2021, pp. 2655 2668. [Online]. Available: https://aclanthology.org/2021. [49] C. Raffel, N. M. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, J. Mach. Learn. Res. , vol. 21, pp. 140:1 140:67, 2019. [Online]. Available: [50] L. A. Goodman, Snowball sampling, Annals of Mathematical Statis- tics, vol. 32, no. 1, pp. 148 170, 1961. [51] G. Rodriguez-Perez, G. Robles, and J. M. Gonzalez-Barahona, How much time did it take to notify a bug? two case studies: Elasticsearch and nova, in 2017 IEEE/ACM 8th Workshop on Emerging Trends in Software Metrics (WETSoM) , 2017, pp. 29 35. [52] G. Rodr guez-P erez, Gregorio, Robles, A. Serebrenik, Andy, Zaidman, D. M. Germ an, Jesus, and J. M. Gonzalez-Barahona, How bugs are born: a model to identify how bugs are introduced in software components, Empirical Software Engineering , vol. 25, pp. 1294 1340, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID: [53] T. Zhang, D. Han, V . Vinayakarao, I. C. Irsan, B. Xu, F. Thung, D. Lo, and L. Jiang, Duplicate bug report detection: How far are we? ACM Trans. Softw. Eng. Methodol. , vol. 32, no. 4, may 2023. [Online]. [54] Z. Rasool, S. Barnett, S. Kurniawan, S. Balugo, R. Vasa, C. Chesser, and A. Bahar-Fuchs, Evaluating llms on document-based qa: Exact answer selection and numerical extraction using cogtale dataset, ArXiv , vol. abs/2311.07878, 2023. [Online]. Available: [55] C. Wang, X. Liu, Y . Yue, X. Tang, T. Zhang, C. Jiayang, Y . Yao, W. Gao, X. Hu, Z. Qi, Y . Wang, L. Yang, J. Wang, X. Xie, Z. Zhang, and Y . Zhang, Survey on factuality in large language models: Knowledge, retrieval and domain-specificity, ArXiv , vol. abs/2310.07521, 2023. [Online]. Available: https://api.semanticscholar. [56] M. P. Polak and D. Morgan, Extracting accurate materials data from research papers with conversational language models and prompt engineering, Nature Communications , vol. 15, 2023. [Online]. [57] L. Basyal and M. Sanghvi, Text summarization using large language models: A comparative study of mpt-7b-instruct, falcon-7b-instruct, and openai chat-gpt models, arXiv preprint arXiv:2310.10449 , October 2023. [Online]. Available: https://arxiv.org/abs/2310.10449 [58] LangChain, SQLDatabaseChain, Webpage. [Online]. [59] Hugging Face, all-MiniLM-L6-v2, Webpage. [Online]. Available: [60] F. Rahutomo, T. Kitasuka, and M. Aritsugi, Semantic cosine similarity, 2012. [Online]. Available: https://api.semanticscholar.org/CorpusID:'},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'text': 'Tool-Integrated Reasoning for Bengali Mathematical Department of Computer Science Bangladesh University of Engineering and Technology Department of Computer Science Bangladesh University of Engineering and Technology Abstract In this research paper, we present an innovative approach tailored to solving mathematical problems in Bengali, developed for the DL Sprint 3.0 - BUET CSE Fest 2024 Competition. Our methodology harnesses the power of advanced deep learning models, notably the Qwen 2.5 series, with iter- ative improvements made through prompt engineering, model quantization, and Tool Integrated Reasoning (TIR) to handle complex calculations. Initially, we explored various model archi- tectures, such as fine-tuned Mistral and quantized Qwen models, progressively refining them through translation techniques, RAG (Retrieval-Augmented Generation), and custom dataset curation. Through manual hyperparameter tuning, we optimized param- eters like temperature and top-p to improve model adaptability and response accuracy. Additionally, the removal of RAG and careful parameter adjustments further contributed to our final models robustness. Our approach demonstrates the potential of advanced NLP techniques in effectively interpreting and solving Index Terms Keywords: Bengali Mathematical Problem Solv- ing, Qwen 2.5, Mathematical Reasoning in Bengali, Tool Integrated Reasoning (TIR), Retrieval-Augmented Generation (RAG), Model Quantization, Prompt Engineering, Self-Consistent The ability to understand and solve mathematical problems is a foundational skill for AI, essential for advancements across fields like science, engineering, and finance. However, while AI models have made strides in various languages, they still face significant challenges when tackling mathematical reasoning in low-resource languages, such as Bengali. This gap becomes particularly evident in tasks involving complex problem-solving and precise calculations. To address this, the DL Sprint 3.0 - BUET CSE Fest 2024 Competition [1] intro- duced the unique challenge of building an AI model capable of solving mathematical problems in Bengali, targeting issues akin to those in the Bengali Math Olympiad. This competition not only tests participants technical skills but also aims to push the boundaries of AIs adaptability and performance in Bengali. Our work contributes to this pioneering effort, focusing on enhancing AIs mathematical reasoning in Bengali through advanced NLP and deep learning techniques. Weexplore and iteratively refine state-of-the-art models, such as the Qwen series, alongside strategies like prompt engineering, Tool Integrated Reasoning (TIR), and manual hyperparameter tuning to achieve robust problem-solving capabilities. By contributing to this research, we aim to advance AIs reach into Bengali language processing, ultimately creating models that can assist students, educators, and researchers in tackling complex problems with precision and reliability. For this task, we aimed to select models capable of effi- ciently solving mathematical problems in Bengali, considering both performance and computational efficiency. Our initial approach involved using a fine-tuned Mistral 7B model [2], which, despite being a strong general-purpose model, did not deliver the desired accuracy for mathematical reasoning tasks. This led us to explore other models better suited for handling mathematical challenges, particularly in a low-resource lan- guage like Bengali. The Qwen series emerged as the most promising option due to its strong performance on mathemat- ical reasoning benchmarks. The Qwen-32B-Instruct model [3], with impressive scores on the MATH benchmark (83.1) and GSM8K benchmark (95.9) [4], was especially appealing for its capability in solving high-level mathematical problems. Given its robust performance, we chose to focus on the Qwen-2.5 series, including the 7B, 14B, and 32B models, each offering different trade-offs between accuracy and computational de- mands. We initially fine-tuned the Qwen-14B-Instruct model for a single epoch, which, though promising, did not provide sufficient improvements in performance. To enhance model ef- ficiency, we implemented VLLM (Variable-Length Language Model) for faster inference, which allowed us to speed up the testing process while maintaining model accuracy. Addition- ally, we employed model quantization techniques to reduce memory requirements, making the models more practical for large-scale inference tasks. To further refine performance, we incorporated Tool Integrated Reasoning (TIR), which enabled the model to perform complex calculations using Python. This 8 Nov 2024method improved the models ability to handle mathematical operations effectively. Moreover, manual hyperparameter tun- ing of parameters like temperature and top-p helped optimize the models response accuracy and adaptability. In summary, after exploring various models, we selected the Qwen series, particularly the Qwen-32B-Instruct model, for its exceptional performance in mathematical reasoning. Combined with tech- niques like VLLM, TIR, and hyperparameter optimization, we were able to enhance the models ability to solve mathematical problems in Bengali effectively. In this project, preprocessing was crucial to handling Ben- gali mathematical problems and enhancing the models ability to solve them accurately. Given the complexity of understand- ing Bengali text directly in the initial stage, we leveraged the Qwen-32B-Instruct model to translate Bengali mathematical questions into English for improved processing. This approach enhanced the performance of our model significantly. Prompt tuning played a key role in optimizing the per- formance of our model as we experimented with various reasoning techniques, including Chain of Thought (COT), Tool Integrated Reasoning (TIR), RAG, self-consistent TIR, and self-consistent COT. We initially designed prompts to guide the model through step-by-step reasoning for COT. For TIR, we adapted the prompts to instruct the model to perform calculations using Python tools for more complex problems. With Self-COT and Self-TIR, the prompts were modified to encourage the model to generate multiple reasoning paths and select the most consistent solution. we experimented with Retrieval-Augmented Generation (RAG). The RAG approach was implemented to provide context and improve the quality of answers in both Bengali and translated English questions. However , after experimenting with Retrieval-Augmented Generation (RAG), we decided to discontinue its use due to its poor performance, as it did not significantly improve the models accuracy compared to other approaches.Additionally, we fine-tuned hyperparameters such as temperature and topp to control the diversity and confidence of the models responses. Lower temperatures were used for more deterministic answers, while higher values promoted creativity.Adjusting topp helped the model select the most plausible solutions.These prompt tuning and hyperparameter adjustments, particularly with TIR and Self-TIR, significantly improved the models ability to solve complex Bengali math problems. In this section, we present the performance of our deep learning model using various approaches. The model was evaluated on the public leaderboard of the DL Sprint 3.0, with the baseline score being 28 out of 100. We tested different configurations, including using various versions of the Qwen model, translation, retrieval-augmented generation (RAG), and Tool Integrated Reasoning (TIR).Translation RAG TIR Self-Consistency Score No No No No 49 Yes(Qwen2.5-14B-Instruct) No No No 61 No Yes No Yes 48 Yes(Qwen2.5-14B-Instruct) Yes No Yes 65 Yes(Qwen2.5-32B-Instruct) Yes No Yes 70 No No No Yes 66 Yes(Qwen2.5-32B-Instruct) No No Yes 73 TABLE I: Performance of Qwen2.5-32B-Instruct with Differ- Temperature Top p Number of Candidates Inference Time(s) Score 0.2 0.9 4 7844.8 68 0.35 0.775 4 7327.0 72 0.4 0.8 4 7391.5 77 0.4 0.8 10 19753.3 77 0.3 0.75 4 7579.1 76 0.3 0.75 5 9721.5 74 0.3 0.75 3 5436.7 71 0.7 0.8 4 7567.7 70 TABLE II: Hyperparameters and performance of the model The experiment began with a baseline score of 28, and the fine-tuned Mistral 7B model scored only 15, indicating the need for a more powerful model architecture and translation capabilities. Scaling to larger Qwen 2.5 models, such as the 7B and 14B-Instruct, improved performance to 40 points but still showed limitations with certain problem types. The quantized Qwen 2.5-32B-Instruct model performed better, reaching a score of 49 without translation, benefiting from its larger Introducing translation using the 14B-Instruct model boosted the score to 61, allowing the model to leverage pre- existing English knowledge. Further improvement occurred when translation was done using the 32B model. Incorporating Retrieval-Augmented Generation (RAG) with translated En- glish questions raised the score to 70, showing that additional context from external datasets enhanced model performance. However, removing RAG led to a score of 73, suggesting that it may have introduced noise, or the larger model could handle The introduction of Tool Integrated Reasoning (TIR), where Python was used for complex calculations, brought the score to 76. This step significantly enhanced the models efficiency and accuracy, especially with Bengali questions, which saw a notable improvement. The final model, optimized with prompt engineering and hyperparameter tuning, achieved a score of 77, marking the culmination of the models refinement in handling diverse problem types.Model Translation TIR Self-Consistency Score Deepseek-math-7b-Instruct(Baseline) No No Yes 28 mCot(Fine-tuned Mistral 7B) No No No 15 Qwen2.5-14B-Instruct No No No 40 Qwen2.5-Math7B-Instruct No Yes No 39 Yes Yes No 48 NuminaMath-7b-TIR(Finetuned Deepseek-math-7b) Yes(Qwen2.5-14B-Instruct) Yes Yes 66 Yes(Qwen2.5-32B-Instruct) Yes Yes 68 TABLE III: Performance of other tried models with Different Configurations Overall pipeline of our solution Fig. 1: Iterative pipeline Throughout the development of our AI model for solving Bengali mathematical problems, we encountered several key Bengali questions presented without translation consistently scored lower than their translated counterparts. Translating questions into English improved model understanding andproblem-solving, leveraging the models richer pre-trained B. Model Size for Translation Using the Qwen 2.5-32B-Instruct model for translation yielded better results compared to the Qwen 2.5-14B-Instruct model. The larger model demonstrated superior language comprehension, which improved translation quality and con- tributed to overall score improvements.C. RAG Limitations Initial experiments with Retrieval-Augmented Generation (RAG) aimed to enhance the models responses by adding contextual information. However, RAG often introduced noise, leading to a decrease in performance. Removing RAG sub- sequently improved scores, indicating that the inherent ca- pabilities of the larger Qwen models were sufficient without additional retrieved context for this task. D. Scarcity of Bengali Datasets The limited availability of high-quality Bengali mathemati- cal datasets restricted the models exposure to diverse problem E. Inference Optimization with VLLM Integrating Variable-Length Language Modeling (VLLM) enabled faster inference, which was crucial for handling the computational demands of larger models. This optimization helped streamline the testing process within our resource F . Prompt Sensitivity The model exhibited a high sensitivity to prompt wording, with minor adjustments in phrasing significantly affecting output quality and accuracy. Effective prompt engineering became essential to guide the model toward optimal solutions, underscoring the importance of precision in prompt construc- G. Quantization for Resource Constraints Due to Kaggles limited GPU memory, we had to quantize the larger Qwen models to make them feasible for testing. This process reduced memory requirements, allowing us to utilize the larger 32B model in a resource-limited environment without compromising performance excessively. H. Fine-Tuning Limitations for Larger Models While fine-tuning smaller Qwen models was manageable, memory constraints in Kaggle prevented us from fine-tuning the Qwen 2.5-32B model. This limitation hindered further customization of the 32B model, restricting our ability to fine- tune it for specific Bengali problem-solving tasks. V. C ONCLUSION AND FUTURE WORK This paper presented a novel approach to solving Bengali mathematical problems by leveraging the Qwen 2.5 series models and optimizing through prompt engineering, trans- lation, and Tool Integrated Reasoning (TIR). Our methods demonstrated the efficacy of advanced NLP techniques in mathematical problem-solving, particularly for low-resource languages like Bengali. The Qwen 2.5-32B model, combined with techniques such as self-consistency and TIR, achieved significant improvements over baseline models, highlighting the models potential in handling complex reasoning tasks with minimal Bengali-specific data. Despite these achievements, challenges remain, especially regarding translation dependen- cies, limited Bengali datasets, and memory constraints forlarge models. These factors impacted both model performance and adaptability to a wider range of problem types. Our results also revealed the sensitivity of model outputs to prompt phrasing, underscoring the need for refined prompt engineering To build on this work, we propose the following areas for Enhanced Bengali Data Collection: Increasing the availability and diversity of Bengali mathematical datasets will enable better model training and adaptability to complex problem types. Domain-Specific Fine-Tuning: With sufficient resources, fine-tuning the Qwen 2.5-32B model specifically on Ben- gali math problems could further improve accuracy and Optimized Prompt Engineering: Researching prompt optimization strategies for mathematical reasoning tasks, especially in low-resource languages, could make the model responses more consistent and accurate. Exploration of Lightweight Models: Investigating smaller, efficient models tailored for Bengali could bal- ance memory constraints and inference speed without By addressing these areas, we aim to enhance the utility of deep learning models in Bengali problem-solving, making ad- vanced educational tools more accessible to Bengali-speaking learners and educators. This research lays a foundation for further progress in low-resource language applications of AI in mathematical education and reasoning. [1] S. Sushmit, A. Rahman, A. Azad, A. Rahman, M. S. Hossain, N. Tahmid, S. Islam, F. A. Akash, M. Islam, A. Muhtasim, T. Tahsin, and T. T. Dipto, DL Sprint 3.0 Bengali AI Math Olympiad, Kaggle, 2024. [Online]. [2] H. Lai and M. Nissim, mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics , Bangkok, Thailand, Aug. 2024. Association for Computational Linguis- [3] Qwen Team, Qwen2.5: A Party of Foundation Models, Sept. 2024. [4] Qwen Team, Qwen2.5 Overview, Qwen2 Official Website. Available:'},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'text': 'ENHANCING CLUSTER RESILIENCE : LLM- AGENT BASED AUTONOMOUS INTELLIGENT CLUSTER DIAGNOSIS SYSTEM AND EVALUATION FRAMEWORK Honghao Shi1Longkai Cheng1Wenli Wu1Yuhang Wang1Xuan Liu1Shaokai Nie1Weixv Wang1 Xuebin Min1Chunlei Men1Yonghua Lin1 Recent advancements in Large Language Models (LLMs) and related technologies such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have enabled the creation of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have developed an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovations include a knowledge base tailored for cluster diagnostics, enhanced LLM algorithms, practical deployment strategies for agents, and a benchmark specifically designed for evaluating LLM capabilities in this domain. Through extensive experimentation across multiple dimensions, we have demonstrated the superiority of our system in addressing the challenges faced in cluster diagnostics, particularly in detecting and rectifying performance issues more efficiently and accurately than traditional methods. Recent advancements in Large Language Models (LLMs) and complementary technologies such as Retrieval- Augmented Generation (RAG) and Diagram of Thought (DoT) have paved the way for the development of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have created an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovative approach includes the establishment of a specialized knowledge base for cluster diagnostics, the enhancement of LLM algorithms to better suit the demands of the domain, practical deployment strategies for agents within real-world environments, and the development of a benchmark specifically tailored to evaluate LLM capabilities in the context of cluster diagnostics. These components collectively contribute to a robust framework that addresses the complexities inherent in managing AI clusters, particularly in scenarios involving performance degradation or other operational anomalies. Through rigorous experimentation, we have validated the effectiveness of our LLM-agent system across multiple dimensions. Our benchmark, which consists of 150 manually crafted advanced questions, serves *Equal contribution1Beijing Academy of Artificial Intelli- gence, Beijing, China. Correspondence to: Yonghua Lin <yh- >.as a comprehensive evaluation tool that highlights the performance differences between our enhanced LLM-agent and baseline open-source models. In practical applications, the LLM-agent demonstrates its superior capability to identify and resolve performance issues more efficiently than traditional methods, reducing the troubleshooting time significantly. For instance, in a simulated scenario where one GPU was throttled to a much lower frequency, our system identified and resolved the issue within a matter of minutes, whereas conventional approaches would have taken a senior operations engineer nearly an hour to diagnose and rectify using pre-written automated detection Moreover, the LLM-agents ability to detect and initiate corrective actions even before the performance degrada- tion is noticed by human operators marks a significant advancement in proactive system maintenance. This capability not only mitigates immediate issues but also enhances the overall availability and reliability of the cluster by preemptively addressing potential faults. By leveraging the strengths of RAG and DoT, the LLM-agent can autonomously execute remediation measures, thereby freeing up engineering resources to focus on more complex and value-driven tasks. Our research underscores the transformative potential of combining AI-driven diagnostics with practical deployment strategies, setting the stage for a new era of intelligent cluster management solutions. 8 Nov 2024Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework 2.1 LLMs Alignment and Enhancement In recent years, generative artificial intelligence centered around large language models(LLMs) has seen rapid development, with powerful natural language generating capabilities demonstrated by proprietary models such as the GPT series(Achiam et al., 2023) and Gemini series(Team et al., 2023), as well as open-source models like Llama(Dubey et al., 2024) and Qwen(Yang et al., There are multiple approaches to enhancing the capabil- ities of LLMs across different stages such as training, inference, and deployment, as well as in areas like data, algorithms, and computational resources. In light of the achievements of autoregressive models like GPT- 2(decoder-only transformers)(Radford et al., 2019) and LLaMA(transformer++)(Touvron et al., 2023), enhancing the quality of the data has become a critical method for improving the efficacy of models during the pre-training process(Adler et al., 2024; Liu et al., 2024). For modern LLMs, there exists several training or fine- tuning works between pre-training and the deployment. ChatGPT(Ouyang et al., 2022) describes this process as Su- pervised Fine-Tuning (SFT), Reward Modeling (RM), and Reinforcement Learning with Human Feedback (RLHF), while LLaMA3.1(Dubey et al., 2024) integrates these into a continuous process known as Continue Training. Besides training, LLMs can leverage Retrieval-Augmented Generation (RAG)(Lewis et al., 2020) to utilize knowledge from data distributions that were not part of the training set. We can refer to the above content as the alignment and 2.2 AI-agent based Applications After the model parameters have been frozen, it is possible to enhance the inherent capabilities of the model through mechanisms such as chain-of-thought(CoT) reasoning(Wei et al., 2022), scaling test time(Snell et al., 2024), and com- bining CoT LLM and AI agents(Castelfranchi, 1998) as LLM-agent(Park et al., 2023). CoT is a prompting technique used to guide LLMs to generate intermediate reasoning steps before arriving at a final conclusion. There are extensions to classic CoT, such as Tree of Thought (ToT)(Yao et al., 2024) for tree- like backtracking, Graph of Thought (GoT)(Besta et al., 2024) for graph-based reasoning, and Diagram of Thought (DoT)(Zhang et al., 2024) for a propose-critique-summarize approach based on topos theory. The development of CoT and the scaling of test-time are unified, with CoT applications always aiming to maintain optimal results with limited test-time or scaling test-time toachieve extraordinaire results(Snell et al., 2024). The CoT series technics are also one of the foundations for build- ing LLM-agents. LLM-agents can leverage LLMs as the processing core while integrating traditional AI-agent capa- bilities such as memory, planning, and execution, creating semi-autonomous software entities that are highly adaptive and capable(Xi et al., 2023). 2.3 Diagnosis and Repair for AI Clusters Constructing and utilizing LLM applications typically re- quire hardware infrastructure on a scale costing millions of or more dollars. Meta constructed the LLM application core LLaMA 3.1 within 54 days, leveraging a cluster that included 16,000 GPUs(Dubey et al., 2024), with just the GPU costs amounting to over billion dollars. However, such complex and expensive systems face significant chal- lenges in terms of reliability and availability. During the 54-day training, the Meta cluster experienced 419 unex- pected interruptions, averaging one disruption every three hours. At such a frequency of interruptions, the cluster, from the operating system to the AI framework and distributed scheduling software, requires the ability to capture, identify, attribute, and repair exceptions to ensure successful and ef- ficient model training. Microsofts Superbench(Xiong et al., 2024) has systematically built a suite of standard test cases to comprehensively assess the availability of clusters. In terms of capture and repair, the Torch(Paszke et al., 2019) Elastic solution aims to enable automatic restarts of model training, while works such as FlashCheckpoint- ing in DLRover(Wang et al., 2023) focus on reducing the cost of checkpoint saving and loading during the automatic restart process. Building upon automatic restart capabili- ties, many works at the AI framework level have conducted research and practical implementations to enhance reliabil- ity and availability, particularly those featuring highly cus- tomized solutions based on Megatron(Shoeybi et al., 2019). ByteDances Megascale(Jiang et al., 2024) and Alibabas Pai-Megatron(Qian et al., 2024) both provide toolkits for cluster diagnostics, which are used to check the health of servers and networks, as well as to perform manual or auto- mated error identification and repair. With the advancement of AI technologies, researchers are beginning to explore the use of AI techniques to address cluster diagnostic issues. Using big data techniques to ana- lyze log files was an typical approach to automating cluster diagnostics(Jung & Chung, 2021). However, such meth- ods primarily involve static or real-time analysis of files produced by the training process, which limits their attribu- tion capabilities and means they lack intelligent autonomy, relying instead on pre-written execution and planning pro- cedure.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework AI computing tasks: refers to programs or processes designed to achieve intelligence, such as training large language models, inference with large language models, world model inference, and LLM-agent inference. AI chips: processors suitable for or dedicated to performing AI computing tasks, such as NVIDIA GPUs, Intel Gaudi AI accelerators, and Google TPUs(Jouppi et al., 2017). AI servers: computers equipped with AI chips that are suitable for or specifically designed to perform AI computing tasks, such as the NVIDIA DGX H100. AI servers often have requirements beyond those of classic servers in terms of stability, availability, cooling, and power AI cluster: a distributed server cluster composed of two or more AI servers set up to accomplish a single target task, such as Metas cluster containing 16 thousand GPUs. Additionally, AI servers typically require RDMA or higher bandwidth interconnect protocals, such as InfiniBand RDMA(Shanley, 2003) and RDMA over Converged Ethernet(RoCE)(Guo et al., 2016), and do not usually adopt Cluster diagnosis: ensuring that AI computing tasks can run with normal performance on the AI cluster, promptly detecting task failures, identifying the points of failure, clarifying the reasons for failure, repairing the corresponding faults, and ensuring the overall availability of the AI cluster. We incorporate advanced techniques from the field of LLM alignment and enhancement to creatively develop a solution for building a cluster intelligent maintenance system based on LLM-agents. Figure 1 illustrates the overall process of The upper part of the figure represents the core component of solution: the LLM-agent. The LLM-agent consists of an agent program and an LLM. The LLM interprets the input information provided by the agent as external stimuli and task instructions, and responds appropriately. The agent then directly writes code or calls specific software interfaces based on the feedback from the LLM, thereby operating the cluster. For LLM itself, there are two main challenges. First, how does the LLM acquire domain-specific knowledge of cluster diagnostics, and furthermore, where does this knowledge come from. Second, how can the LLM reason and plan? For the entire LLM-agent, ensuring that the LLMs inputs and outputs match with the actual operations performed by the agent controlling the cluster is another crucial aspect that needs to be addressed. Figure 1. Overview of the Intelligent Maintenance System Based In order to solve the above problems, we have introduced three innovations. First, we use 250 cluster failure records collected from GitHub as a starting point, and treat the cluster operation failure logs actually managed by the LLM-agent as a continuous source of data. We utilize RAG(Lewis et al., 2020) to enable the LLM to capture detailed knowledge corresponding to specific terms within the context. Figure 1 describes the alert, compute cluster, and storage sections, along with their communication with the LLM-agent, which outlines this process. Second, we use DoT(Zhang et al., 2024) enables the model to effectively handle non-natural language information such as symbols, formulas, and code. Similar to vision-text multimodal models, we effectively leverage textual elements that go beyond the inherent meaning of natural language based on DoT. The planning algorithm section at the top of Figure 1 illustrates this innovation. Third, we use self-play technology(Snell et al., 2024) to enable the LLM to autonomously, also intelligently, devides long tasks or challenging reasoning objectives into multiple steps, self-assess the output of each step, and ultimately The lower part of Figure 1 forms the basis of our work. It includes a mature operations alarm troubleshooting and repair process, as well as several mature or advanced software tools. Based on related works, we have developed a unified, multi-level, multi-dimensional cluster diagnostic toolkit as Figure 2. This tool diagnoses the health status of the cluster fromEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Figure 2. Tools for LLM-agent to Diagnose AI Cluster both the supply side and the demand side simultaneously. The bottom part of Figure 2 lists the various components required to build an AI cluster, including the computing component, storage component, network component, and others. AI clusters following different technical routes provide similar capabilities, as shown in the middle part of Figure 2. We inspect all resource supply items affecting AI computing tasks to determine if their content is correct, if their performance is appropriate, and if they are stable. For example, for the feature of RDMA read/write between two GPUs across servers, our tool checks whether the read/write content is correct, whether the IOPS, bandwidth, latency, and other performance metrics are appropriate, and the stability under complex scenarios such as long-duration or multi-process read/writes. Most of these tools are improved versions of packages provided by chip, server, or operating system vendors. The top part of Figure 2 takes the demand side into consideration, evaluating the metric of concern for AI computing tasks with various characteristics. In summary, we have built an LLM-agent capable of retriev- ing and utilizing vast amounts of external information, with autonomous planning, learning, reasoning, and execution capabilities. This LLM-agent works alongside either custom-written tools or existing mature tools to perform early warning, troubleshooting, and repair tasks for the 4.2 Cluster Diagnosis Domain-specific Knowledge Our knowledge base consists of two sources. One part is logs, monitoring information, or program output content, come from pre-collected, cleaned, and organized GitHub data, carefully selected to address pain points in the cluster diagnostics and troubleshooting domain, incorporating knowledge from issues in the GitHub community, also come from operational data acquired after the initial deployment and operation of the LLM-agent. We call it Diagnosis Dataset. The second part is composed of symbolicreasoning. These reasoning structures use AI computation tasks and hardware specification information as input, and through a bottom-up modeling approach, predict the theoretical performance of the given AI computation tasks, thereby determining the correctness of the performance. We drew on effective practices from Alibabas experience in managing cluster startup operations(Xu et al., 2024) to build a database. We cleaned, organized, and structured the unstructured data obtained from GitHub, ultimately forming an effective dataset. We collected over a thousand questions and feedback items from the GitHub issue section. Through automated processes and manual review, we filtered out over 200 entries with substantive knowledge content and well-structured Q&A formats. Each piece of organized data contains four fields: problemkey, rawtext, function, and The problemkey is a domain keyword identified either manually or based on openai o1. Rawtext refers to the original content of a website after simple formatting, stored as a long string containing the questions asked on the web page and the developers responses. The function is based on our cluster diagnosis toolkits and is manually correlated by cluster troubleshooting personnel. This part is used as annotation in the portion of the dataset that the model can perceive, it is not perceived by the model for the answers used in the benchmark evaluation part, and it serves as the starting point for knowledge acquisition after the LLM-agent is deployed. The final results are the causes of the faults extracted from the rawtext based on the developers answers. For an LLM capable of driving an agent to perform cluster diagnostics, we expect it to be able to determine the causes of faults based on real-time operational information from the cluster and to call existing tools or write tool code on-the-fly for cluster repairs, without relying on rawtext containing developer replies. We will demonstrate this capability in subsequent experiments. We use a series of progressive methods to model the correct performance of given AI computation tasks, and through the DoT, we convert this special modal data into tokens to feed into the model. In addition to cluster health check, we have included modules in the toolkits to determine whether different AI computing tasks exhibit correct performance. These modules can, on one hand, be invoked by the agent to provide results to the LLM for analysis, and on the other hand, they can be called by the LLM to have the agent check We start modeling with the simplest task types. ConsideringEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework that existing AI clusters are composed of computing devices with the von Neumann architecture, AI computing tasks require the use of computing cores, memory, and I/O ports. It is worth noting that what AI computing tasks occupy are not narrowly defined CPU computing cores, main memory, or input/output ports, but rather in a broader sense, such as computing cores dedicated to matrix multiplication, HBM memory composed of multi-level caches, and high-speed I/O ports formed by PCIe or RDMA protocols. To build a unified model, we use the concepts of equivalent computing power, equivalent memory bandwidth, and equivalent I/O We refer to computational tasks that occupy or primarily oc- cupy one type of resource as single-resource computational tasks. We construct a single-variable computational task per- formance model and use experiments based on Khinchins law of large numbers to get the results. We assume that for a certain computational task T, the total amount of resource Rirequired is Mi. The hardware running this task can pro- videNiunits of resource Riper second. Assume that the single-variable task Txdepends only on resource R0. We determine M0based on the mathematical formula used for the tasks computation. For N0, we consider it a random variable. Through a large number of repeated experiments after warm-up, we ensure that the difference between the measured results and the expected value of the random vari- able approaches zero. We define performance as the number of times a specific task can be executed per unit time. For the aforementioned task Tx, we predict its performance to For non-single-variable tasks, we focus on modeling whether the different resources they depend on can oper- ate in parallel. A widely used method in multivariate task modeling is the roofline model(Ofenbeck et al., 2014). The roofline model introduces a new variable: task characteristic CT. The Roofline model introduces a new variable: the task characteristic CT. Consider a task Txdepends on two resources R0andR1, the effective utilization of resource R0is plotted on the Y-axis, and the ratio of effective utiliza- tion of resource R0to resource R1is plotted on the X-axis. By changing CT, a scatter plot can be drawn, forming a shape like a roofline. The Roofline model is equivalent to modeling the performance of multivariable tasks under fully parallel scenarios, which does not align with real-world conditions. Additionally, in the context of existing LLM performance modeling, changes in CTare not about varia- tions in the input size of a single task but about the changing proportions of two different primary resource-consuming tasks within the total task. Therefore, we use the proportion of different subtasks as variables to model multivariable tasks for the three main resources provided by AI clusters: equivalent floating- point computing power for matrix multiplication, memory read/write bandwidth, and I/O port bandwidth. The results 0% computeA B CFigure 3. Multi-variable Task Performance Modeling. A shows compute-memory, B shows interconnect-memory, C shows at figure 3 show that computing and memory are in domains that are completely non-parallelizable, whereas computing, memory, and I/O ports can approach full parallelization. This conclusion and related figures have been compiled and placed in the RAG documentation. 4.3 Create LLM-agent with RAG-DoT-Selfplay 4.3.1 Using RAG to Build an LLM That Can Utilize RAG integrates two core components: retrieval and genera- tion. The retrieval module is responsible for finding context- relevant information from an external knowledge base, a process that typically involves indexing large volumes of documents to quickly locate the most pertinent segments. The retrieved information is then passed to the generation module as additional input. The generation module builds upon a pre-trained language model, leveraging the retrieved context to enhance its generation capabilities, thereby pro- ducing responses that are more accurate and better aligned Considering other similar technologies, SFT requires sub- stantial computing resources and may diminish the models inherent generalization capabilities. In-context learning consumes context length and inference time, making it unsuitable for importing datasets with millions of entries. RAG can acquire relevant knowledge during inference with minimal resources and inference time, without altering the weights of the model itself. 4.3.2 Using DoT to Build an Agent That Can Reason and DoT(Diagram of Thoughts)(Zhang et al., 2024) models iter- ative reasoning in LLMs as constructing a Directed Acyclic Graph (DAG) within a single model. The DAG consists of nodes representing propositions, critiques, refinements, and verifications, with edges indicating the logical relationships or dependencies between them. We use XML to handle mul- timodal special symbol data and perform reasoning based Based on the principles of DoT, we use XML tags to sep-Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework arate different types of text, including plain text, special symbols, code, formulas, and inference rules. Thanks to the rope positional encoding adopted by LLama3.1, the model can accurately capture the content within XML pairs. Based on the reasoning graph, our experiments confirmed that this application allows the LLM to correctly reason according to specific rules, achieving the capability to support the agent in completing cluster fault attribution and repair tasks. This significantly exceeds the capabilities of pre-trained or 4.3.3 Using Selfplay Techniques to Construct a With the help of RAG and DoT, the LLM can utilize information from outside the training set as well as abstract symbolic reasoning information. However, this still has limitations for an agent designed for intelligent cluster diagnostics. We permit the LLM to generate content over a longer duration. The quality of solutions to challenging problems can be enhanced through multiple rounds of planned selfplay or spontaneous self-questioning and answering by the agent. Spontaneous self-questioning and answering is applied in DoT reasoning. On the planned selfplay process, we transform the complex problem of cluster fault attribution into a three-round process. In the first round, the agent, based on error logs passed from the cluster, prompts the LLM to identify potential keywords from the error items and corresponding solutions from the knowledge base, performing information extraction and RAG. In the second round, the LLM evaluates its own answers, making corrections or accepting them directly, then proceeds to write or call appropriate tools for the Agent to execute. In the final round, the LLM makes an accurate attribution judgment based on the results of the agents interaction with the actual cluster. Compared to existing selfplay work focused on the text side, we integrate it with the agent, granting it the permissions to operate machines and interact with the environment, fully simulating the capabilities of a human engineer to solve problems. We conducted a three-phase experiment to demonstrate the advanced nature of the proposed LLM-agent in the field of cluster intelligent diagnostics. The first phase involves creating a dataset and benchmark for the field of cluster intelligent diagnostics. First, we define the statistical characteristics of the external data knowledge base and introduce the process of generating an evaluation benchmark from this knowledge base. Next, we describe the features of this benchmark and explain its advanced naturein the field of cluster intelligent diagnostics. Throughout this process, we emphasize fairness and impartiality, strictly distinguishing between the parts of the model that can be perceived and the scoring portions of the evaluation. We further elaborate on the benchmark using the results of the mainstream open-source model LLaMA3.1-70B. The second phase involves evaluating the innovative aspects of the three models we proposed RAG, DoT, and selfplay using the aforementioned benchmark for comparative assessment. The experiments in the second phase are aimed at demonstrating the advanced nature of our proposed models in the field of cluster intelligent In the third phase, we expose the LLM-agent to both the training and testing sets in the benchmark, allowing it to operate in its most complete form to address real-world problems encountered in production environments. We demonstrate the accuracy, efficiency, and autonomous intelligence of this solution through two typical cases. Specifically, we found that this solution can provide early warnings for AI clusters, further enhancing the availability Finally, we will conduct a qualitative analysis and discus- sion on the topics of correctness, safety, and reliability, which are at the forefront of the LLM and LLM-agent fields and have yet to be conclusively resolved, to demonstrate the series of work we have undertaken in these areas. 5.1 Statistics and Evaluation for Dataset and The materials provided to the LLM come from three sources. The first source is automatically collected Q&A data from relevant GitHub communities involved in AI cluster troubleshooting, such as the issue sections of repositories like Megatron, PAI, Deepspeed, and NCCL. This serves as our initial dataset. The data has undergone two rounds of filtering, both automatic and manual, retaining parts with clear solutions and logical dialogues. The second source is the program output obtained by the LLM-agent using RAG+DoT technology on several AI clusters running tasks. These tasks are executed on clusters ranging from 4 to 100 A800 AI servers. The third part consists of special modal data such as symbolic representations and formulas processed using XML according to DoT logic, all of which are unified into the text modality. The total amount of pure text material is 200+ items compared with 1.2GB origin files. This also confirms that if more than 200 items consist of pure text content is fully pre-tokenized to serve as the context for LLM inference, it not only poses a significant challenge to the LLMs capability to handle long texts but also increases theEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework consumption of inference resources, thereby slowing down the execution speed of the LLM-agent. 5.1.2 Benchmarks Source and Statistics for Benchmark We divided the original dataset into two parts, approximately in a 20%-80% ratio. From the 80%, we manually compiled 150 questions to assess the LLMs capabilities in the field of cluster diagnostics. During comparative experiments, unless otherwise specified, we provide only 20% of the original data to all models. During case studies and practical applica- tions, we provide the entire original dataset to the deployed We designed three evaluation metrics. Metric A evaluates the large models information extraction capabilities, in- cluding extracting the cluster IP addresses and SSH port numbers from conversations, as well as the ability to deter- mine whether further execution is needed, evaluated through string matching. The challenge here is to assess the models ability to follow instructions and extract information, since logs are derived from user conversations and may contain unnecessary commands that need to be ignored during the determination process. Metric B evaluates the large models code generation capabilities in the diagnostic domain, in- cluding the ability to generate prescribed code based on descriptions given in conversations, control the input and output of the code, and create unseen test cases, imple- mented in a manner similar to human-eval(Chen et al., 2021) but transferred to a real distributed cluster. Metric C eval- uates the large models information attribution capabilities in the diagnostic domain, including the ability to provide attribution based on users error logs and information. This is currently implemented through multiple-choice questions. 5.1.3 Evaluation of Benchmark on Standard We applied this benchmark to several of the most widely used open-source LLMs, namely LLaMA3.1-70B, nemotron-70B(Adler et al., 2024), mistral-120B(Jiang et al., 2023), and llama3.2 3B. The results is in table 1. Due to the lack of relevant data and information, as well as reasoning logic such as DoT, all models were only able to complete the first task, scoring zero on the second and third tasks. Since the results of llama3.2 3B did not meet the minimum requirements for building the LLM-agent, and the 120B model is difficult to infer on a single AI server, we opted for the better-performing and more widely used LLama3.1-70B out of the two 70B models as the basis for subsequent SFT (Supervised Fine-Tuning) and the application of RAG, DoT, and selfplay.Table 1. Benchmarks Results on Open-source LLMs 70Bno yes 0.8658 0.0 0.0 70Bno yes 0.7315 0.0 0.0 120Bno no 0.7383 0.0 0.0 3Byes yes 0.047 0.0 0.0 Table 2. MMLU Benchmarks Results on LLama3.1 and Nemotron Model SFT or notMMLU We conduct two parts of experiments to comprehen- sively evaluate and compare the innovative effects of our work. In the first part, we use the mature and universal MMLU(Hendrycks et al., 2020) benchmark to evaluate the comprehensive ability of the model in basic text understand- ing after it has been enhanced by RAG, DoT, and self-play. In the second part, through ablation and comparison exper- iments, combined with the focus areas of the sub-items in our proposed benchmark, we quantitatively demonstrate the advantages of our three innovations. 5.2.2 General Capability Evaluation Based on MMLU Firstly, we aim to substantiate why SFT is not advisable in this domain. Although the LLM that supports the agent needs to possess extensive knowledge in cluster diagnostics, performance modeling, and code writing, we discovered that when the LLM reaches a level where this knowledge can be effectively applied, it often lacks the fundamental interaction capabilities required to engage with the agent. We illustrate this point using the MMLU benchmark. We converted the knowledge repository into tokens compatible with the model and constructed an instruction dataset. We iterated through multiple training rounds until the model could respond correctly to instructions. We thenEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Table 3. Multi Comprehensive Benchmarks Results on LLMs 70Bno 0.6246 0.8691 0.8786 0.3720 0.8230 70Byes 0.6032 0.8649 0.8862 0.3680 0.8007 70Bno 0.6280 0.8620 0.8780 0.3680 0.8234 70Byes 0.6126 0.8653 0.8859 0.3580 0.7917 120Bno 0.6544 0.8788 0.9012 0.3980 0.8229 3Bno 0.4352 0.7428 0.7835 0.2800 0.6040 evaluated the SFT model that reached this state against the original open-source model using the Multi-Machine Learning Understanding (MMLU) benchmark. The results are presented in Table 2. From the above results, it can be seen that Supervised Fine-Tuning (SFT) leads to a decline in performance when evaluated using general assessment methods such as MMLU. Subsequently, in our proposed cluster diagnostics benchmark, we further observed adverse consequences of this performance decline in metric C. As a result, we ultimately decided not to use the SFT approach to construct To avoid the potential risks associated with relying solely on MMLU, we further selected three additional LLM benchmarks that are closely related to the problems we aim to solve in our domain or are entirely generalizable: Abstraction and Reasoning Challenge(ARC)(Peter, 2022), BoolQ(Clark et al., 2019), and OpenbookQA(Mihaylov et al., 2018). The results are presented in the table 3. The results of this set of experiments support the conclusions we drew from the MMLU benchmark. 5.2.3 Results of Our Benchmark Table 4 presents all of our experimental results. The second column of the table indicates whether there was cheating. We define experiments that do not participate fairly in the benchmark as cheating. While this is unfair for the benchmark portion, it is clearly meaningful for our core research objective: to build an LLM-agent system that can autonomously and intelligently perform cluster diagnostics and troubleshooting. When evaluating the benchmark section, the cheating items can be considered as ground These experimental results can illustrate several conclusions.Table 4. Benchmarks Results on Open-source LLMs(baselines) 70BNone None 0.8658 0.0 0.0 plan)None 0.8658 0.4615 0.6470 70BNone SFT 0.0 0.0 0.0 planSFT 0.0 0.9230 0.0 70BNone RAG 0.8658 0.0 0.0 planRAG 0.8658 0.4615 0.7059 70BNone None 0.7315 0.0 0.0 planNone 0.7315 0.4615 0.7059 120BNone None 0.7383 0.0 0.0 planNone 0.7383 0.7692 0.8235 3BNone None 0.047 0.0 0.0 planNone 0.047 0.2307 0.1176 First, we found that a pre-defined plan can help a naive LLM control the agent. However, this plan was specifically written based on the benchmark questions and cannot beEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework used in a production environment. Correspondingly, all experiments utilizing DoT technology and not cheating scored well on metrics B and C for evaluating the agent, although the scores were slightly lower than those achieved with preplanning. This indicates that our proposed knowl- edge processing approach based on DoT and self-play can be used to control cluster troubleshooting agents. Second, we found that SFT significantly improved the scores on metric B, which focuses on evaluating code writing or the invocation of diagnostic tools. However, as a trade-off, all models that underwent SFT, even with preplanning, were unable to control the agent properly, resulting in poor performance on metric C. Third, we found that the results based on LLama3.1-70B were not significantly different from those of Mistral-120B, which has nearly twice the number of parameters. Twice the number of parameters implies double or more inference costs (considering multi-GPU linearity), making it impractical. On the other hand, the 3B smaller model, even with preplanning in a cheating scenario, is still unable to handle the task of We proceeded with subsequent experiments and actual deployment using the LLM-agent enhanced with the whole dataset and all of our innovative methods. 5.3 Intelligent Early Warning and Troubleshooting: A To demonstrate the superiority of the LLM-agent system we have built in the context of intelligent cluster diagnostics, we can present a concrete example to illustrate how the system operates and how it is more efficient and accurate compared to traditional methods. In the production environment of AI clusters, abnormal events or interruptions are not the most challenging problems to resolve. Clear information about anomalies or interruptions can effectively guide senior engineers in diagnosing the causes of issues. Current research is also progressively integrating technologies such as automatic restarts and automatic scheduling into the procedures for handling anomalies or interruptions in AI computing tasks. However, once an AI computing task exhibits slow performance, it becomes difficult to quickly identify the problem, and it is even harder to pinpoint the cause of the slowdown. Assume there is an AI training cluster composed of dozens of servers, where one of the servers suddenly experiences a performance drop. This could be due to various reasons, such as increased network latency, memory leaks, high CPU load, or insufficient storage space. Traditionally, administrators or engineers would check the log files of the cluster to manually identify possible issues. This would involve reviewing logs from different nodes, monitoring system metrics, attempting to reproduce the problem, andso on. This method is time-consuming and labor-intensive and may require multiple attempts to pinpoint the root cause. In our system, the LLM-agent automatically gathers relevant log information, performance metrics, and other necessary data from the nodes of the cluster. Leveraging the LLM-agents capabilities assessed through the benchmark, the system extracts useful information from the collected data, such as cluster IP addresses, SSH ports, and other crit- ical diagnostic details. Using its diagnostic capabilities in code generation and information attribution, the LLM-agent identifies the root cause of the issue based on the collected data and information. This may include generating new test cases to validate hypotheses. Once the problem is identified, the LLM-agent generates corresponding remediation scripts and requests human review. After approval, the LLM-agent executes the remediation measures in the cluster. Following the execution of remediation measures, the system collects data again to assess the outcome, forming a closed loop of data, algorithm, and hardware to optimize future diagnostic We manually constructed a scenario. This scenario would lead to slow performance in AI model training tasks and has repeatedly occurred in the development environment. We simulated an extreme heat situation with HV AC failure, throttling the frequency of one of the dozens of GPUs to approximately 200 MHz, rather than the 1410 MHz that the A800 GPUs should operate at. Observing the actual logs shows that the speed of this AI computing task decreased to approximately one-third of its normal performance. Our LLM-system initially flagged the slow AI task through power consumption monitoring and performance modeling results, triggering an automatic alert. Following this, through three rounds of self-play, it recommended checking the GPU core frequencies, a suggestion that the agent then dispatched for execution across all GPUs. Based on the execution results, the LLM accurately pinpointed the GPU with the low core frequency that we had specifically altered. The entire troubleshooting process took less than 10 minutes. In contrast, a senior operations engineer would typically need about one hour to correctly identify the problem and then use a pre-written automated detection software tool created by engineers to determine the specific GPU with the low-frequency fault. More importantly, our LLM-agent can identify the fault before algorithm engineers or operations engineers detect the slow-down phenomenon and automatically complete the repair. This achieves resolving the issue before the fault occurs, thereby enhancing the overall availability of the cluster.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework 5.4 Qualitative Analysis of Correctness, Safety, and Based on the existing research that is not yet fully mature, and in the context of this specific field of study, we provide reasonable definitions for correctness, safety, and reliability. In this study, we define correctness as whether the process and results of the LLM-agent executing tasks are correct. Compared to evaluating the output of the LLM, assessing the correctness of the LLM-agents actions is more challenging. An apparently incorrect operation process may produce the correct result, whereas seemingly perfect output at the textual level might lead to an erroneous result when executed. Since we focus on the field of cluster diagnostics with the actual output being the execution of procedures by the agent, we do not investigate the potential harmfulness or bias in the textual content generated by the LLM. Instead, we examine the ability of our LLM-agent to avoid performing harmful operations on the cluster when the information fed back to the agent changes, or even when malicious content is inserted by an attacker, such as deleting files, shutting down, overclocking, or modifying critical system configurations. Regarding reliability, we define it as the overall quality of fault handling by the LLM-agent compared to human engineers or expert human engineers. In addition to whether the attribution is correct, we also consider factors such as the time taken to complete typical fault handling, the resources consumed, and the ability to We incorporate the assessment of correctness into the benchmark evaluation. For the potential risks associated with the LLM-agent, we implement a whitelist plus human review approach. Initially, we ensure the safety of the existing toolkit, followed by creating a whitelist for the program interfaces included in the toolkit and conducting human reviews for the LLM-agents requests to execute self-authored code. Finally, we observed that the LLM-agent can attribute faults with an average of fewer than three test cases across multiple rounds of self-play, which is more efficient than the twelve cases typically required by human experts. However, regarding communication abilities, the LLM-agent currently does not possess such capabilities. The qualitative analysis described above is mainly aimed at reducing the probability of harmful incidents. Quantitative analysis or a comprehensive model still necessitates further advancements in the field of 6 CONCLUSION AND DISCUSSION 6.1 Work Summary and Further Plan Based on our experience and research in the fields of cluster diagnostics, LLM enhancement, and LLM-agent construc-tion, we innovatively proposed a system solution utilizing LLM-agents to autonomously and intelligently perform clus- ter troubleshooting. In terms of LLM algorithms, we intro- duced a benchmark consisting of 150 advanced problems manually crafted, demonstrating the performance differ- ences between our constructed LLM-agent and the original open-source LLMs under fair data conditions. In the realm of LLM-agent construction, we innovatively proposed inte- grating DoT reasoning mathematics and the ability to handle special symbols and formulas into the agent, enabling the LLM to operate machines at the software level and receive feedback. Ultimately, we applied our innovative achieve- ments to cluster diagnostics, exploring the potential in this field, and were pleasantly surprised to find that the LLM- agent systems, despite being in their extremely early stages, are already capable of handling repetitive and low-end tasks, thus freeing industry practitioners to tackle more challeng- ing and valuable problems. In the future, we will continue our work in four aspects. In terms of LLM algorithms, we will expand and upgrade the existing benchmark and build a more comprehensive and valuable metrics system. In the Agent field, we will further unlock the potential of DoT and make self-written code by the LLM gradually become the main execution body, re- ducing reliance on preset tools. At the system application level, we will form a closed loop of data, algorithm, and hardware, enriching the database with results from actual deployments. Finally, in terms of safety and reliability, we will continue to work with researchers in related fields to ensure and evaluate the safety and reliability of the agents. 6.2 Shortcomings and Limitations Our research still has shortcomings and limitations. In terms of shortcomings, our agent currently relies on a mechanism of human review to ensure safety, depends on pre-written tools for code, and relies on data sourced from GitHub as a starting point. An ideal LLM-agent system should form a self-sustained relationship with the AI cluster, maintaining In terms of limitations, our work depends on the LLM within the LLM-agent, but smaller models like llama3.2-3B cur- rently cannot support the capabilities of the agent. There- fore, our work can only be applied to data centers or large- scale distributed clusters and cannot be deployed in edge computing or personal computer scenarios. We need to continuously monitor the development of smaller models and explore the possibility of teaching the capabilities of the LLM-agent to smaller models in the form of DoT when appropriate.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Co- hen, J., et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704 , 2024. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Pod- stawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. InProceedings of the AAAI Conference on Artificial In- telligence , volume 38, pp. 17682 17690, 2024. Castelfranchi, C. Modelling social action for ai agents. Artificial intelligence , 103(1-2):157 182, 1998. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint Guo, C., Wu, H., Deng, Z., Soni, G., Ye, J., Padhye, J., and Lipshteyn, M. Rdma over commodity ethernet at scale. InProceedings of the 2016 ACM SIGCOMM Conference , Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring mas- sive multitask language understanding. arXiv preprint Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint Jiang, Z., Lin, H., Zhong, Y ., Huang, Q., Chen, Y ., Zhang, Z., Peng, Y ., Li, X., Xie, C., Nong, S., et al. {MegaScale }: Scaling large language model training to more than 10,000 {GPUs }. In 21st USENIX Symposium on Net- worked Systems Design and Implementation (NSDI 24) , pp. 745 760, 2024.Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual inter- national symposium on computer architecture , pp. 1 12, Jung, H. and Chung, K. Social mining-based clustering process for big-data integration. Journal of Ambient In- telligence and Humanized Computing , 12(1):589 600, Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K uttler, H., Lewis, M., Yih, W.-t., Rockt aschel, T., et al. Retrieval-augmented generation for knowledge- intensive nlp tasks. Advances in Neural Information Pro- cessing Systems , 33:9459 9474, 2020. Liu, Y ., Tao, S., Zhao, X., Zhu, M., Ma, W., Zhu, J., Su, C., Hou, Y ., Zhang, M., Zhang, M., et al. Coachlm: Automatic instruction revisions improve the data quality in llm instruction tuning. In 2024 IEEE 40th International Conference on Data Engineering (ICDE) , pp. 5184 5197. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint Ofenbeck, G., Steinmann, R., Caparros, V ., Spampinato, D. G., and P uschel, M. Applying the roofline model. In2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS) , pp. 76 85. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems , 35:27730 27744, 2022. Park, J. S., OBrien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology , pp. 1 22, 2023. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems , 32, 2019. Peter, E. Abstraction and reasoning challenge. 2022.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Qian, K., Xi, Y ., Cao, J., Gao, J., Xu, Y ., Guan, Y ., Fu, B., Shi, X., Zhu, F., Miao, R., et al. Alibaba hpn: a data center network for large language model training. In Proceedings of the ACM SIGCOMM 2024 Conference , Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. Shanley, T. InfiniBand network architecture . Addison- Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multi- billion parameter language models using model paral- lelism. arXiv preprint arXiv:1909.08053 , 2019. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test- time compute optimally can be more effective than scal- ing model parameters. arXiv preprint arXiv:2408.03314 , Team, G., Anil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971 , 2023. Wang, Q., Sang, B., Zhang, H., Tang, M., and Zhang, K. Dlrover: An elastic deep training extension with auto job resource recommendation. arXiv preprint Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824 24837, Xi, Z., Chen, W., Guo, X., He, W., Ding, Y ., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864 , 2023. Xiong, Y ., Jiang, Y ., Yang, Z., Qu, L., Zhao, G., Liu, S., Zhong, D., Pinzur, B., Zhang, J., Wang, Y ., et al. {SuperBench }: Improving cloud {AI}infrastructure reli- ability with proactive validation. In 2024 USENIX Annual Technical Conference (USENIX ATC 24) , pp. 835 850, Xu, Y ., Chen, Y ., Zhang, X., Lin, X., Hu, P., Ma, Y ., Lu, S., Du, W., Mao, Z., Zhai, E., et al. Cloudeval-yaml: Apractical benchmark for cloud configuration generation. Proceedings of Machine Learning and Systems , 6:173 Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y ., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems , 36, 2024. Zhang, Y ., Yuan, Y ., and Yao, A. C.-C. On the diagram of thought. arXiv preprint arXiv:2409.10038 , 2024. APLEASE ADD SUPPLEMENTAL MATERIAL Put anything that you might normally include after the refer- ences as an appendix here, not in a separate supplementary file. Upload your final camera-ready as a single pdf, includ-'},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'text': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification Yohei Seki1, Hakusen Shu2, Ana s Lhuissier3, Hanwool Lee4, Juyeon Kang3,Min-Yuh Day5,Chung-Chi Chen6 1Institute of Library, Information, and Media Science, University of Tsukuba, Japan, 2College of Knowledge and Library Sciences, School of Informatics, University of Tsukuba, Japan, 33DS Outscale, France,4Shinhan Securities Co., Korea, 5Graduate Institute of Information Management, National Taipei University, Taiwan6AIST, Japan Promises made by politicians, corporate lead- ers, and public figures have a significant impact on public perception, trust, and institutional rep- utation. However, the complexity and volume of such commitments, coupled with difficul- ties in verifying their fulfillment, necessitate innovative methods for assessing their credi- bility. This paper introduces the concept of Promise Verification, a systematic approach in- volving steps such as promise identification, evidence assessment, and the evaluation of tim- ing for verification. We propose the first mul- tilingual dataset, ML-Promise, which includes English, French, Chinese, Japanese, and Ko- rean, aimed at facilitating in-depth verification of promises, particularly in the context of En- vironmental, Social, and Governance (ESG) reports. Given the growing emphasis on corpo- rate environmental contributions, this dataset addresses the challenge of evaluating corpo- rate promises, especially in light of practices like greenwashing. Our findings also explore textual and image-based baselines, with promis- ing results from retrieval-augmented generation (RAG) approaches. This work aims to foster further discourse on the accountability of pub- lic commitments across multiple languages and In a world where promises shape perceptions and drive decisions, the integrity of commitments made by politicians, corporate leaders, and public fig- ures must be scrutinized. These promises, ranging from environmental sustainability to social respon- sibility and governance ethics, significantly influ- ence the general publics and stakeholders trust, as well as government and corporate reputations. Yet, the complexity and abundance of such commit- ments, coupled with the challenge of verifying their fulfillment, create a pressing need for innovative approaches to assess their strength and verifiabil- ity. Recognizing the critical role of transparencyand accountability in todays society, we propose a groundbreaking task: Promise Verification. To perform promise verification, several steps are required, including (1) identifying the promise, (2) linking the promise with supporting evidence, (3) assessing the clarity of the promise-evidence pair, and (4) inferring the timing for verifying the promise. For example, after identifying a promise, the availability of evidence to support that the speaker is taking action to fulfill the promise could serve as a coarse-grained evaluation of the promises quality. However, the clarity of the ev- idence may also affect the quality of the promise, which would be a fine-grained evaluation. Addi- tionally, whether the speaker provides a clear time- line for verifying the promise is an important crite- rion. For instance, we will achieve net zero carbon emissions within five years is a stronger promise than we will achieve net zero carbon emissions. Following this line of thought, this paper proposes the first multilingual dataset for in-depth promise verification, including Chinese, English, French, In recent years, increasing emphasis has been placed on companies environmental contributions, especially in addressing climate change, deforesta- tion, and compliance with labor conditions and gov- ernance, when evaluating their investment value. In the evolving landscape of ESG (environmental, social, and governance) criteria, the ability to accu- rately assess a companys promises and adherence to its ESG promises has become paramount. How- ever, unlike traditional financial statements, ESG reports still lack clear standards regarding corporate promises. This allows some companies to use mis- leading information to project an overly positive environmental image, a practice known as green- washing. As Gorovaia and Makrominas (2024) points out, companies involved in environmental misconduct tend to produce longer, more positive, and more frequent reports. We hypothesize that 7 Nov 2024Task Label English French Chinese Japanese Korean Promise IdentificationYes 84.5 80.5 40.2 74.9 77.5 No 15.5 19.5 59.8 25.1 22.5 Supporting EvidenceYes 20.1 71.6 20.1 66.4 75.6 No 79.9 28.4 79.9 33.6 24.4 Clarity of Promise-Evidence PairClear 53.3 56.6 64.7 61.2 94.8 Not Clear 42.9 41.9 35.3 34.7 5.2 Misleading 3.8 1.5 0.0 4.1 0.0 Timing for VerificationWithin 2 years 1.9 12.4 37.5 7.3 45.5 2-5 years 14.1 15.0 10.0 9.3 8.4 Longer than 5 years 9.0 21.6 15.0 18.7 17.5 Other 75.0 51.0 37.5 64.7 28.7 Table 1: Label distribution in each language. (%) such reports may lack substantive evidence, or the information presented may be irrelevant or ambigu- ous, leading to misinterpretation. To this end, the proposed dataset, ML-Promise, focuses on ESG reports released by corporations in five countries: the U.K., France, Taiwan, Japan, and Korea. In addition to exploring text-based baselines, we also provide pilot results on image-based experi- ments, as most reports are released in PDF format. Our experiment further shows that the retrieval- augmented generation (RAG) approach (Lewis et al., 2020) can help in some language cases. Al- though we do not find a silver bullet for all lan- guages and tasks, we hope the proposed dataset can open a new chapter in discussions on the respon- sibility of promises, especially those from public Recent studies have sought to improve the anal- ysis of ESG or sustainability reports for estimat- ing company values using contextual embedding approaches. For example, Gutierrez-Bustamante and Espinosa-Leal (2022) evaluated sustainability reports from publicly listed companies in Nordic countries using latent semantic analysis (LSA) and the global vectors for word representation (GloVe) model, enhancing document retrieval performance based on similarity. Garigliotti (2024) explored the integration of sustainable development goals (SDGs) into environmental impact assessments (EIAs) using a RAG framework powered by large language models (LLMs). Their work focused on two tasks: detecting SDG targets within EIA reports and identifying relevant textual evidence, specifically in European contexts. Hillebrand et al. (2023) introduced sustain.AI, a context-aware rec- ommender system designed to analyze sustainabil- ity reports in response to increasing corporate so-cial responsibility (CSR) regulations. The system, based on a BERT architecture, identified relevant sections of lengthy reports using global reporting initiative (GRI) indicators and demonstrated strong performance on datasets from German companies. Previous studies have a few shortcomings. First, most of them focus solely on reports from one country. Second, none of them attempt to analyze corporate promises, despite the abundance of sus- tainability reports. To address these problems, our study extends these works by focusing on multi- lingual companies from both European and Asian regions, including Taiwan, the UK, France, Japan, and Korea. With the proposed new task, we aim to highlight the importance of anti-greenwashing by evaluating corporate promises in ESG reports. We collect ESG reports from five countries: the UK, France, Taiwan, Japan, and Korea. The annotators are native speakers of the target language or are familiar with the language at the work level. The task designs are as follows when given a paragraph in the ESG reports. 1.Promise Identification : This is a boolean label (Yes/No) based on whether a promise exists. 2.Supporting Evidence : This is a boolean label (Yes/No) based on whether supporting evidence exists. 3.Clarity of the Promise-Evidence Pair : We designed three labels (Clear/Not Clear/Misleading) for this task, which should depend on the clarity of the given evidence in relation to the promise. 4.Timing for Verification : Following the MSCI guide- lines and previous work (Tseng et al., 2023), we set timing labels (within 2 years/2-5 years/longer than 5 years/other) to indicate when readers/investors should return to verify the promise. Here, other denotes the promise has already been verified or doesnt have a specific timing to verify it.Approach Task English French Chinese Japanese Korean w/o RAGPromise Identification (PI) 0.842 0.816 0.521 0.670 0.849 Supporting Evidence (SE) 0.680 0.746 0.163 0.720 0.792 Clarity of Promise-Evidence Pair (CPEP) 0.411 0.443 0.569 0.450 0.897 Timing for Verification (TV) 0.636 0.523 0.317 0.632 0.406 w/ RAGPromise Identification 0.866 0.798 0.540 0.659 0.807 Supporting Evidence 0.757 0.732 0.503 0.850 0.774 Clarity of Promise-Evidence Pair 0.467 0.487 0.628 0.465 0.939 Timing for Verification 0.693 0.601 0.469 0.684 0.571 Table 2: Experimental Results (F1-Score). The best performance in each language is denoted in bold . Finally, we obtained 3,010 instances, i.e., 600 for each language and 10 additional instances in the Chinese dataset. The Cohens agreement (Co- hen, 1960; McHugh, 2012) for these tasks is ap- proximately 0.65-0.96, 0.71-0.88, 0.62-0.80, and 0.60-0.89, respectively. Table 1 presents the distri- bution of the proposed ML-Promise dataset. First, we observe that around 35-40% of the evidence is not clear in supporting the associated promises in four out of five languages. This highlights the necessity of the proposed task for evaluating the quality of the promise-evidence pairs from corpora- tions. Furthermore, about 4% of instances contain (potentially) misleading evidence in the English and Japanese datasets. It is crucial for corporations to re-examine this evidence, and it is also essen- tial for supervisory authorities to monitor these instances. Second, we noted that corporations in Taiwan and Korea tend to make more short-term promises (within 2 years), whereas corporations in the remaining countries tend to make longer-term promises. This finding shows the need for a multi- lingual comparison of ESG reports across different countries, as the narrative styles vary among them. RAG (Lewis et al., 2020) was introduced as a method to enhance LLMs by integrating external knowledge sources. This approach combines re- trieval mechanisms with generative models, pro- ducing more accurate and contextually relevant outputs. Yu et al. (2024) highlights the advantages of RAG systems, particularly their ability to extract domain-specific information. By incorporating ex- ternal retrieval processes, RAG enables generative models to access a broader, field-specific knowl- edge base, improving the accuracy and relevance of responses. This capability is especially impor- tant for handling domain-specific queries, an areaRAG TaskChinese Korean Image-Based Text-Based Image-Based Text-Based w/oPI 0.530 0.521 0.837 0.849 SE 0.124 0.163 0.812 0.792 CPEP 0.510 0.569 0.922 0.897 TV 0.202 0.317 0.201 0.406 w/PI 0.580 0.540 0.843 0.807 SE 0.512 0.503 0.845 0.774 CPEP 0.618 0.628 0.893 0.939 TV 0.297 0.469 0.330 0.571 Table 3: Image-based experimental results. Bolded denotes the best performance in each language. Underlined denotes performance with RAG better than where existing LLMs often encounter difficulties. Fan et al. (2024) discusses training strategies for RAG, including independent, sequential, and joint methods, which can be tailored to optimize retrieval and generation for specific domains. For Chinese language applications, Wang et al. (2024b) empha- sizes the importance of domain-specific corpora over general knowledge sources. Ardic et al. (2024) applied RAG to analyze sustainability reports from ten Turkish companies, focusing on ESG factors. The study found BM25 outperformed BERTurk in retrieving relevant sections, highlighting the effec- tiveness of advanced retrieval techniques. Following the findings of previous studies, we also explore and design the RAG approach for the proposed tasks. Specifically, when given a para- graph, we first retrieve the six most similar samples in the training set. We leveraged Multilingual E5 Text Embeddings (Wang et al., 2024a) to calcu- late the cosine similarity between target paragraphs and instances from the training set. Then, we pro- vide the top-six examples for the LLM to perform in-context learning (Dong et al., 2022). In our ex- periment, we use GPT-4o as the base LLM. In the experiment, we randomly select 200 in- stances from each language as the test set, and the remaining instances are used for training. We use the F1 score to evaluate the performance of eachtask. Table 2 shows the performance of each task in each language. First, the performance of most tasks improves when adopting RAG. Specifically, for En- glish and Chinese, all tasks perform better when using RAG. Second, RAG enhances performance in estimating the clarity of the promise-evidence pair and inferring the timing for verification, regard- less of the language used. These results suggest the usefulness of RAG in these two novel tasks. Addi- tionally, the findings demonstrate the value of the proposed annotations. With the proposed dataset, the performance of fine-grained promise evaluation can be improved. Third, although the performance in promise identification and supporting evidence identification tasks may slightly decrease in French, Japanese, and Korean, the declines are minimal (less than 2% in most cases). These results suggest that the method for retrieving and suggesting sam- ples similar to the paragraph requires refinement for imbalanced boolean datasets. In future work, we will focus on improving the RAG approach by extracting balanced samples, particularly for minor We noticed a significant difference between Tai- wan/Korea reports and the reports from other coun- tries.1The reports from these two countries uti- lize a large number of graphs instead of textual descriptions. This observation raises the question of whether we could use multimodal LLMs to read PDF files directly instead of relying on extracted text. To explore this, we align the annotations with a PDF page and employ GPT-4o to reassess the tasks using an image as input. For RAG, we lever- aged E5-V Universal Embeddings (Jiang et al., 2024) to calculate the cosine similarity between target pages and instances from the training set. Table 3 presents the performance. First, using GPT-4o with image input reduces performance in three out of four tasks in the Chinese dataset and in two out of four tasks in the Korean dataset. Sec- ond, RAG improves the performance of most tasks when using image input. Third, with RAG, the per- formance in promise identification and supporting evidence identification tasks improves with Chi- nese image input, and the performance of support- ing evidence identification improves with Korean image input. However, for estimating the clarity of 1We provide some examples in Appendix A.Input RAG Task ROUGE-L Table 4: Results of promise and evidence extraction. the promise-evidence pair and inferring the timing for verification, using text input with RAG remains superior. In summary, our experimental results sug- gest that image input should be used for PI and SE tasks, while text input is preferable for CPEP and TV tasks. Additionally, RAG performs well regardless of input type. 5.2 Promise and Evidence Extraction In the previous section, we explored the promise and evidence identification tasks. However, the task can also be formulated in an extractive man- ner. Instead of only outputting a yes or no, we can also ask models to extract the promise and evidence from the report. We provide additional annotations in the Chinese dataset and experiment in multimodal settings with and without RAG. The ROUGE-L (Lin, 2004) score is used to evaluate ex- traction performance. Table 4 presents the results. These results indicate that the best performance is achieved in the image-based setting with RAG for both promise and evidence extraction. This empha- sizes the importance of exploring multimodal input for ESG report understanding. In this paper, we introduce the concept of Promise Verification, a novel task aimed at evaluating the credibility and fulfillment of promises made by corporations, particularly in the context of ESG reports. We propose the first multilingual dataset, ML-Promise, to emphasize the importance of assessing corporate environmental and social promises. Our results demonstrate that RAG im- proves performance, while also showing the po- tential of multimodal approaches in promise ver- ification. Our annotations will be released under the CCBY-NC-SA 4.0 license. We hope this work serves as a foundation for the robustness of promise verification systems and contributes to greater ac- countability in corporate and public disclosures.Limitation Several limitations warrant discussion. First, al- though the ML-Promise dataset includes five lan- guages?Chinese, English, French, Japanese, and Korean?its scope is still limited to a few countries and may not fully capture the diversity of corpo- rate promise communication styles globally. The dataset focuses on ESG reports from specific re- gions, which may limit the generalizability of the findings to other languages and cultural contexts. Future studies can follow our design to expand the dataset to include more regions and languages, which could enhance the robustness and applicabil- ity of the proposed methods. Second, although the study uses RAG to improve performance, the re- sults show that this approach does not consistently outperform baseline models across all languages and tasks. These inconsistencies suggest that RAG may require further optimization or task-specific adjustments, particularly in handling the nuances of each language and dataset structure. These limitations and our findings highlight ar- eas for future research, including expanding the dataset, refining the RAG approach, enhancing mul- timodal learning, and addressing the inherent am- biguities in corporate ESG reporting. Ozgur Ardic, Mahiye Uluyagmur Ozturk, Irem Demir- tas, and Secil Arslan. 2024. Information Extraction from Sustainability Reports in Turkish through RAG Approach. In 2024 32nd Signal Processing and Com- munications Applications Conference (SIU) , pages Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhi- fang Sui. 2022. A Survey on In-context Learning. arXiv preprint arXiv:2301.00234 . Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs: To- wards Retrieval-Augmented Large Language Models. Dario Garigliotti. 2024. SDG target detection in envi- ronmental reports using Retrieval-augmented Gener- ation with LLMs. In Proceedings of the 1st Work- shop on Natural Language Processing Meets Cli- mate Change (ClimateNLP 2024) , pages 241 250, Bangkok, Thailand. Association for Computational Linguistics.Nina Gorovaia and Michalis Makrominas. 2024. Identi- fying greenwashing in corporate-social responsibility reports using natural-language processing. European Marcelo Gutierrez-Bustamante and Leonardo Espinosa- Leal. 2022. Natural Language Processing Meth- ods for Scoring Sustainability Reports? A Study of Nordic Listed Companies. Sustainability , 14(15). Lars Hillebrand, Maren Pielka, David Leonhard, To- bias Deu er, Tim Dilmaghani, Bernd Kliem, R diger Loitz, Milad Morad, Christian Temath, Thiago Bell, Robin Stenzel, and Rafet Sifa. 2023. sustain.AI: a Recommender System to analyze Sustainability Re- ports. In Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law , ICAIL 23, page 412?416, New York, NY , USA. Association Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. 2024. E5-V: Universal Embeddings with Multimodal Large Language Mod- Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich K ttler, Mike Lewis, Wen-tau Yih, Tim Rock- t schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge- intensive NLP tasks. In Proceedings of the 34th International Conference on Neural Information Pro- cessing Systems , Red Hook, NY , USA. Curran Asso- Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out , pages 74 81, Barcelona, Spain. Association for Computational Linguistics. Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica , 22(3):276 282. Yu-Min Tseng, Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen. 2023. DynamicESG: A Dataset for Dynamically Unearthing ESG Ratings from News Articles. In Proceedings of the 32nd ACM Interna- tional Conference on Information and Knowledge Management , pages 5412 5416. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024a. Multi- lingual E5 Text Embeddings: A Technical Report. arXiv preprint arXiv:2402.05672 . Shuting Wang, Jiongnan Liu, Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yu- tao Zhu, and Zhicheng Dou. 2024b. Domain- RAG: A Chinese Benchmark for Evaluating Domain- specific Retrieval-Augmented Generation. Preprint , Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of Retrieval- Augmented Generation: A Survey. Preprint , We provide five ESG report exam- ples in this section, and please refer to our training set for more instances:'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_rag_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OPENAI API KEY\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OPENAI API KEY\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Chunking and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into chunks for embeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = []\n",
    "\n",
    "for paper in top_5_rag_papers:\n",
    "    chunks = text_splitter.split_text(paper[\"text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append({\n",
    "            \"paper_id\": paper[\"paper_id\"],\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"content\": chunk,\n",
    "            \"chunk_id\": i\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1: 47 chunks\n",
      "Paper 66: 78 chunks\n",
      "Paper 77: 17 chunks\n",
      "Paper 80: 61 chunks\n",
      "Paper 87: 25 chunks\n"
     ]
    }
   ],
   "source": [
    "# Counts the number of chunks for each paper\n",
    "\n",
    "chunk_counts = Counter(doc[\"paper_id\"] for doc in documents)\n",
    "\n",
    "# Print results\n",
    "for paper_id, count in chunk_counts.items():\n",
    "    print(f\"Paper {paper_id}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation Robin D. Pesl1, Jerin G. Mathew2, Massimo Mecella2, and Marco Aiello1 1University of Stuttgart, Stuttgart, Germany 2Sapienza Universit di Roma, Rome, Italy Abstract Integrating multiple (sub-)systems is essential to create ad- vanced Information Systems (ISs). Difficulties mainly arise when integrat- ing dynamic environments across the IS lifecycle, e.g., services not yet existent at design time. A traditional approach is a registry that provides the API documentation of the systems endpoints. Large Language Mod- els (LLMs) have shown to be capable of automatically creating system integrations (e.g., as service composition) based on this documentation but require concise input due to input token limitations, especially regard- ing comprehensive API descriptions. Currently, it is unknown how best to preprocess these API descriptions. Within this work, we (i) analyze the usage of Retrieval',\n",
       "  'chunk_id': 0},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'best to preprocess these API descriptions. Within this work, we (i) analyze the usage of Retrieval Augmented Generation (RAG) for endpoint discovery and the chunking, i.e., preprocessing, of state-of-practice OpenAPIs to reduce the input token length while preserving the most relevant informa- tion. To further reduce the input token length for the composition prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that only receives a summary of the most relevant endpoints and retrieves specification details on demand. We evaluate RAG for endpoint discovery using the RestBench benchmark, first, for the different chunking possibil- ities and parameters measuring the endpoint retrieval recall, precision, and F1 score. Then, we assess the Discovery Agent using the same test set. With our prototype, we demonstrate how to successfully employ RAG for endpoint discovery to reduce the token count. While revealing high values for recall, precision, and F1, further research is',\n",
       "  'chunk_id': 1},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'the token count. While revealing high values for recall, precision, and F1, further research is necessary to retrieve all requisite endpoints. Our experiments show that for preprocessing, LLM-based and format-specific approaches outperform na ve chunking methods. Relying on an agent further enhances these results as the agent splits the tasks into multiple fine granular subtasks, improving the overall RAG performance in the token count, precision, and F1 score. Keywords: Retrieval augmented generation Large language models OpenAPI Endpoint discovery RestBench. OpenAPI is the state-of-practice for describing interfaces for integrating Informa- tion Systems (ISs). It contains formal elements like paths and natural language 29 Nov 20242 Pesl et al. constituents like descriptions. For integrating these systems automatically, au- tomated service composition using Large Language Models (LLMs) has been proposed [ 27,28,29]. These approaches exploit the capabilities of LLMs to process formal',\n",
       "  'chunk_id': 2},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'has been proposed [ 27,28,29]. These approaches exploit the capabilities of LLMs to process formal and natural language input, combining them with the inherent nature of automated service composition of decoupling and independent lifecycle manage- ment. While prohibiting any manual modeling effort by relying on already broadly available OpenAPIs, the approaches face the challenge of limited input token length [29]. This bounds the quantity and extent of the input service description. Even for proprietary models with a large input token context, e.g., OpenAIs GPT4 with a context size of 128,000 tokens [ 23], an economic constraint emerges as these models are paid by input and output token count. Therefore, a smaller prompt length is beneficial to (i) insert further service documentation and (ii) reduce costs for proprietary models. To address these challenges, Retrieval Augmented Generation (RAG) [ 16] has emerged as a promising resort. In this approach, the external information is',\n",
       "  'chunk_id': 3},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': '(RAG) [ 16] has emerged as a promising resort. In this approach, the external information is collected in a database, typically structured as a set of documents or document chunks. The primary goal is retrieving only a small subset of the most relevant documents or document chunks, which is then inserted into the prompt [ 16]. How to optimally apply RAG for endpoint discovery in IS is open to investigation, leading to the following research questions: RQ1.How best to preprocess, i.e., chunk, OpenAPIs for RAG endpoint discovery? RQ2.Can LLM agents be employed to reduce token count further and improve To answer RQ1, we develop an OpenAPI RAG system that takes as input service descriptions. We apply different token-based and LLM-based chunking strategies to split the documentation and evaluate them based on retrieval quality. The token-based strategies process the document using a classical parser and then split the parts, e.g., endpoints, into equal-sized chunks. The LLM-based',\n",
       "  'chunk_id': 4},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'classical parser and then split the parts, e.g., endpoints, into equal-sized chunks. The LLM-based strategies let an LLM create a description, i.e., a summary or a question, for each endpoint and then use these descriptions for similarity matching. We employ mainstream open-source and proprietary embedding models for similarity matching, which can create an embedding vector for an input. The similarity between two inputs can then be determined by comparing their embedding vectors using, e.g., the cosine similarity. We evaluate the OpenAPI RAG and the different chunking strategies by relying on the already available RestBench benchmark [ 35] for LLMs agents, measuring recall, precision, and F1 score for each chunking strategy. The benchmark consists of the OpenAPI descriptions of Spotify and TMDB and corresponding queries, each with a set of endpoints as the sample solution. To address RQ2, we propose an LLM agent called Discovery Agent . As LLM agents allow the usage of external',\n",
       "  'chunk_id': 5},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'RQ2, we propose an LLM agent called Discovery Agent . As LLM agents allow the usage of external tools, we first investigate using one tool that simply inputs the results of the RAG to the prompt. Then, we experiment with using two tools: the first tool filters and enters the LLM endpoint summaries to the prompt using RAG, while the second tool allows the retrieval of the endpoint details on demand. We resort to the same RestBench benchmark for evaluation and measure recall, precision, F1 score, and additional token count.Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 3 As the chunking strategy, we rely on the LLM-based summary strategy with OpenAIs text-embedding-3-large embedding model [24]. The remainder of the paper is structured as follows. First, we provide an overview of related works regarding service discovery and LLMs in Section 2. Then, we present how to use RAG for endpoint discovery and the OpenAPI chunking strategies in Section 3. We evaluate and discuss',\n",
       "  'chunk_id': 6},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'for endpoint discovery and the OpenAPI chunking strategies in Section 3. We evaluate and discuss the RAG and the different chunking strategies in Section 4. Finally, we conclude with Section 5. Regarding endpointdiscovery,we provide abrief overview ofthe essential concepts of the various service discovery approaches. Additionally, we provide relevant insights into LLMs and the novel approach of integrating LLMs with tools, known as LLM agents, and how they relate to our approach. The most common service discovery implementation is a service registry, which collects information about available services and offers search facilities. This service registry is usually backed by a component residing at the middleware or application level [ 15]. It is characterized by the syntax used to describe the services and their invocation and the expressive power of the available query language. The typical integration model is a pull model where service consumers search for the required services.',\n",
       "  'chunk_id': 7},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'typical integration model is a pull model where service consumers search for the required services. Less is a push model as used in the UPnP protocol, where service providers regularly advertise their services [32]. In the early days of XML-based services, the infrastructure for service discov- ery was the Universal Description, Discovery, and Integration (UDDI) specifi- cation [7]. UDDI had a global incarnation called the UDDI Business Registry (UBR), intended to offer an Internet-wide repository of available web services and promoted by IBM, Microsoft, and SAP. Unfortunately, UBR never gained widespread adoption and was short-lived (2000-2006). Significant research in the early days focused on enhancing service discovery on UDDI, improving search capabilities, and creating federated registries, e.g., [ 3,4,11]. Alternatively, WS-Discovery is a multicast protocol that finds web services on a local network. Nowadays, OpenAPI is the de facto standard for describing services. While not',\n",
       "  'chunk_id': 8},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'on a local network. Nowadays, OpenAPI is the de facto standard for describing services. While not offering a discovery protocol and mechanism, given its popularity, OpenAPI would also benefit from discovery [ 34]. So, additional infrastructure for discovery has been proposed, such as centralized repositories (SwaggerHub or Apiary), service registry integration (Consul, Eureka), API Gateways (Kong, Apigee), or Populating registries of services requires effort from service providers, which often hinders the success of such approaches, especially if the service provider is expected to provide extensive additional information beyond the service endpoints. This additional effort has often been the reason for the failure of some of these technologies, most notably UBR. Approaches confined to specific applications,4 Pesl et al. domains, or enterprises have been more successful, e.g., Eureka. Developed by Netflix as part of its microservices architecture [ 36], Eureka helps clients find',\n",
       "  'chunk_id': 9},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'Developed by Netflix as part of its microservices architecture [ 36], Eureka helps clients find service instances described by host IP, port, health indicator URL, and home page. Developers can add additional data to the registry for extra use cases. extensive modeling, e.g., as semantic annotations. Hence, our approach relies on already broadly available state-of-practice OpenAPI specification and their 2.2 Large Language Models (NLP) and machine learning field [ 1,2,13]. Often containing billions of parameters, these models are trained on extensive text corpora to generate and manipulate human-like text [ 30]. They are primarily based on an encoder-decoder architecture tasks using decoder-only models such as GPT [ 31]. Usually, the input is a natural language task called prompt, which first needs to be translated to a sequence of input tokens. The model processes this prompt and returns an output token sequence, which can then be translated back to a natural language answer. As',\n",
       "  'chunk_id': 10},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'an output token sequence, which can then be translated back to a natural language answer. As these models can, in general, capture intricate linguistic nuances and semantic contexts, they can be applied to a wide range of tasks, e.g., in software engineering [ 10]. LLMs can be used to create integration based on endpoint documentation automatically [ 27,28,29]. Yet, these face strict input token limitations, e.g., 128,000 tokens for current OpenAI models [23,29]. With this paper, we analyze how RAG can be used to preprocess API documentation to mitigate this issue. Another approach is encoder-only models such as BERT [ 8], often referred to as embedding models. They allow condensing the contextual meaning of a text into a dense vector, termed embedding. Using similarity metrics such as dot product, cosine similarity, or Euclidean distance allows for assessing the similarity of two input texts. Embedding models are usually used for the similarity search in RAG systems [6], which we',\n",
       "  'chunk_id': 11},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'texts. Embedding models are usually used for the similarity search in RAG systems [6], which we also do in our implementation. LLMs have shown remarkable capabilities in solving complex tasks by decompos- ing them in a step-by-step fashion [ 38] or by exploring multiple solution paths simultaneously [ 41]. Typically, these plans are generated iteratively by using the history of the previously generated steps to guide the generation of the next step. Additionally, recent studies have shown the potential of providing LLMs access to external tools to boost their reasoning capabilities and add further knowledge. This approach consists of prompting the LLM to interact with external tools to solve tasks, thus offloading computations from the LLM to specialized functions. Notable examples of such tools include web browsers [ 20], calculators [ 5], andAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 5 Python interpreters [ 12]. In practice, a tool is usually a Python function,',\n",
       "  'chunk_id': 12},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'Generation 5 Python interpreters [ 12]. In practice, a tool is usually a Python function, which can be called during the interaction with the LLM. The LLM agent paradigm [ 19,22,40] combines the concepts of (i) external tool usage, (ii) the planning capabilities of LLMs, and adds a shared (iii) memory, to solve complex tasks. Given an input task, an LLM agent uses its reasoning capabilities to decompose the task into a set of simpler subtasks. For each subtask, the LLM finds and interacts with the set of tools to solve the subtask. Then, based on the outcome of the current task and the history of previously executed subtasks, the LLM agent generates a new subtask and repeats the steps mentioned above or terminates if the original task is solved. To instruct the processing, the outcome of the tool invocations and the history of the subtasks are stored in the memory, typically consisting in the LLM agents own context. Within this work, we apply the LLM agent paradigm to create the',\n",
       "  'chunk_id': 13},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'in the LLM agents own context. Within this work, we apply the LLM agent paradigm to create the Discovery Agent as an LLM agent for endpoint discovery. A critical challenge for LLM agents is the accessibility to a set of common APIs and tasks for their evaluation, e.g., tested using benchmarks like API Bank [17] or RestBench [ 35]. API Bank is a benchmark to evaluate the tool use of an LLM consisting of a set of APIs exposed through a search engine. Unfortunately, the available code of the benchmark is incomplete. The RestBench benchmark contains a collection of tasks and endpoints expressed using the OpenAPI specification of Spotify and TMDB [ 35]. As the currently most extensive available benchmark, we employ RestBench to validate our results. OpenAPIs within LLM agents have been used in RestGPT [ 35] and Chain of Tools [ 33]. The former combines multiple LLM agents to solve complex tasks by interacting with a set of tools exposed using the OpenAPI specification. The latter solves an',\n",
       "  'chunk_id': 14},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'by interacting with a set of tools exposed using the OpenAPI specification. The latter solves an input query by framing the problem as a code generation task and interacts with the set of tools to generate Python code to solve the query. In contrast, our Discovery Agent does not directly interact with the endpoints stated in the OpenAPIs. Instead, it filters and returns matching endpoints that can be used for subsequent processing. Even when considering the similarity to the tool selection within LLM agents, the task of selecting a set of tools from a larger pool to solve a specific problem remains relatively underexplored [ 42]. Existing research primarily focuses on the a priori selection of human-curated tools [ 25], heuristic-based methods for tool selection [ 18], choosing the relevant tool by scoring each query against every tool based semantic retrieval using a combination of different vector databases [ 42]. With our work, we contribute the analysis of preprocessing OpenAPIs',\n",
       "  'chunk_id': 15},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'vector databases [ 42]. With our work, we contribute the analysis of preprocessing OpenAPIs into this We first introduce the general architecture to employ RAG for endpoint discovery. As state-of-practice for service documentation, we then investigate how to chunk OpenAPIs as preprocessing for RAG.6 Pesl et al. 3.1 RAG for Endpoint Discovery RAG comprises a preprocessing step ahead of the answer generation of an LLM to a semantic search based on some knowledge sources. Usually, the semantic search is done by embedding similarity, and the data from the knowledge sources is reduced to small chunks to allow fine-grained information retrieval [16]. Figure1. RAG for Endpoint Discovery Figure 1 depicts the application of RAG for endpoint discovery, i.e., the OpenAPI RAG. Initially, the chunking strategy determines how the chunks are created from the OpenAPIs, i.e., how many chunks are created and what they contain. Each chunk has an embedding as metadata for similarity search in addition to',\n",
       "  'chunk_id': 16},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'and what they contain. Each chunk has an embedding as metadata for similarity search in addition to its content. The chunking strategy specifies which data is used as input to the embedding model to create the embedding. This input does not have to match the chunk content, e.g., it can be a summary instead of the entire content. The chunks are finally stored in the chunk database. For retrieval, the user submits in 1a natural language query qto the chunk retriever, which converts qinto the embedding eusing the same embedding model as for the chunk creation. In 2, the chunk retriever queries the chunk database using e. The chunk database compares eusing a similarity metric with the embeddings of the service chunks contained in the database. The results are the top kmost similar chunks according to the metric, which are then returned to the chunk retriever in 3. Finally, in 4, the chunk retriever forwards the retrieved results to the user, who can add them to their prompt either',\n",
       "  'chunk_id': 17},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'chunk retriever forwards the retrieved results to the user, who can add them to their prompt either manually or automatically through integration into their tooling. The benefit of employing RAG is the insertion of only the gist of the available information, which allows picking more and only the most relevant information for the fix LLM context size. A drawback is that, based on the retrieval algorithm, not all relevant information may be retrieved. Further, fixing kreveals the advantage of controlling the result size. An alternative would be to return all chunks about a certain similarity threshold, introducing the question about the optimal cutoff. Figure 2 shows how the Discovery Agent extends on the RAG from Figure 1 shown in yellow hued. Instead of passing qto the RAG, the user submits it in1to the Discovery Agent, which then iteratively decomposes qinto a set of fine-grained tasks in 2. Breaking down the query into smaller, more manageable tasks can potentially fill the gap',\n",
       "  'chunk_id': 18},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'in 2. Breaking down the query into smaller, more manageable tasks can potentially fill the gap between the coarse semantics of the query and the specificities in the services documentation. In 3, the Discovery AgentAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 7 Figure2. Overview of the Discovery Agent Approach for Endpoint Discovery submits each task to the RAG to retrieve the set of relevant chunks to solve the current task specifically. Finally, in 4, the Discovery Agent collects the retrieval results of each individual task, filters them, and repeats 2ifqneeds further processing or returns the results to the user in 5. 3.2 OpenAPI Chunking Strategies A critical step in the RAG workflow is creating the chunks for the chunk database. registries can contain tens of thousands of services, each containing multiple potentially lengthy endpoints due to detailed descriptions or extensive input and output schemas. So, a single service might not fit into the context size of',\n",
       "  'chunk_id': 19},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'or extensive input and output schemas. So, a single service might not fit into the context size of the embedding model or even exceed the limit of the LLM that further processes the output of the RAG system. In addition, service documentation can also feature additional metadata that, while valuable for understanding service details, is not necessarily relevant for composing services to solve a query. Table 1. Implemented Chunking Strategies Category Splitting Refinement Meta-Parameters Token-based No split Token chunking m(model), s(chunk size), l(overlap) Endpoint split Token chunking m(model), s(chunk size), l(overlap) Endpoint split Remove examples m(model) Endpoint split Relevant fields m(model) JSON split Token chunking m(model), s(chunk size), l(overlap) LLM-based Endpoint split Query m(model) Endpoint split Summary m(model) To determine advantageous chunking strategies, we employ the seven different and output is a list of chunks. The chunking strategies can be categorized',\n",
       "  'chunk_id': 20},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'the seven different and output is a list of chunks. The chunking strategies can be categorized into token-based andLLM-based strategies. Each strategy consists of a splitting method, which dissects the OpenAPI specification into a list of intermediate chunks, and another refinement step, which converts the intermediate chunks to the final list of chunks. In addition, there is the meta-parameter for the used embedding model m. For the token chunking refinement step, there is also the8 Pesl et al. chunk size sin tokens and their overlap l, i.e., how many tokens two consecutive chunks share, in tokens. For the token-based approaches, we consider three main splitting methods. Theno split method returns a single intermediate chunk for each OpenAPI containing the whole specification. The endpoint split divides the OpenAPI into one chunk per endpoint. The JSON split is a built-in LlamaIndex3splitting strategy tailored to files in the JSON format. This strategy parses the JSON file and',\n",
       "  'chunk_id': 21},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'strategy tailored to files in the JSON format. This strategy parses the JSON file and traverses it using depth-first search, collecting leaf nodes, i.e., key-value pair where the value is a primitive type, e.g., strings, numbers, etc.. During this traversal, the parser concatenates keys and values into single lines of text to create a comprehensive textual representation of each leaf node. For the refinement, we implemented token chunking ,remove example , and relevant field . Thetoken chunking splits each intermediate chunk into a list of fixed-size chunks of stokens respecting an overlap of ltokens with the previous node. The remove example removes the requestBody and recursively all examples fields for each endpoint as these are typically lengthy but contribute little information. The relevant field extracts representative fields, i.e., service title, service description, endpoint verb, endpoint path, and endpoint description, which contribute much information but few tokens. For',\n",
       "  'chunk_id': 22},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'endpoint path, and endpoint description, which contribute much information but few tokens. For the LLM-based processing strategies, we apply the endpoint split and a summary (similar to [ 21]) andqueryapproach for refinement. In the summary approach, we prompt an LLM to generate a summary for each OpenAPI endpoint. For thequeryapproach, we instruct the LLM to generate a possible query matching the OpenAPI endpoint, as this might be closer to a possible input query than the summary. For both approaches, we only consider the LLM output for the embedding creation. The chunk content remains the original OpenAPI endpoint information. The no split and JSON split splitting methods can only be used with token chunking since all other refinement strategies rely on exactly one endpoint as an intermediate chunk. To evaluate the OpenAPI RAG and the Discovery Agent, we implement it as a fully operational prototype. Then, we employ the RestBench [ 35] benchmark to validate it in a real-world',\n",
       "  'chunk_id': 23},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'operational prototype. Then, we employ the RestBench [ 35] benchmark to validate it in a real-world setting. We implement the OpenAPI RAG and Discovery Agent approaches as open- source prototypes based on the LlamaIndex library.3For the prototypes, we rely solely on OpenAPIs as the state-of-practice for service descriptions. All sources and results are available online.4 4https://doi.org/10.18419/darus-4605Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 9 For the OpenAPI RAG, we focus on the components presented in Figure 1. At the first start, the system loads the OpenAPIs and applies a chunking strategy to create chunks and their embeddings for their later retrieval. The chunks contain thereby the information from the OpenAPIs, e.g., a whole endpoint or a part of it. A chunk embedding does not necessarily have to match the chunks content; for example, the content can be the endpoint, and the embedding is created using a natural language summary of the endpoint. Thus,',\n",
       "  'chunk_id': 24},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'the endpoint, and the embedding is created using a natural language summary of the endpoint. Thus, the matching is performed based on the embedding, and the result returned is the chunks content, which can include additional information not required for the matching process. As the service database, we use FAISS, which allows the storage and the similarity search of chunks [ 9]. We use a so-called QueryEngine from LlamaIndex for the chunk retriever, which allows us to query a chunk database based on textual We realize the Discovery Agent from Figure 2 using a LlamaIndex Ope- nAIAgent, which implements the LLM agent pattern for OpenAIs LLMs. An OpenAIAgent takes a list of tools, i.e., Python functions with a name and a description as parameters, and interacts with these using the OpenAI API. We implement two strategies for the tools. In the first strategy ( Query), we use the OpenAPI RAG as input for a LlamaIndex QueryEngineTool, which allows the agent to interact with the RAG on',\n",
       "  'chunk_id': 25},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'RAG as input for a LlamaIndex QueryEngineTool, which allows the agent to interact with the RAG on demand. This has the advantage of being a simple, straightforward implementation but may increase the token count as the results of the RAG are fed into the chat history, which is transferred to the LLM for the reasoning on this data. The second strategy ( Summary ) uses a RAG with chunks of the endpoints verb, path, and summary as contents and for their embeddings. We create the summary by instructing an LLM to create it based on the endpoint information, i.e., as in the summary chunking strategy. This should reduce the token count, as the chunks are much smaller, as not all endpoint details are returned and processed. To account for the same function as the first approach with the OpenAPI RAG and provide all information, we introduce a second tool, which takes the endpoint verb and path as input parameters and returns the whole endpoint information. The complete data is only inserted',\n",
       "  'chunk_id': 26},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'as input parameters and returns the whole endpoint information. The complete data is only inserted into the history for indispensable endpoints. To enable measuring the retrieved endpoints, we attach the endpoint infor- mation, i.e., verb and path, to each chunk as metadata. For the endpoint split splitting strategies, we take the information from the endpoint. For the other strategies, we first attach a list of all endpoints to the nodes before splitting and then filter on the endpoint paths in the final chunks after splitting. So, for each chunk, we know to which endpoint or endpoints it relates to. 4.2 Dataset and Metrics We evaluate our approach using the RestBench benchmark, covering the Spotify and TMDB OpenAPI specifications [ 35]. With 40 endpoints for Spotify and 54 for TMDB, this benchmark is much more complex than usual Service-Oriented Com- puting (SOC) case studies containing usually just three to seven endpoints [28].10 Pesl et al. Nevertheless, a holistic benchmark',\n",
       "  'chunk_id': 27},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'usually just three to seven endpoints [28].10 Pesl et al. Nevertheless, a holistic benchmark covering various domains is still missing (see RestBench contains 57 queries for Spotify and 100 for TMDB. For each of these queries, a solution set of endpoints is given, i.e., one to four endpoints that must be called to fulfill the query. For example, one query is Who directed the top-1 rated movie? The solution contains the endpoints GET /movie/top_rated As embedding models, we employ OpenAIs text-embedding-3-large [24] as one of the currently leading proprietary models. As open-source models, we utilize BAAI/ bge-small-en-v1.5 [39], which is relatively small while still producing reasonable results, allowing the model to be executed on commonly available hardware like laptops, and Nvidias NV-Embed-v1 [14] as one of the leading open-source models. For the LLM, we use OpenAIs gpt-4o-2024-05-13 . We evaluate the quality of the retrieved information in terms of accuracy and the token count of',\n",
       "  'chunk_id': 28},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': '. We evaluate the quality of the retrieved information in terms of accuracy and the token count of the returned result. We measure the accuracy using standard information retrieval metrics, namely recall =TP To correctly solve the query q, a service discovery approachs recall should ideally be maximal. However, this could result in retrieving several irrelevant services, turning into a drop in precision. The F1 score represents a balance between recall and precision. Table 2 shows the RestBench results for the OpenAPI RAG on the Spotify API. In recall, the JSON split method performs exceptionally well, especially with a high chunk size s, as this approach densely packs the information from the JSON into the chunks by removing all formatting. For precision and F1, the endpoint splitting approaches perform best because each chunk corresponds to precisely one endpoint. Differences between the models are minor, except that thebge-small-en-v1.5 performs worse for the no split approach. We',\n",
       "  'chunk_id': 29},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'the models are minor, except that thebge-small-en-v1.5 performs worse for the no split approach. We also tested s= 2048and s= 4096, which are not reported here for space reasons. We show s= 1024because it is the default chunk size of LlamaIndex and s= 8191 because it is the maximum input token count for the OpenAI model. It is worth mentioning that with an increasing chunk size, the token size of the returned result also increases. Generally, a higher recall seems to correlate with a higher token count, e.g., no splitting with s= 1024, and l= 0has 4717tokens output on average. In contrast, the JSON split has 10056with the same parameters, but this needs further analysis. Due to length limitations, we cannot show the token count comparison and other values for top khere. We also tested top k= 5and topk= 20. Recall increases with a higher top k, but precision drops. Additional data is available in the complementary material.4 Table 3 presents the OpenAPI RAG RestBench results for the',\n",
       "  'chunk_id': 30},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'in the complementary material.4 Table 3 presents the OpenAPI RAG RestBench results for the TMDB API. The TMDB OpenAPI is more complex in length and extent than the Spotify OpenAPI. In this case, the endpoint split-based approaches performs best inAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 11 Table 2. Results for the OpenAPI RAG for top k= 10with the Spotify API. The bge-small-en-v1.5 and NV-Embed-v1 , respectively. TC is for token chunking, RE for remove examples, and RF for relevant fields. Recall, precision, and F1 are in percent. The highest values per column are marked in bold. Category Splitting Refinement s lOSNOSNOSN Character No TC 1024 0674062161219261829 No TC 8191 0887191777121313 Endpoint TC 1024 0707576191920293131 Endpoint TC 8191 0737576191920293031 JSON TC 1024 08184859810171519 JSON TC 8191 09795100555101010 LLM Endpoint Query N/AN/A715758181515292324 Table 3. Results for the RAG for top k= 10with the TMDB API. Schema as in Category Splitting',\n",
       "  'chunk_id': 31},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'Table 3. Results for the RAG for top k= 10with the TMDB API. Schema as in Category Splitting Refinement s lOSNOSNOSN Character No TC 1024 0114733173821511 No TC 8191 0301715195923811 Endpoint TC 1024 0404046201518272126 Endpoint TC 8191 0664759191214291923 JSON TC 1024 0444446181216261924 JSON TC 8191 0616050886141411 LLM Endpoint Query N/AN/A566546131510202417 Endpoint Summary N/AN/A69596516131529222412 Pesl et al. Table 4. Results of the Discovery Agent experiments. We set topk= 10and use OpenAIs text-embedding-3-large as the embedding model. Spotify and TMDB are the two test sets from the RestGPT benchmark. RAG are the results for the summary chunking strategy from the Tables 2 and 3. Query is the standard LlamaIndex QueryEngineTool to retrieve data from a RAG system. The summary is our approach with a QueryEngineTool for summaries and a details-on-demand fetcher. Accuracy values are in percent. #Token is the number of tokens per query averaged over all queries in the test set. The',\n",
       "  'chunk_id': 32},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'in percent. #Token is the number of tokens per query averaged over all queries in the test set. The best value per row is marked in bold. Accuracy Recall71.92 63.70 66.4469.33 43.11 46.67 Precision 18.4267.3970.29 15.60 45.9750.97 #Token Prompt4233.658606.873125.21 41001.46 65699.75 4544.57 Completion 0.00262.30 256.26 0.00242.65 231.73 Total4233.658869.183411.47 41001.46 65942.40 4776.30 precision and F1. The no split approaches achieve high values in precision due to their low value of true positives. Overall, the endpoint split tends to outperform no splitting. The JSON splitting benefits Spotify as the endpoints are already very dense, i.e., the endpoints do not contain examples, and schemas are only referenced. Therefore, many endpoints can be condensed into one chunk. This approach performs much worse for the lengthier endpoints in the TMDB API. The summary refinement outperforms the query refinement, leading to the Discovery Agent. We present the RestBench results of the',\n",
       "  'chunk_id': 33},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'the query refinement, leading to the Discovery Agent. We present the RestBench results of the Discovery Agent in Table 4. For accuracy, we measure recall, precision, and F1 equally to the OpenAPI RAG experiments. For the token count, we measure the actual tokens sent from the agent to the LLM from the agent as prompt, the tokens received as completion , and their sum astotal. For the RAG approach, we accumulate the tokens of the retrieved The results show that both agent approaches improve precision and F1 but reduce recall. The Query approach increases the tokens in the prompt. Contrarily, in the total token count. The completion token count is by a magnitude smaller than the prompt token count for the agent approaches, which is relevant as completion tokens are usually more expensive than prompt tokens. No LLM is invoked in the RAG approach, so the completion tokens are zero.Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 13 We demonstrated the effectiveness of the',\n",
       "  'chunk_id': 34},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'OpenAPI Chunking for Retrieval-Augmented Generation 13 We demonstrated the effectiveness of the OpenAPI RAG and the Discovery Agent using our implementation. They are able to retrieve large portions of relevant data while not revealing all relevant information in all cases. To address RQ1, we implemented the OpenAPI RAG to apply RAG for endpoint discovery with seven chunking strategies and numerous parameter combinations. We showed its effectiveness using the RestBench benchmark. Overall, the ability to adequately reduce the token size to fit into the LLM context size while maintaining most of the relevant information is exhibited by the prototype. Regarding the chunking strategies, endpoint split-based chunking strategies achieve favorable accuracies. Limitations are primarily that the RAG results may not contain all relevant information, and the precision is low due to the retrieval of exactly kchunks. Additional research is needed to improve the retrieval performance further and',\n",
       "  'chunk_id': 35},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'of exactly kchunks. Additional research is needed to improve the retrieval performance further and prove the results in a generalized setting across For RQ2, we introduced the Discovery Agent, which transfers the LLM agent pattern to endpoint discovery. Especially using Summary approach, the Discovery Agent showed strong improvement over the OpenAPI RAG in terms of precision, F1, and token count. Further research is needed to improve the decline in recall due to the processing through the LLM. While we rely on the research benchmark RestBench for our results, which covers two extensive OpenAPIs, queries, and ground truth, it is still limited to these two services. OpenAPI RAG systems in practice may operate on much larger datasets. For the data processing, we rely on standard RAG implementations like LlamaIndex, which are already designed to operate on large amounts of data. The performance evaluation, especially in larger real-world scenarios, remains open for future research. The',\n",
       "  'chunk_id': 36},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'evaluation, especially in larger real-world scenarios, remains open for future research. The applicability of the OpenAPI RAG depends on the availability of service documentation. We try to mitigate this issue by relying on widely adopted OpenAPI specifications, but this might not be valid for all domains. A solution to consider is automatically generating service documentation using an LLM. Another factor influencing the discovery is the quality of the OpenAPIs. The discovery may fail if no descriptions, meaningful naming, or erroneous information is given. This is not an issue of the approach, as a human developer would face the same problem, but it highlights the importance of high-quality documentation. strategies, e.g., CRAFT [ 42], could be added to the OpenAPI RAG. These could improve retrieval performance by combining multiple strategies or by creating a custom chunking strategy for a specific kind of service documentation. Another advancement could also be creating a custom',\n",
       "  'chunk_id': 37},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'for a specific kind of service documentation. Another advancement could also be creating a custom embedding model tailored explicitly to service descriptions and service description chunks. This model may also be trained for one specific chunking strategy or intended use case. Additionally, the RAG output may be trimmed to boost precision. This could be done by, e.g., employing a similarity threshold.14 Pesl et al. The presented Discovery Agent could be further improved to handle whole service compositions. In this case, the agent would be extended by an additional component for the service composition, and the user would only submit their ser- vice composition task to the agent to retrieve the executable service composition Besides capabilities of the RAG system, resource consumption is a major issue in LLM-based systems. The OpenAPI RAG only uses embedding models. These are much more efficient than LLMs, resulting in costs in fractions of a cent per query. In contrast, the Discovery',\n",
       "  'chunk_id': 38},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'than LLMs, resulting in costs in fractions of a cent per query. In contrast, the Discovery Agent requires significantly more resources, i.e., running RestBench in our experiments resulted in about $50 of API fees. Further work is needed to reduce this resource footprint. different ISs. With the application of automated LLM-based service composition approaches, the LLM input context limitations have become prominent, as the the preselection of relevant information. To address this issue, we proposed an OpenAPI RAG, which facilitates semantic search based on state-of-the-practice OpenAPIs and reduces the input token size. Further, we show an advanced integration through a Discovery Agent, which can retrieve service details on demand to reduce the input token count further. Our evaluation based on the RestBench benchmark shows that our approach is viable and performing. Limitations are especially in the restriction of RestBench to two services of the entertainment domain. We will address',\n",
       "  'chunk_id': 39},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'in the restriction of RestBench to two services of the entertainment domain. We will address this in an extended version of this work. Further improvements are in optimizing the implementation and extending the agent for additional tasks, e.g., whole service compositions. We leave this for Acknowledgments. This work was partially funded by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) project Software-Defined Car (SofD- Car) (19S21002). The authors acknowledge support by the state of Baden-W rttemberg Disclosure of Interests. The authors Pesl and Aiello are listed as inventors of a patent [27], which covers automated service composition using LLMs for the automotive 1.Achiam, J., et al.: GPT-4 technical report (2023), https://arxiv.org/abs/2303. 2.AI@Meta: Llama 3 model card (2024), https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.mdAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 15 3.Baresi, L., Miraz, M.: A distributed approach',\n",
       "  'chunk_id': 40},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'Chunking for Retrieval-Augmented Generation 15 3.Baresi, L., Miraz, M.: A distributed approach for the federation of heterogeneous registries. In: ICSOC 2006. pp. 240 251. Springer (2006). https://doi.org/10. 4.Bohn, H., Golatowski, F., Timmermann, D.: Dynamic device and service discovery extensions for WS-BPEL. In: ICSSSM 2008. pp. 1 6. IEEE (2008). https://doi. 5.Cobbe, K., et al.: Training verifiers to solve math word problems (2021), https: 6.Cuconasu, F., et al.: The power of noise: Redefining retrieval for RAG systems. In: SIGIR. vol. 47, pp. 719 729 (2024). https://doi.org/10.1145/3626772.3657834 7.Curbera, F., et al.: Unraveling the web services web: an introduction to SOAP, WSDL, and UDDI. IEEE Internet Computing 6(2), 86 93 (2002). https://doi. 8.Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: NAACL-HLT 2019. pp. 9. Douze, M., et al.: The Faiss library (2024),',\n",
       "  'chunk_id': 41},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'for language understanding. In: NAACL-HLT 2019. pp. 9. Douze, M., et al.: The Faiss library (2024), https://arxiv.org/abs/2401.08281 10.Fan, A., et al.: Large language models for software engineering: Survey and open 11.Fikouras, I., Freiter, E.: Service discovery and orchestration for distributed service repositories. In: ICSOC 2003. pp. 59 74. Springer (2003). https://doi.org/10. 12. Gao, L., et al.: Pal: Program-aided language models. In: International Conference on Machine Learning. pp. 10764 10799. PMLR (2023) 13.Kim, M., Stennett, T., Shah, D., Sinha, S., Orso, A.: Leveraging large language models to improve REST API testing. In: ICSE. vol. 44, pp. 37 41 (2024). https: 14.Lee, C., et al.: Nv-embed: Improved techniques for training llms as generalist embedding models (2024), https://arxiv.org/abs/2405.17428 15.Lemos, A.L., Daniel, F., Benatallah, B.: Web service composition: A survey of techniques and tools. ACM Comput. Surv. 48(3) (dec 2015). https://doi.org/10. 16.Lewis, P., et',\n",
       "  'chunk_id': 42},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'of techniques and tools. ACM Comput. Surv. 48(3) (dec 2015). https://doi.org/10. 16.Lewis, P., et al.: Retrieval-augmented generation for knowledge-intensive NLP tasks. In: NeurIPS. vol. 33, pp. 9459 9474. Curran Associates (2020) 17.Li, M., et al.: API-Bank: A comprehensive benchmark for tool-augmented LLMs. In: EMNLP. Association for Computational Linguistics (2023). https://doi.org/ 18.Liang, Y., et al.: Taskmatrix.AI: Completing tasks by connecting foundation models with millions of APIs. Intelligent Computing 3, 0063 (2024). https://doi.org/ 19.Mialon, G., et al.: Augmented language models: a survey (2023), https://arxiv. 20.Nakano, R., et al.: WebGPT: Browser-assisted question-answering with human 21.Nogueira, R., Yang, W., Lin, J., Cho, K.: Document expansion by query prediction 22.OpenAI: Function calling and other API updates (Jun 2024), https://openai.com/ index/function-calling-and-other-api-updates/ , last accessed 2024-07-18 23.OpenAI: GPT-4 Turbo in the OpenAI API.',\n",
       "  'chunk_id': 43},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': ', last accessed 2024-07-18 23.OpenAI: GPT-4 Turbo in the OpenAI API. https://help.openai.com/en/ articles/8555510-gpt-4-turbo-in-the-openai-api (2024), last accessed 2024- 11-1916 Pesl et al. 24.OpenAI: New embedding models and API updates (Jan 2024), https://openai. com/blog/new-embedding-models-and-api-updates , last accessed 2024-07-18 25.Parisi, A., Zhao, Y., Fiedel, N.: Talm: Tool augmented language models (2022), 26.Patil, S.G., Zhang, T., Wang, X., Gonzalez, J.E.: Gorilla: Large language model connected with massive APIs (2023), https://arxiv.org/abs/2305.15334 27.Pesl, R.D., Klein, K., Aiello, M.: Verfahren zur Nutzung von unbekannten neuen Systemdiensten in einer Fahrzeuganwendung (2024), Patent DE102024108126A1 28.Pesl, R.D., St tzner, M., Georgievski, I., Aiello, M.: Uncovering LLMs for service- composition: Challenges and opportunities. In: ICSOC 2023 WS. Springer (2024). 29.Pesl, R.D., et al.: Compositio Prompto: An architecture to employ large language models in',\n",
       "  'chunk_id': 44},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': '29.Pesl, R.D., et al.: Compositio Prompto: An architecture to employ large language models in automated service computing. In: ICSOC 2024. Springer (2024) 30.Radford, A., Wu, J., Amodei, D., Amodei, D., Clark, J., Brundage, M., Sutskever, I.: Better language models and their implications. OpenAI blog 1(2) (2019), https: //openai.com/index/better-language-models/ , last accessed 2024-11-28 31.Radford, A., et al.: Improving language understanding by generative pre-training 32.Santana, J.M.S., Petrova, M., Mahonen, P.: UPnP service discovery for heteroge- neous networks. In: IEEE PIMRC. vol. 17, pp. 1 5. IEEE (2006) 33.Shi, Z., et al.: Chain of tools: Large language model is an automatic multi-tool 34.Soki, A.T., Siqueira, F.: Discovery of RESTful Web services based on the OpenAPI 3.0 standard with semantic annotations. In: AINA. pp. 22 34. Springer (2024). 35.Song, Y., et al.: RestGPT: Connecting large language models with real-world applications via restful APIs (2023),',\n",
       "  'chunk_id': 45},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'RestGPT: Connecting large language models with real-world applications via restful APIs (2023), https://arxiv.org/abs/2306.06624 36.Th nes, J.: Microservices. IEEE software 32(1), 116 116 (2015). https://doi.org/ 37. Vaswani, A., et al.: Attention is all you need. NeurIPS 30(2017) 38.Wei, J., et al.: Chain-of-thought prompting elicits reasoning in large language models. NeurIPS 35, 24824 24837 (2022) 39.Xiao, S., Liu, Z., Zhang, P., Muennighoff, N.: C-pack: Packaged resources to advance general chinese embedding (2023), https://arxiv.org/abs/2309.07597 40.Yao, S., et al.: React: Synergizing reasoning and acting in language models (2023), 41.Yao, S., et al.: Tree of thoughts: Deliberate problem solving with large language 42.Yuan, L., et al.: CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets (2024), https://arxiv.org/abs/2309.17428',\n",
       "  'chunk_id': 46},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet? Abstract Hallucinations, the tendency to produce irrelevan- t/incorrect responses, are prevalent concerns in generative AI- based tools like ChatGPT. Although hallucinations in ChatGPT are studied for textual responses, it is unknown how ChatGPT hallucinates for technical texts that contain both textual and technical terms. We surveyed 47 software engineers and produced a benchmark of 412 Q&A pairs from the bug reports of two OSS projects. We find that a RAG-based ChatGPT (i.e., ChatGPT tuned with the benchmark issue reports) is 36.4% correct when producing answers to the questions, due to two reasons 1) limita- tions to understand complex technical contents in code snippets like stack traces, and 2) limitations to integrate contexts denoted in the technical terms and texts. We present CHIME ( ChatGPT Inaccuracy Mitigation Engine) whose underlying principle is that if we can preprocess the technical',\n",
       "  'chunk_id': 0},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Inaccuracy Mitigation Engine) whose underlying principle is that if we can preprocess the technical reports better and guide the query validation process in ChatGPT, we can address the observed limitations. CHIME uses context-free grammar (CFG) to parse stack traces in technical reports. CHIME then verifies and fixes ChatGPT responses by applying metamorphic testing and query transformation. In our benchmark, CHIME shows 30.3% more correction over ChatGPT responses. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT Index Terms ChatGPT, Hallucination, Software Issue Reports The reliability of LLMs is often questioned due to their tendency to produce nonsensical or incorrect outputs, a phe- nomenon commonly referred to as hallucination [1], [2], [3]. Like any LLM, ChatGPT can also suffer from hallucination issues like inconsistency in responses [4], [5] or factual inaccuracies. These problems can arise even when',\n",
       "  'chunk_id': 1},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'inconsistency in responses [4], [5] or factual inaccuracies. These problems can arise even when the model is provided with the context as a document/paragraph. While progress is made to assess hallucinations in textual data [6], [7], we are not aware of how hallucinations can be detected and mitigated for software technical reports that contain both textual and technical terms (e.g., crash dumps, code snippets, This paper studies the detection and mitigation of ChatGPT inaccuracies in technical reports. We pick software bug reports for our study, because bug reports often contain a blend of descriptive text, technical terminology, code references, and snippets of crash/system dumps [8]. These documents are crucial for tracking and resolving software issues but can be overwhelming due to their volume and complexity [9]. An AIchatbot, trained to understand these reports, may streamline the process by extracting information. But for that, first we need to ensure that the responses from',\n",
       "  'chunk_id': 2},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'process by extracting information. But for that, first we need to ensure that the responses from the chatbot are correct. In the first phase of our study , we conducted a survey of 47 software engineers to understand the types of questions they ask while exploring bug reports and for which they wish for an automated Q&A tool like a chatbot. We found that developers ask diverse questions during bug exploration, which we could group into five types: 1) issue analytics, 2) issue trends, 3) issue summary, 4) issue labeling, and 5) issue backlogs. Based on the survey findings, we produced a benchmark of 412 Q&A pairs by consulting our industry partner (with whom we conducted regular bi-weekly sessions) and the literature. The Q&A pairs are collected by analyzing the issue reports of two popular open-source software (OSS). In the second phase of our study , we tuned ChatGPT with issue reports from the two studied OSS based on the Retrieval Augmented Generation (RAG) techniques [10], [11],',\n",
       "  'chunk_id': 3},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'from the two studied OSS based on the Retrieval Augmented Generation (RAG) techniques [10], [11], [12], [13]. We then evaluated the correctness of ChatGPT re- sponses against our benchmark. Each question was asked and compared automatically to its expected answer. Correctness was assessed as the ratio of questions whose answers were found as correct. We found that our RAG-based ChatGPT was correct in only 36.4% cases. For the rest of the questions, it hallucinated by producing incorrect or irrelevant answers. We manually examined each hallucination case and identified two limitations in ChatGPT to process technical documents like bug reports: 1) limitations to understand complex technical contents in code snippets like stack traces (e.g., when a partial code snippets/crash dump is provided and the question is about determining the cause of the crash by assessing both the crash dump and the textual contents), and 2) limitations to integrate contexts denoted in the technical terms and',\n",
       "  'chunk_id': 4},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'the textual contents), and 2) limitations to integrate contexts denoted in the technical terms and texts (e.g., when ChatGPT was required to assess the relationships among multiple metadata and the technical terms). In the third phase of our study , we designed CHIME (ChatGPT Inaccuracy Mitigation Engine) to address the above two limitations. The underlying principle in CHIME is that (1) if we can preprocess the technical reports better and store information relevant to an issue report as a combination of metadata and actual contents and (2) then guide the query validation process in ChatGPT with guided iterative prompting approaches, we can address the observed limitations. The 11 Nov 2024TABLE I: Demography of Survey Participants Current Role 0-3 4-5 6-10 11-15 16-20 Total Developer 13 20 4 2 - 39 QA Engineer 1 1 - - - 2 Project Manager - 1 - - - 1 Other 2 - 2 - 1 5 Total 16 22 6 2 1 47 (a)Resolution Triaging Reporting Management89.36% Fig. 1: (a) Frequency & (b) Reasons of Bug',\n",
       "  'chunk_id': 5},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '2 1 47 (a)Resolution Triaging Reporting Management89.36% Fig. 1: (a) Frequency & (b) Reasons of Bug Report Exploration usage of metadata is found to improve LLM search capabilities [14]. For us, such metadata could be generated by organizing the mix of textual and technical contents into a structured form. A challenge was how to separate the textual and technical contents and process the code terms within a crash dump and then organize those within a structure. We introduce a novel context-free grammar (CFG) in CHIME to efficiently parse stack traces in technical reports. As for the second principle (i.e., guided prompting for verification), we extended recent similar work on textual content. CHIME verifies and fixes ChatGPT responses by using query transformation [15] and by extending CoVe [16] with metamorphic testing (MT) [17]. CoVe is a zero-shot iterative prompting-based query verifi- cation technique. We evaluated CoVes response by further mutating the question using MT, because',\n",
       "  'chunk_id': 6},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'cation technique. We evaluated CoVes response by further mutating the question using MT, because CoVe may discard correct responses or promote incorrect responses. We evaluated CHIME using our benchmark. CHIME shows on average 30.3% improvement over ChatGPT responses by offering more correct answers. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT without CHIME. Our replication package (https://bit.ly/4fyaMIP) contains all the data and code developed in the study. II. R ELATED WORK A. Hallucinations in Large Language Models Extensive studies in the literature identified the causes of hallucinations as sub optimal training, inference [18], [19], [20], [21], [22], and insufficient/low-quality data [23], [24], [25]. Techniques such as bidirectional auto-regressive models [26] and attention-sharpening mechanisms [27] have been developed to address training-related hallucinations. Inference issues, primarily due',\n",
       "  'chunk_id': 7},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'have been developed to address training-related hallucinations. Inference issues, primarily due to decoding strategies, often result in in- accurate outputs. Strategies like factual-nucleus sampling [28]and in-context pretraining [29] are implemented to mitigate these inaccuracies. Challenges posed by flawed data sources introduce biases and inaccuracies into models, stemming from misinformation, duplication biases, and social biases in the training datasets. Mitigating data biases involves manual dataset creation [30], integrating high-quality sources such as the Pile [31], and up-sampling factual data [32]. Furthermore, knowledge editing [33], [34] and Retrieval-Augmented Gen- eration (RAG) [35], [11], [12], [13] are employed to bridge knowledge gaps, utilizing external sources for more accurate Our study utilizes RAG-based ChatGPT for technical bug report understanding. We enhance ChatGPTs knowledge base by integrating it with a database of bug reports through RAG methods. We then',\n",
       "  'chunk_id': 8},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'knowledge base by integrating it with a database of bug reports through RAG methods. We then develop CHIME, which refines both the preprocessing of input data and the validation of RAG-based B. LLMs for Software Engineering In recent years, the application of LLMs has been widely utilized in Software Engineering (SE) tasks, ranging from code analysis to bug detection [36], [37], [38], [39], [40], [41], [42], [43], [44]. Encoder-only models like BERT [45] and its derivatives, including CodeBERT [46] and GraphCodeBERT [47], excel in processing code. While encoder-decoder models, such as T5 and PLBART excel in understanding semantics of code for tasks like code summarization [48], [49]. Decoder- only models like the GPT series and specialized versions like CodeGPT and Codex generate direct responses from prompts. However, the specific challenge of mitigating inaccuracies in software technical reports remains unexplored. Addressing this gap, we introduce CHIME to reduce inaccuracies in',\n",
       "  'chunk_id': 9},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'reports remains unexplored. Addressing this gap, we introduce CHIME to reduce inaccuracies in ChatGPT- generated responses during bug report exploration. III. AI C HATBOT NEEDS FOR BUGREPORT EXPLORATION To evaluate a chatbot on software technical documents, we needed a benchmark, which at the time of our study was not available. We thus adopted a systematic approach to create such a benchmark. First, we conducted a survey of software developers to produce a catalog of questions that they ask during bug reports. Second, we used the catalog to produce our benchmark (see Section IV). This section discusses the survey, which answers the following research question (RQ): RQ1. What types of questions would software practitioners like to ask a chatbot during bug report exploration? We employed the snowball sampling approach [50] to recruit participants, resulting in 47 software practitioners. The majority of respondents (83%) held developer roles. The largest proportion of participants (55%)',\n",
       "  'chunk_id': 10},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'majority of respondents (83%) held developer roles. The largest proportion of participants (55%) reported having 4- 5 years of experience in the field. Table I illustrates the distribution of participants across their roles in the software industry and years of experience. We ensured that all se- lected participants regularly use Issue Tracking Systems (ITS), such as Jira, GitHub, or in-house systems for several keyTABLE III: Key Questions (KQ) of the Survey. 1 How would you like to utilize a chatbot during bug 2 Would you want to use the chatbot for T#? C 3 Rate the usefulness of T# for your work C 4 Rate the potential usefulness of the following bench- mark questions of T#C TABLE II: Identified Task Types for Bug Report Exploration T# Task Title Task Description T1 Issue Analytics Explores complex details within/across bug reports, including technical jargon, error codes, and contextual nuances, etc. T2 Issue Trends Patterns and trends in bug occurrences. T3 Issue Summaries',\n",
       "  'chunk_id': 11},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'contextual nuances, etc. T2 Issue Trends Patterns and trends in bug occurrences. T3 Issue Summaries Summaries of key topics discussed in an issue or across multiple issues. T4 Issue Labeling Inquires labels for bug reports to help orga- nize and categorize them effectively. T5 Issue Backlog Explores whether the issue remains open for purposes. Around 89.36% of respondents used ITS for issue resolution addressing and for solving bugs; followed by issue triaging which involves prioritizing and assigning issues. Project Management and documenting issues for records or stakeholder communication were also noted. A vast majority of respondents (70.21%) reported engaging with issue reports daily. Figure 1a presents the distribution of issue report inter- action frequency and Figure 1b illustrates the survey responses for the primary reason for issue report exploration. Before the main survey, we consulted two software profes- sionals from a reputed software company to get insights on the',\n",
       "  'chunk_id': 12},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'we consulted two software profes- sionals from a reputed software company to get insights on the potential tasks that an AI chatbot can support during bug report understanding. Each had 16 and 35 years of experience respectively. We finally settled on five distinct tasks as shown in Table II). For our survey, we formulated questions to address During the survey, participants were prompted to envision an AI chatbot with comprehensive access to bug reports. Subsequently, they were asked whether they would like to utilize the chatbot for the specified tasks (T#) and to rate the perceived usefulness of each task for their work. We condensed the key survey questions into Table III for clarity. All survey questions can be found in our online appendix. C. Understanding Preferences of Software Practitioners on Identified Task Types (RQ1) In Table III, with Key Question 1 (KQ1), we tried to identify the desired chatbot roles during bug report explorations. With KQ2-4, we examined how',\n",
       "  'chunk_id': 13},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'to identify the desired chatbot roles during bug report explorations. With KQ2-4, we examined how participants perceived the useful- ness of each task type. The questions are summarized in TableIV. For each question, we show quantitative evidence from the survey about what the participants thought of the usefulness 1) KQ1. Desired Chatbot Roles: We asked KQ1 as an open- ended question to check whether the participants considered the same five task types that we identified in Table II. As such, we did not show them KQ2-4 until they answered KQ1. Participants desired a tool whose capabilities closely aligned with our predefined tasks (T1 T5). They emphasized the chat- bots potential to analyze issues (T1), such as extracting pivotal details like exceptions or log entries from similar bug reports. According to R43: Queries for potential duplicate issues could be really helpful. The capability to analyze trends (T2) within reported issues to uncover recurring problems was seen as crucial.',\n",
       "  'chunk_id': 14},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'to analyze trends (T2) within reported issues to uncover recurring problems was seen as crucial. Participants also noted the importance of the chatbots ability to summarize (T3) and categorize (T4) issues. The survey responses indicated a significant interest in features that would allow users to query backlogged items (T5), as R43 stated: Queries for issues with very little recent activity and no clear resolution would be super helpful. Apart from these tasks, participants also expressed desires for additional capabilities such as sorting issues based on priority, severity, or difficulty and forecasting resolution times using historical data. We leave support for those tasks as our future work. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 2: (a) Interest for T1 (b) Usefulness Perception of T1 2) KQ2-4. Issue Analytics (T1): The survey results reveal a strong preference for the chatbots analytical capabilities, particularly in the context of',\n",
       "  'chunk_id': 15},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'reveal a strong preference for the chatbots analytical capabilities, particularly in the context of analyzing multiple issues, with more than 80% of participants expressing interest in utilizing the chatbot for detecting similar or duplicate issues and finding it useful; while 72.34% participants value the chatbots utility in analyzing individual issues as illustrated in Figure 2. In Table IV, we show 10 questions under T1 that each par- ticipant assessed. On average, 73.62% of participants marked these useful. The capability to identify and summarize stack traces (Q1.1) within the single issue analysis domain was highly valued, evidenced by a utility score of 78.72% and remark from respondent R11, Summary of stack trace is a good idea . In comparison, the importance of determining the environment linked to an exception (Q1.2) was rated lower, at 57.45%. For the analysis of multiple issues, the ability to find similar issues (Q1.3) was highly valued at 82.98%, as quoted by R09, They',\n",
       "  'chunk_id': 16},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'the ability to find similar issues (Q1.3) was highly valued at 82.98%, as quoted by R09, They are all extremely time-consuming when done manually. A chatbot will definitely help with this. 0 20% 40% 60% 0No Maybe Yes 0 20% 40% 60% 80% 0Not Useful Neutral Useful Fig. 3: (a) Interest for T2 (b) Usefulness Perception of T2TABLE IV: Perceived Usefulness of Benchmark Questions Presented in the Survey. Not Useful Neutral Useful Q# Question Perceived Usefulness of T# (KQ4) T1 - Issue Analytics Extracting Information from Issue Details or Find Similarities Among Issues Q1.1 Is there a stack trace provided in issue 123, and can you summarize it? Q1.2 Where in the code does the exception in issue 123 occur? Q1.3 What is the exception reported in issue 123? Q1.4 How many tests failed as reported in issue 123? Q1.5 Which environment is associated with the exception reported in issue 123? Q1.6 Are there any issues similar to issue 123? Q1.7 Find duplicate reports of the X error (or other) in A',\n",
       "  'chunk_id': 17},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'there any issues similar to issue 123? Q1.7 Find duplicate reports of the X error (or other) in A module Q1.8 Find all similar issues related to X failures] Q1.9 Identify any performance degradation issues reported on last month Q1.10 Has there been a report of a crash on a X machine running the A Test recently? T2 - Issue Trend Detect and Analyze the Trends and Patterns among Issues Q2.1 What are the frequently encountered errors in the nightly builds? Q2.2 What are the recurring themes in bug reports post the latest OS update? T3 - Issue Summary Obtain a Comprehensive Overview of Reports Selected by Different Criterion Q3.1 List all issues related to an X feature and their current status Q3.2 Generate a report detailing the distribution of issues across different project modules Q3.3 Compile a summary of unresolved issues not older than 60 days Q3.4 Can you generate a summary of all issues tagged as bug in the last 30 days? Q3.5 Create a summary of user-reported issues versus',\n",
       "  'chunk_id': 18},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'all issues tagged as bug in the last 30 days? Q3.5 Create a summary of user-reported issues versus internally identified issues T4 - Issue Label Provide Suggestions for Categorizing and Tagging Issues with Appropriate Labels Q4.1 Suggest existing labels to tag issue 123 Q4.2 Can you recommend labels for performance-related issues? T5 - Issue Backlog Analyze Unresolved Issues Reported but not yet Addressed Q5.1 Are there any long-standing issues that have been consistently postponed? Q5.2 Find issues that have not been assigned to any milestone but are older than 60 days Q5.3 List issues that have missed two or more release cycles Q5.4 Identify issues with no activity in the last 30 days. 3) KQ2-4. Issue Trend (T2): 87.2% of participants ex- pressed interest in using this feature and 85.1% found the corresponding questions on identifying and analyzing trends within bug reports useful (see Figure 3). In Table IV, we show two questions that we asked under this task. Both received a',\n",
       "  'chunk_id': 19},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '(see Figure 3). In Table IV, we show two questions that we asked under this task. Both received a favorable response. Respondent R31 noted, By focusing on recurring errors and themes, these questions provide valuable insights that can guide decision-making, resource allocation, and issue resolution efforts. When participants were asked about the utility of chatbots in identifying frequently encoun- tered errors in the nightly builds of their development envi- ronment (Q2.1), e.g., in CI/CD pipelines, 76.60% perceived this functionality as useful. 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 4: (a) Interest for T3 (b) Usefulness Perception of T3 4) KQ2-4. Issue Summary (T3): 80.6% of participants were keen on a chatbot to produce summaries of issues and 70.2% considered the asked questions useful for efficiently understanding and resolving software issues. In Table IV, regarding the chatbots ability to report on how issues are distributed across',\n",
       "  'chunk_id': 20},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'issues. In Table IV, regarding the chatbots ability to report on how issues are distributed across different project modules (Q3.1), 76.6% found this function useful. On the other hand, the feature for distinguishing between user-reported and internallyidentified issues (Q3.2) was seen as useful by 51.06% of participants, indicating a notable but more moderate interest in differentiating the sources of issues. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 5: (a) Interest for T4 (b) Usefulness Perception of T4 5) KQ2-4. Issue Labeling (T4): 70.2% of participants are interested in leveraging chatbots for the task of issue labeling (see Figure 5). However, its worth noting that this task re- ceived the lowest percentage of perceived usefulness (61.70%) compared to others. Regarding the chatbots ability to suggest appropriate labels for an issue (Q4.1) and to recommend labels for performance-related issues (Q4.2), about 68% of respondents considered',\n",
       "  'chunk_id': 21},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'and to recommend labels for performance-related issues (Q4.2), about 68% of respondents considered these features to be useful. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 6: (a) Interest for T5 (b) Usefulness Perception of T5 6) KQ2-4. Issue Backlog (T5): The management of Issue Backlogs is an essential aspect of software development. R16 highlighted the challenge: Sometimes change of prioritiespushes issue out of find and stay unresolved for days. So it is good to find out long-running or inactive issues. Despite its importance, this task garnered the least interest (63.8%) among all tasks for potential chatbot utilization. Notably, T5 records the highest percentage of No responses (17.02%) regarding interest and Not Useful perceptions (19.15%). 70.2% of respondents see value in identifying long-standing, postponed issues (Q5.1) but interest slightly drops to 61.70% for detecting issues with no recent activity over the last 30 Summary of RQ1.',\n",
       "  'chunk_id': 22},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'drops to 61.70% for detecting issues with no recent activity over the last 30 Summary of RQ1. When examining software practition- ers preferences for AI chatbot capabilities in exploring bug reports, the identification of similar issues and the analysis of recurring error trends were highly favored. In contrast, capabilities related to categorizing issues and handling pending bugs were deemed less critical. IV. A B ENCHMARK OF Q&A P AIRS TO EVALUATE AI CHATBOTS FOR BUGREPORT EXPLORATION In Table IV, we showed a catalog of 23 questions that we validated with our survey participants and for which they wished for chatbot support. Each question is a template, which can be used to produce multiple similar questions.Based on the question templates in Table IV, we produced a total of 412 questions from the issue reports of two popular OSS repos, OpenJ9 and ElasticSearch. OpenJ9 was chosen due to its alignment with our industrial partner. ElasticSearch [51], [52], [53] is frequently',\n",
       "  'chunk_id': 23},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'due to its alignment with our industrial partner. ElasticSearch [51], [52], [53] is frequently referenced in academic studies. We then produced an answer to each question by assessing the two OSS repos and by consulting among the authors. Four authors (the first two and last two) engaged in many hours of discussions that spanned over six months (both in- person and over formal presentations). The last two authors are also among the maintainers of OpenJ9. Given the benchmark was created via mutual discussion, we did not compute any standard agreement analysis metrics. We created the benchmark by selecting 80 complex issues (40 from each repository). Following Deeksha et al. [9], we define an issue as complex if it is excessively long and/or it has stack traces. We picked issues within the last year of our analysis because those are likely to be explored more by developers. We sorted issues by length and selected 40 issues with stack traces (per repo). Following the standard chatbot',\n",
       "  'chunk_id': 24},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'by length and selected 40 issues with stack traces (per repo). Following the standard chatbot evaluation process, we contained three types of answers: binary [54], factual [55], [56], and summary [57]. The binary (i.e., Query Type = Y/N) queries have ques- tions with answers as Y/N. These are designed to assess the chatbots accuracy in identifying clear-cut, definitive binary decisions based on information available in bug reports; such as the presence of a particular error code or the applicability of a specific scenario. To verify a chatbot response for these queries, we simply need to check for Y/N in their responses and match those against the benchmark answer.Q1. Type: Y/N. Source: ElasticSearch Question: Is there any issue similar to issue 100071? The factual (i.e., Query Type = Factual) queries assessed the chatbots ability to extract concrete information from bug reports, such as identifying, retrieving, and presenting specific details from the dataset, such as error messages,',\n",
       "  'chunk_id': 25},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'identifying, retrieving, and presenting specific details from the dataset, such as error messages, stack traces, configuration settings, etc. Like binary queries, this method also allows for a straightforward assessment of the chatbots accuracy, and thus direct matching can be used for verification. Q2. Type: Factual. Source: ElasticSearch Question: What existing label is recommended for issues that need immediate triaging? The summary-based (i.e., Query Type = Summary) queries challenge the chatbot to engage in deeper analysis and syn- thesis of data. These queries require the chatbot to identify patterns and even to propose potential solutions based on the analysis of multiple data points. Since these queries demand a synthesis of information and provide insights or summaries, we need a similarity analysis between a response and the expected answer for verification. Q3. Type: Summary. Source: ElasticSearch Question: Summarize similarities between issues 103072 Expected Answer:',\n",
       "  'chunk_id': 26},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Source: ElasticSearch Question: Summarize similarities between issues 103072 Expected Answer: Issues 103072 and 103344 both involve test failures within the LearningToRankRescorerIT class. The root cause of these failures stems from a named object notfound exception and xcontent parse exception, resulting in ... TABLE V: Distribution of Benchmark Questions over Survey- identified Tasks from Table IV. T# Y/N Fact Summarization T1 - Issue Anlys(S)48 140 24 212 T1 - Issue Anlys(M)12 20 8 40 T2 - Issue Trend16 16 8 40 T3 - Issue Summary8 32 40 T4 - Issue Labeling12 20 8 40 T5 - Issue Backlog12 24 440 Table V shows the distribution of question types Yes/No, Fact, and Summarization across OpenJ9 and ElasticSearch, totaling 206 questions per project. OpenJ9 and ElasticSearch have a similar overall structure, with a strong emphasis on factual questions (114 for OpenJ9, 114 for ElasticSearch), followed by binary (yes/no) and summarization questions. Our online appendix contains details about',\n",
       "  'chunk_id': 27},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'followed by binary (yes/no) and summarization questions. Our online appendix contains details about each of the 412 questions and how each question maps to our catalog of 23 survey questions.V. E FFECTIVENESS OF CHATGPT ON THE BENCHMARK In this section, we answer the following research question: RQ2. How effective is a RAG-enhanced ChatGPT to answer to the benchmark questions while exploring the corre- The RAG architecture combines ChatGPT with an external knowledge retriever to provide responses to queries. This framework utilizes external database sources, primarily issue reports with structural data and metadata fetched by the GitHub API. Figure 7 illustrates the pipeline for this. It functions by first retrieving pertinent information from the database based on the input query. This step is crucial as it aligns the models focus with the most relevant data. Then, the augmented data from the retrieval step are combined with the inherent generative capabilities of ChatGPT to help',\n",
       "  'chunk_id': 28},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'from the retrieval step are combined with the inherent generative capabilities of ChatGPT to help ChatGPT provide high-quality responses. We used ChatGPT 3.5-turbo within LangChain framework [58] to implement this pipeline. We used a temperature setting of 0. A temperature value above 0 produces slightly different answers to a prompt across multiple runs, which is unnecessary when we expect consistent answers from ChatGPT. We ran it multiple times on our benchmark dataset to ensure that the answers were indeed consistent across multiple runs. Fig. 7: Pipeline of the RAG-based ChatGPT We measure the effectiveness of the above RAG-based ChatGPT on our benchmark by calculating correctness (C): C=Number of Correct Responses Total Number of Queries 100% (1) Correctness analysis involved comparing the generated re- sponses against the predefined correct answer for a query in the benchmark. We adopted the following automated approaches to measure the correctness of the responses for the',\n",
       "  'chunk_id': 29},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'We adopted the following automated approaches to measure the correctness of the responses for the three types of queries in our benchmark (i.e., Y/N, Fact, and Summaries). For evaluating yes/no responses, we used a zero-shot clas- sification approach, enabling us to automatically determine if detailed answers from the chatbot align with a simple Yes or No expectation. For evaluating factual query responses, we combine two approaches: direct comparison of extracted information (such as issue numbers) and semantic similarity assessment for non-listed facts. First, we extract and compare key factual elements. If the response and expectation directly match or share common elements, the correctness is assessed accordingly. For responses without explicit factual elements, we utilize the SentenceTransformer library, employing the all- MiniLM-L6-v2 model [59] to encode the actual and expected answers into embeddings. Subsequently, we gauge the seman- tic similarity between these embeddings by',\n",
       "  'chunk_id': 30},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'into embeddings. Subsequently, we gauge the seman- tic similarity between these embeddings by computing thecosine similarity [60] and assessing how closely the actual answer aligns with the expected fact, considering nuances in phrasing and context. For evaluating Summarization queries, we compute the semantic similarity like before between the actual summary provided by ChatGPT and the expected sum- mary. Based on empirical observations (see Section VIII-B), we used a similarity threshold of 0.7. The RAG-based pipeline achieved 36.4% accuracy in our benchmark (see Table VI). We manually assessed each of the 262 incorrect answers to determine the causes of its incorrectness. Given that ChatGPT is a black-box model, our assessment is based on the nature of the questions asked and the provided answers. We observed two limitations in L1. Limitations in Understanding Complex Technical Content L2. Contextual Understanding Challenges (57.3% cases). TABLE VI: Correctness of RAG Based ChatGPT',\n",
       "  'chunk_id': 31},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'L2. Contextual Understanding Challenges (57.3% cases). TABLE VI: Correctness of RAG Based ChatGPT T# Y/N Fact Sum Total Total 49.0% 30.7% 36.9% 36.4% T1 - Issue Anlys(S) 41.7% 32.1% 33.3% 34.4% T1 - Issue Anlys(M) 50.0% 15.0% 0.0% 22.5% T2 - Issue Trend 50.0% 12.5% 37.5% 32.5% T3 - Issue Summary 0.0% 75.0% 43.8% 50.0% T4 - Issue Labeling 58.3% 40.0% 62.5% 50.0% T5 - Issue Backlog 66.7% 25.0% 25.0% 37.5% The challenge in understanding technical contents (L1) occurred when ChatGPT faced challenges in deeply under- standing and accurately processing highly technical content, particularly when it involved intricate programming/technical concepts. For example, when asked to provide the root location of the exception encountered in issue 18151; it gives a non- useful answer; even though the issue provides a stack trace. Q4. Asking ChatGPT - OpenJ9 Question: Where in code exception of issue 18151 occur? Incorrect Answer: The exception of issue 18151 occurs in the body of the issue. Again,',\n",
       "  'chunk_id': 32},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '18151 occur? Incorrect Answer: The exception of issue 18151 occurs in the body of the issue. Again, for asking to find the line number where the error occurs, ChatGPT just gives an irrelevant answer by showing Q5. Asking ChatGPT - OpenJ9 Question: Which line in CharacterDataLatin1 class trig- gered ArrayIndexOutOfBoundsException in issue 17063? Irrelevant Answer: The line of code that is The Contextual Understanding Challenges category (i.e., L2) shows a fundamental difficulty in ChatGPT to processand respond to queries about technical issues. This category primarily involves the models struggles with: Integrating and interpreting the context in which queries are made. Some context is explicitly stated within the query or the referenced issue, such as a specific error message or stack trace. Other times, the context is implicit, requiring the model to infer based on its broader knowledge or related data points. Handling ambiguous or insufficiently detailed queries necessitates the',\n",
       "  'chunk_id': 33},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'or related data points. Handling ambiguous or insufficiently detailed queries necessitates the chatbot to fill in the gaps with assumptions or inferred knowledge. Not all contextual in- formation holds equal relevance to a given query, requiring the chatbot to prioritize the most pertinent context based on the nuances of the query. Adapting to the technical conventions of specific domains. Technical domains often have their own conventions for documentation, communication, and issue tracking. For instance, understanding that a particular label in an issue tracking system denotes the responsible team, requires domain-specific knowledge that the AI must possess. On asking to find a responsible team for an issue, ChatGPT searches on the assignee list, but the team details are on the issue labels. Due to this lack of contextual information, it fails to answer the question. Q6. Asking ChatGPT - ElasticSearch Question: Which team is responsible for issue 104160? Incorrect Answer: The team',\n",
       "  'chunk_id': 34},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '- ElasticSearch Question: Which team is responsible for issue 104160? Incorrect Answer: The team responsible for issue 104160 is not specified in the database. Even when the relevant context is identified, retrieving and applying it accurately to generate a response is challenging. This includes understanding the specific ways in which information is structured or presented within data sources and how it relates to the users query. Effectively bridging this gap is crucial for generating accurate and contextually Summary of RQ2. A RAG-based ChatGPT showed an average correctness of 36.4% on our benchmark. The pipeline encountered challenges in comprehending com- plex technical content and grasping contextual nuances, leading to inaccuracies in its responses. VI. CHIME: C HATGPT I NACCURACY MITIGATION Our observations in Section V of ChatGPT limitations contributed to the design of CHIME, as a suite of techniques to detect and fix incorrectness in ChatGPT responses. The underlying',\n",
       "  'chunk_id': 35},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'as a suite of techniques to detect and fix incorrectness in ChatGPT responses. The underlying principle of CHIME is that by offering ChatGPT with a more structured representation of bug reports and by applying a systematic approach to assess ChatGPT responses, we can address the two limitations we observed in Section V-B. A more structured representation of bug reports can be achieved if we can process the different technical and textual terms properly and store those in a structured way, e.g., in a database with metadata offering more information about thoseterms. A systematic approach to verify the responses can be achieved by applying/adapting the techniques of guided itera- tive prompting of LLM responses that are used in the literature for textual content. As such, we designed to preprocess the inputs (both the bug report and the query) and to verify the CHIME treats ChatGPT as an API, where the inputs (ques- tions) and outputs (answers) are processed for inaccuracy detection and',\n",
       "  'chunk_id': 36},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'where the inputs (ques- tions) and outputs (answers) are processed for inaccuracy detection and mitigation. We can use another LLM as an API in CHIME and apply all the techniques we developed. Doing so would simply require changing the API endpoints to point to the other LLM within the LangChain toolkit. CHIME will need to be updated significantly while using multi-modal LLMs, e.g., to process/validate modalities other than texts, etc. Fig. 8: The Architecture of CHIME In Figure 8, we show the architectural diagram of CHIME. We apply an Issue Preprocessor component to parse crash dumps and other technical terms. Issue Preprocessor employs Context-Free Grammars (CFGs) to interpret complex technical data, addressing ChatGPTs limitations in understanding com- plex technical content. To address the second limitation (i.e., Contextual understanding challenges), we introduce two more components in CHIME. First, we apply Query Preprocessor component to decipher users intents. Second, we',\n",
       "  'chunk_id': 37},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'in CHIME. First, we apply Query Preprocessor component to decipher users intents. Second, we designed the Response Validator component, which evaluates the accuracy in a response using a combination of two techniques: Chain of Verification (CoVe) [16] and Metamorphic Testing (MT) [17]. A query goes through each of these components. We discuss We developed a CFG as shown in Listing 1 which sup- ports parsing stack traces to the level of individual code elements. Stack traces can also contain auxiliary information like timestamps and memory addresses (e.g., crash dumps). We preprocessed these auxiliary information and applied our CFG to parse the code elements in a Java stack trace. Finally, our issue preprocessor stored the code blocks in the database by separating the code blocks from the textual contents. The issue preprocessor saves the following information by parsing an issue report: title, body, stack trace, and other metadata(creation date, status, etc). When ChatGPT, enhanced',\n",
       "  'chunk_id': 38},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'title, body, stack trace, and other metadata(creation date, status, etc). When ChatGPT, enhanced with Issue Preprocessor, is presented with the query regarding the type of exception in issue 18151 (Q4) or 17063 (Q5), it correctly identifies the location of the exception detailed in Listing 1: CFG Grammar for Stack Traces 1 Root ::= StackTraceElems 2 StackTraceElems ::= StackTraceElem StackTraceElems | 3 StackTraceElem ::= ExceptionElems |CodeDetails 4 ExceptionElems ::= ExceptionElem ExceptionElems | 5 ExceptionElem ::= ExceptionType |ExceptionMessage 6 CodeDetails ::= ClassElem |MethodElem |FileElem | Asking CHIME with Q4 from Section V. OpenJ9 Question: Where in the code exception of issue 18151? Correct Answer: The exception in issue 18151 can be triggered at line 98 in the JvmErgonomics.java file. Asking CHIME with Q5 from Section V. OpenJ9 Question: Which code in CharacterDataLatin1 triggered the ArrayIndexOutOfBoundsException in issue 17063? Correct Answer: Line 72 in the',\n",
       "  'chunk_id': 39},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'triggered the ArrayIndexOutOfBoundsException in issue 17063? Correct Answer: Line 72 in the CharacterDataLatin1 A user query may not provide enough details, leading ChatGPT to generate responses based on faulty assumptions or context. We leveraged the query transformation algorithm from [15] that strategically rephrases and modifies a query. Each query goes through this component for error correction as follows. We prompt ChatGPT with instructions on how to transform a query based on the question type. For example, for a Yes/No query type, we instruct ChatGPT as follows Change is/are/have there issues to check if there are any issues with the provided condition. For a summarization type question, the instruction is Summarize the contents from issue title, exceptions, body, and labels. We then provide some example transformations (in a few-shot setting) to clarify the instruction, like the following. Original Query: Are issue 18102 and 18669 similar? Transformed Query: Compare the',\n",
       "  'chunk_id': 40},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'the following. Original Query: Are issue 18102 and 18669 similar? Transformed Query: Compare the exceptions, stack traces, and descriptions of issues 18102 and 18669 to Finally, GPT answers based on the transformed query. For example, when presented with the query, Q6 from Section V, the Query Preprocessor directs the question with additional in- structions: Summarize the team responsible for issue 104160 based on the assignee and labels. This instruction results in generating a more precise SQL query to our database (we used LangChain SQL engine + RAG ChatGPT pipeline).Asking CHIME with Q6 from Section V. ElasticSearch Question: Which team is responsible for issue 104160? Correct Answer: The team responsible for issue 104160 is the ML team, with no specific assignee and labels indicating it is related to ML. The third component of CHIME is added to validate ini- tial responses from RAG-based ChatGPT. We employed two strategies sequentially, CoVe [16] and Metamorphic Testing (MT)',\n",
       "  'chunk_id': 41},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'RAG-based ChatGPT. We employed two strategies sequentially, CoVe [16] and Metamorphic Testing (MT) [17]. Drawing inspiration from investigative interroga- tion techniques, CoVes methodology is designed to test the consistency and authenticity of ChatGPTs responses through a series of challenge questions. CoVes process can be unfolded into three distinct stages 1) Upon receipt of a query, ChatGPT formulates an initial response based on its interpretation of the query within the context of available data. 2) CoVe then autonomously generates a set of follow-up questions related to the initial response. CoVe dynamically determines the number of follow-up questions. These questions aim to clarify its preceding statements. 3) The subsequent responses are ana- lyzed for consistency with the initial response. Any observed contradictions signal the need for a review and adjustment of the initial response, leading to the refinement or generation of a new, more accurate answer. Listing 2: How',\n",
       "  'chunk_id': 42},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'response, leading to the refinement or generation of a new, more accurate answer. Listing 2: How CoVe validates in CHIME Initial Question: Is issue 18102 and 18669 similar? Response: No, issue 18102 and 18669 are not similar. Follow-up Questions Generated by CoVe 1. Can issue 18102 and 18669 be considered similar based on their descriptions and reported problems? 2. Have there been any interactions or dependencies between the teams working on issue 18102 and 18669 that could 3. Are the issues 18102 and 18669 related to the same error? ChatGPT Responses to Follow-up Questions 1. Yes, based on the descriptions and reported problems, they can be considered similar as both involve 2. Yes, there have been interactions between the teams working on issue 18102 and 18669. 3. No, issues 18102 and 18669 are not related to same error. Final Response: Yes, issue 18102 and 18669 are similar. However, CoVe can discard correct responses (if not found in responses to challenge questions) or promote',\n",
       "  'chunk_id': 43},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'CoVe can discard correct responses (if not found in responses to challenge questions) or promote incorrect responses (if responses are found both in base and challenge questions). We thus refine CoVe with MT as follows. Using CoVe to verify ChatGPTs initial responses, we get an intermediate response which is then passed for consistency verification to MT. Central to MT is the concept of metamor- phic relations (MRs), which involve applying transformationsto input queries to anticipate changes in output. In CHIME, this approach is leveraged to introduce nuanced variations to the queries for testing the consistency of its responses under diverse conditions. A specific implementation of MRs in CHIME involves Sentence-Level Mutation, where equivalent questions are generated to test. This process includes 1) using predefined MRs to subtly alter the phrasing or focus of the original queries. In our implementation, MT generates three mutated questions each time in CHIME. For instance, the',\n",
       "  'chunk_id': 44},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'In our implementation, MT generates three mutated questions each time in CHIME. For instance, the original query might be What causes error X in module Y?, and its mutated equivalent could be Why does error X occur in module Y?. 2) The responses ChatGPT provides for both the original and mutated queries are compared to assess consistency with MR expectations. If an MR implies that adding specific details should refine the response, the eval- uation checks for this level of refinement. After the refinement via MR, CHIME generates the final response for the users. The user in CHIME does not see the mutated questions from CoVe or the mutated questions from MRs. Instead, the user only sees the final response from CHIME. However, CHIME can print the mutated questions to the user if needed. Listing 3: How CoVe + MT validates in CHIME Initial Question: Does Elasticsearch require the UseG1GC option to be present during issue 18151 startup stage? Initial ChatGPT Response: No, Elasticsearch',\n",
       "  'chunk_id': 45},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'option to be present during issue 18151 startup stage? Initial ChatGPT Response: No, Elasticsearch does not require the UseG1GC option to be present during its startup stage in issue 18151. Follow-up Questions Generated by CoVE : 1. Is Elasticsearch the software mentioned in the response? 2. Is the UseG1GC option not required? 3. Is issue 18151 not requiring the UseG1GC option? ChatGPT Responses to Follow-up Questions : 1. Yes, Elasticsearch is mentioned in the response. 2. No, the UseG1GC option is not required. 3. No, issue 18151 is not requiring the UseG1GC option. CoVE : No, the UseG1GC option is not required during Mutated Questions Generated by MT : 1. Is the UseG1GC option necessary for Elasticsearch to be present during its startup stage in issue 18151? 2. Must the UseG1GC option be included during Elasticsearch s startup stage for issue 18151? 3. Is it required to have the UseG1GC option present during Elasticsearchs startup stage for issue 18151? ChatGPT Responses to Mutated',\n",
       "  'chunk_id': 46},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'option present during Elasticsearchs startup stage for issue 18151? ChatGPT Responses to Mutated Questions : 1. No, the UseG1GC option is not necessary for Elasticsearch s to be present during its startup stage in issue 18151. 2. Yes, the UseG1GC option must be included during Elasticsearchs startup stage for issue 18151. 3. Yes, it is required to have the UseG1GC option present during Elasticsearchs startup stage for issue 18151. Final Response: Yes, it is required to have the UseG1GC option during Elasticsearchs startup stage .VII. E FFECTIVENESS OF CHIME We evaluate CHIME by answering three RQs: RQ3. How well can CHIME fix inaccuracies in ChatGPT RQ4. How well do individual components in CHIME per- RQ5. Would responses from CHIME be favored like those from ChatGPT when both are correct? RQ3 and RQ4 investigate whether CHIME and its components can fix inaccuracies in ChatGPT while we use our benchmark. Similar to RQ2, we use the correctness metric from Equation 1 to answer RQ3 and',\n",
       "  'chunk_id': 47},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'use our benchmark. Similar to RQ2, we use the correctness metric from Equation 1 to answer RQ3 and RQ4. RQ5 assesses the usability of CHIME when it is used by developers instead of a RAG-based ChatGPT. We conduct a user study to answer RQ5. TABLE VII: Correctness of CHIME. Column Improv shows percent improvement over RAG-based ChatGPT T# Y/N Fact Sum Total Improv Total 80.0% 61.4% 65.5% 66.7% +30.3% T1 - Issue Anlys(S) 83.3% 67.1% 66.7% 70.8% +36.3% T1 - Issue Anlys(M) 58.3% 30.0% 50.0% 42.5% +20.0% T2 - Issue Trend 68.8% 43.8% 50.0% 55.0% +22.5% T3 - Issue Summary 0.0% 87.5% 68.8% 72.5% +22.5% T4 - Issue Labeling 83.3% 60.0% 87.5% 72.5% +22.5% T5 - Issue Backlog 100.0% 58.3% 50.0% 70.0% +32.5% A. How well can CHIME fix ChatGPT inaccuracies? (RQ3) Table VII presents the assessments of the correctness of CHIME in our benchmark by offering overall results, across the three types of queries and also across the five task types in our benchmark. Overall, CHIME offers around 30.3%',\n",
       "  'chunk_id': 48},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'of queries and also across the five task types in our benchmark. Overall, CHIME offers around 30.3% improvement over the RAG-based ChatGPT pipeline from Section V. The improvement is consistent across all five task types, with issue analytics and backlog tasks benefiting the most from CHIME. CHIME showcases enhancements over ChatGPT across all tasks for both OpenJ9 and ElasticSearch: 29.6% and 31.1% improvement over ChatGPT for OpenJ9 and ElasticSearch respectively. The detailed result for each project is provided in our online appendix. We manually assessed the responses where CHIME was inaccurate and observed three main reasons as follows. Query-Directed Retrieval Failure (60.6%) : CHIME relies on its ability to query a database of stored data and generate SQL queries based on the provided questions. However, when user or verifying queries lack clarity, the query fails to provide clear instructions for formulating SQL queries. In such cases, CHIME produces incorrect or irrelevant',\n",
       "  'chunk_id': 49},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'instructions for formulating SQL queries. In such cases, CHIME produces incorrect or irrelevant responses. Logical Inference Errors (27%) : This pertains to cases where CHIME fails to accurately apply logical inference principles. It occurs when CHIME incorrectly deduces information from the data or makes faulty assumptions during reasoning. Semantic Discrepancy (10.9%) : CHIME relies on similarity scores to match user queries with existing data or responses. However, discrepancies in semantic similarity assessments can lead to incorrect matches or associations.TABLE VIII: Correctness of components of CHIME. Column Improv shows percent improvement over RAG-based Chat- Component Y/N Fact Sum Total Improv Issue Preprocessor 59.0% 43.9% 46.4% 48.1% +11.7% Query Preprocessor 57.0% 35.1% 45.2% 42.5% +6.1% Response Validator 55.0% 36.0% 47.6% 43.0% +6.6% CoVe 58.0% 28.9% 38.1% 37.9% +1.5% MT 69.0% 37.7% 53.6% 48.5% +12.1% Summary of RQ3. CHIME achieves an average correct- ness of 66.7% and',\n",
       "  'chunk_id': 50},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '69.0% 37.7% 53.6% 48.5% +12.1% Summary of RQ3. CHIME achieves an average correct- ness of 66.7% and an improvement of 30.3% over a RAG- based ChatGPT on our benchmark of bug report questions. B. How do individual components in CHIME perform? (RQ4) We ran each component of CHIME individually and deter- mined the contribution of the component within the pipeline. In Table VIII, we show the performance of each component per query type and also show whether the component offered an improvement over a RAG-based ChatGPT. We discuss how we ran each component while analyzing the results below. Issue Preprocessor. In our CHIME pipeline, we kept this component and removed the other two components (i.e., Query Processor and Response Validator). Hence, issue reports are preprocessed by this component and then stored in the database. From here, we utilize RAG-based ChatGPT for Q&A. The integration of the Issue Processor enhances the accuracy of the baseline GPT model by 11.7% on average for both',\n",
       "  'chunk_id': 51},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'of the Issue Processor enhances the accuracy of the baseline GPT model by 11.7% on average for both projects. This improvement is particularly notable in technical question comprehension and analysis tasks. Query Preprocessor. Similar to the above setup, we only kept this component and removed the other two components in our CHIME pipeline (i.e., Issue Preprocessor and Response Val- idator). On average, this process demonstrates an improvement of 6.1% over a RAG-based ChatGPT. Response Validator. We used it to validate responses from an RAG-based ChatGPT. Overall, this component contributed to a 6.6% improvement over a RAG-based ChatGPT. The bottom two rows in Table VIII further illustrate the performance of the two modules in the Response Validator, i.e., CoVe and MT. Interestingly, MT as an individual module worked even better than the Response Validator component. MT offered a 12.1% improvement over RAG-ChatGPT while CoVe offered a 1.5% improvement. However, we kept the',\n",
       "  'chunk_id': 52},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'a 12.1% improvement over RAG-ChatGPT while CoVe offered a 1.5% improvement. However, we kept the combinations of CoVe and MT in the response validator, because CoVe + MT may become more useful for other repos where responses may need a sequence of challenges via both CoVe and MT. For example, when the responses from CoVe contain references to the fact (but with incorrect summarization), MT can double- check those facts via follow-up mutated questions. As we can see from Tables VII and VIII, CHIME as an end-to-end pipeline offers 30.3% improvement over RAG-based ChatGPT, while none of the individual components in CHIME could offer more than 12% improvement over RAG-based ChatGPT. This means that the ensemble of all the components in CHIMEs pipeline helped the fixing of ones mistake by others. For instance, when a user queries List all pending issues rather than simply providing the count of pending issues, the transformed query from Query Preprocessor prompts the system to generate a',\n",
       "  'chunk_id': 53},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'of pending issues, the transformed query from Query Preprocessor prompts the system to generate a list of issue numbers, which increases its accuracy. Summary of RQ4. Each component in CHIME can offer an improvement over a RAG-based ChatGPT by correcting the inaccuracies in ChatGPT responses. The components work best when they are all put together in CHIME as an C. Would responses from CHIME favored like those from ChatGPT when both are correct? (RQ5) A comparative study was conducted to assess the practical efficacy of CHIME, involving 31 participants. The majority (93%) had 0-5 years of experience in the software industry, with 57% being software developers and 33% researchers. 1) Survey Setup: Participants were presented with two random questions from each task in our benchmark dataset. The questions include a summarization of the failure of an issue, similarities between multiple issues, recurring themes in a component, pending issues, identification of error-prone components,',\n",
       "  'chunk_id': 54},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'issues, recurring themes in a component, pending issues, identification of error-prone components, unresolved or blocker issues, guidelines for label- ing, and label suggestions. For these questions, responses from both CHIME and ChatGPT, along with links to associated bug reports, were provided for evaluation. To ensure a fair comparison, only questions with correct responses from both systems were selected. Participants were then asked to rate the correctness and perceived usefulness of the responses in addressing software bug-related queries. The survey questions are provided in our online appendix. 2) Survey Result: Participant feedback in Table IX indi- cates that CHIME was the preferred choice for the majority of tasks when the answers were correct and selected more frequently for 6 out of 10 questions. It was favored in issue analysis (T1) with a 79% participant preference. On average, for this task, 63.6% of participants found the responses to be comprehensive and covering all',\n",
       "  'chunk_id': 55},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'for this task, 63.6% of participants found the responses to be comprehensive and covering all necessary aspects, 45.1% felt that they provided additional information helpful for a better understanding of the problem, and 33.3% thought the responses were clear and easy to follow. Their preference also extended to issue summarization (T3) with a 65% preference and to issue labeling (T4), with a 63% preference. However, for the issue trending task (T2), there was a slight preference for ChatGPT. Nonetheless, for two questions, participants seemed undecided, indicating a comparable level of usefulness between CHIME and ChatGPT when the responses were correct.0.60.65 0.70.75 0.80.85 0.90.9510%20%30%40%50%60%70%80%90% ThresholdGPT Issue Pre. Query Pre. RV Fig. 9: Impact of Threshold on Similarity Analysis. Here, Issue Pre. = Issue Preprocessor, Query Pre. = Query Preprocessor, RV = Response validator TABLE IX: Selection Preference of CHIME and ChatGPT Provided Correct Responses across Tasks',\n",
       "  'chunk_id': 56},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'TABLE IX: Selection Preference of CHIME and ChatGPT Provided Correct Responses across Tasks T1 - Issue Analytics21% 79% T2 - Issue Trend58% 42% T3 - Issue Summary35% 65% T4 - Issue Labeling37% 63% T5 - Issue Backlog48% 52% Summary of RQ5. In a comparative study with 31 partici- pants, CHIME responses were preferred over a stand-alone ChatGPT for the majority of tasks when both provided correct answers. This preference was particularly evident for tasks related to issue analysis, summarization, and A. Accuracy of our CFG We evaluated the CFG-based stack trace parsing by as- sessing the 80 issue reports that we used to create our benchmark dataset. The CFG is designed to identify key elements, such as exception types, messages, and code details (e.g., class/method/file names, etc.). For each stack trace, we checked whether the parser found all key elements as expected. We used three metrics to compute accuracy: precision, recall, and F1-score. Precision is the ratio of correctly',\n",
       "  'chunk_id': 57},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'metrics to compute accuracy: precision, recall, and F1-score. Precision is the ratio of correctly identified elements to the total elements identified by the parser. Recall is the ratio of correctly identified elements to the total actual elements in the stack trace. F1-score ( F1) is the harmonicmean of precision and recall. We manually created a list of the expected elements for accurate comparison for each of the 80 issue reports. We observed an average precision of 0.99 and recall of 0.91 (F1-score = 0.93). The few errors in parsing were mainly due to the limitations in our regular expressions used in the CFG parser, and the variations in stack trace formats across issue reports. Our replication package contains the details of B. Threats to Validity Concerns regarding construct validity arise from the bench- marks design. However, we derived our benchmark queries from survey responses. The selection of issues from OpenJ9 and ElasticSearch may affect the generalizability of the',\n",
       "  'chunk_id': 58},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'The selection of issues from OpenJ9 and ElasticSearch may affect the generalizability of the find- ings across various software engineering contexts. The partic- ipant pool in surveys might not comprehensively represent the diverse perspectives in the broader software engineering com- munity. Finally, the methodology used for evaluating CHIME could affect the accuracy and objectivity of our effectiveness assessment and may introduce methodological bias . However, we have analyzed the accuracy rates of summary queries across various similarity threshold values of CHIME, ranging from 0.60 to 0.95, and chose the threshold of 0.7 as it gives the highest accuracy (see Figure 9), an optimal trade-off between capturing relevant information and minimizing false positives. We have introduced CHIME to mitigate the inaccuracy of ChatGPT response during bug report exploration. CHIME demonstrates 30.3% improvements over ChatGPT in terms of providing more correct responses for bug exploration',\n",
       "  'chunk_id': 59},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '30.3% improvements over ChatGPT in terms of providing more correct responses for bug exploration tasks. Our industrial partner is working on deploying CHIME as a Slack bot. The conceptualization of CHIME originated from an internal demo of a similar chatbot created by our partner one year ago. Their initial chatbot lacked the required accuracy, which we sought to address by developing CHIME. To further motivate the need for such a chatbot beyond our industrial partner, we conducted a survey of 47 software practitioners (see Section III). The survey findings highlight the necessity of such chatbots in the real world. Feedback from 31 industry participants, presented in Section VII-C shows that CHIME is preferred for its ability to analyze, summarise, and label issues. Our industrial partner was involved in the design and evalu- ation of CHIME, which was crucial for advancing CHIME from the proof-of-concept stage to the current deployment stage within the company. Like any innovation,',\n",
       "  'chunk_id': 60},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'the proof-of-concept stage to the current deployment stage within the company. Like any innovation, we expect to improve CHIME in an agile manner, i.e., based on user In the future, we will also focus on expanding CHIMEs grasp of more technical terminologies and other documents. To handle other documents, in CHIME we will improve the issue preprocessor module e.g., to separate code and textual contents, and to adapt the CFG to handle code snippet- s/traces/crash dumps in those documents or using a static partial program analyzer to handle code examples in API documentation. We expect that the other modules in CHIME can be used with minimal changes.DATA AVAILABILITY The code and data used for this study can be found here: [1] Z. Ji et al. , Survey of hallucination in natural language generation, ACM Computing Surveys , vol. 55, no. 12, pp. 1 38, 2023. [2] K. Filippova, Controlled hallucinations: Learning to generate faithfully from noisy data, in Findings of the Association for',\n",
       "  'chunk_id': 61},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'hallucinations: Learning to generate faithfully from noisy data, in Findings of the Association for Computational Linguistics: EMNLP 2020 , T. Cohn, Y . He, and Y . Liu, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 864 870. [3] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, ArXiv , vol. abs/2311.05232, 2023. [Online]. Available: [4] M. Jang, D. S. Kwon, and T. Lukasiewicz, BECEL: Benchmark for consistency evaluation of language models, in Proceedings of the 29th International Conference on Computational Linguistics , N. Calzolari, C.-R. Huang, H. Kim, J. Pustejovsky, L. Wanner, K.-S. Choi, P.-M. Ryu, H.-H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim, Y . Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S.-H. Na, Eds. Gyeongju, Republic of Korea: International Committee on',\n",
       "  'chunk_id': 62},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Lee, E. Santus, F. Bond, and S.-H. Na, Eds. Gyeongju, Republic of Korea: International Committee on Computational Linguistics, Oct. 2022, pp. 3680 3696. [Online]. [5] M. Jang and T. Lukasiewicz, Consistency analysis of chatgpt, in Conference on Empirical Methods in Natural Language Processing , 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID: [6] R. Cohen, M. Hamri, M. Geva, and A. Globerson, LM vs LM: Detecting factual errors via cross examination, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 12 621 12 640. [Online]. Available: https://aclanthology.org/2023.emnlp-main. [7] B. A. Galitsky, Truth-o-meter: Collaborating with llm in fighting its [8] OpenJ9. (2024, February) Issue 18151: To accept openjdk option +useg1gc to enable startup of elasticsearch application]. [Online]. [9] D. Arya, W. Wang, J. L. C. Guo,',\n",
       "  'chunk_id': 63},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'to enable startup of elasticsearch application]. [Online]. [9] D. Arya, W. Wang, J. L. C. Guo, and J. Cheng, Analysis and detection of information types of open source software issue discussions, inProceedings of the 41st International Conference on Software Engineering , ser. ICSE 19. IEEE Press, 2019, p. 454 464. [Online]. [10] Z. Jiang, F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang, J. Callan, and G. Neubig, Active retrieval augmented generation, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 7969 7992. [Online]. Available: https: [11] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K uttler, M. Lewis, W.-t. Yih, T. Rockt aschel, S. Riedel, and D. Kiela, Retrieval-augmented generation for knowledge-intensive nlp tasks, in Proceedings of the 34th International Conference on Neural Information',\n",
       "  'chunk_id': 64},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'nlp tasks, in Proceedings of the 34th International Conference on Neural Information Processing Systems , ser. NIPS20. Red Hook, NY , USA: Curran [12] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang, Realm: retrieval-augmented language model pre-training, in Proceedings of the 37th International Conference on Machine Learning , ser. ICML20. [13] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, Retrieval augmentation reduces hallucination in conversation, in Findings of the Association for Computational Linguistics: EMNLP 2021 , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3784 3803. [Online]. Available: https: [14] K. Beelen and D. van Strien, Metadata might make language models better, 2022. [Online]. Available: https://arxiv.org/abs/2211.10086[15] X. Ma, Y . Gong, P. He, hai zhao, and N. Duan, Query rewriting in retrieval-augmented large language models, in The 2023',\n",
       "  'chunk_id': 65},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'hai zhao, and N. Duan, Query rewriting in retrieval-augmented large language models, in The 2023 Conference on Empirical Methods in Natural Language Processing , 2023. [Online]. [16] S. Z. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. E. Weston, Chain-of-verification reduces hallucination in large language models, 2024. [Online]. Available: [17] T. Chen, F. Kuo, H. Liu, P. Poon, D. Towey, T. Tse, and Z. Zhou, Metamorphic testing: A review of challenges and opportunities, ACM Computing Surveys , vol. 51, no. 1, pp. 1 27, 2018. [18] C. Wang and R. Sennrich, On exposure bias, hallucination and domain shift in neural machine translation, 07 2020. [19] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang, Exposing attention glitches with flip-flop language modeling, in Thirty-seventh Conference on Neural Information Processing Systems , 2023. [Online]. [20] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, Deduplicating',\n",
       "  'chunk_id': 66},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, Deduplicating training data makes language models better, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 8424 8445. [Online]. [21] N. Dziri, A. Madotto, O. Za ane, and A. J. Bose, Neural path hunter: Reducing hallucination in dialogue systems via path grounding, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 2197 2214. [Online]. [22] H.-S. Chang and A. McCallum, Softmax bottleneck makes language models unable to represent multi-mode word distributions, in Proceedings of the 60th Annual Meeting of the Association',\n",
       "  'chunk_id': 67},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'multi-mode word distributions, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 8048 8073. [Online]. [23] B. Dhingra, M. Faruqui, A. P. Parikh, M.-W. Chang, D. Das, and W. W. Cohen, Handling divergent reference texts when evaluating table-to-text generation, ArXiv , vol. abs/1906.01081, 2019. [Online]. [24] S. Lin, J. Hilton, and O. Evans, TruthfulQA: Measuring how models mimic human falsehoods, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 3214 3252. [25] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee, A. Roberts, T. B. Brown, D. X. Song, U. Erlingsson, A. Oprea, and C. Raffel,',\n",
       "  'chunk_id': 68},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Herbert-V oss, K. Lee, A. Roberts, T. B. Brown, D. X. Song, U. Erlingsson, A. Oprea, and C. Raffel, Extracting training data from large language models, in USENIX Security Symposium , 2020. [Online]. Available: [26] Z. Li, S. Zhang, H. Zhao, Y . Yang, and D. Yang, Batgpt: A bidirectional autoregressive talker from generative pre-trained transformer, ArXiv preprint , vol. abs/2307.00360, 2023. [27] Y . Liu, Y . Yao, J.-F. Ton, X. Zhang, R. Guo, H. Cheng, Y . Klochkov, M. F. Taufiq, and H. Li, Trustworthy llms: A survey and guideline for evaluating large language models alignment, ArXiv preprint , vol. [28] N. Lee, W. Ping, P. Xu, M. Patwary, M. Shoeybi, and B. Catanzaro, Factuality enhanced language models for open-ended text generation, ArXiv , vol. abs/2206.04624, 2022. [Online]. Available: https://api. [29] W. Shi, X. Han, M. Lewis, Y . Tsvetkov, L. Zettlemoyer, and S. W.-t. Yih, Trusting your evidence: Hallucinate less with context-aware decoding, ArXiv preprint , vol.',\n",
       "  'chunk_id': 69},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Yih, Trusting your evidence: Hallucinate less with context-aware decoding, ArXiv preprint , vol. abs/2305.14739, 2023. [30] A. Radford et al. , Language models are unsupervised multitask learn- ers, OpenAI blog , vol. 1, no. 8, p. 9, 2019. [31] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al. , The pile: An 800gb dataset of diverse text for language modeling, arXiv preprint arXiv:2101.00027 , 2021.[32] H. Touvron et al. , Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2306.00186 , 2023. [33] A. Sinitsin, V . Plokhotnyuk, D. Pyrkin, S. Popov, and A. Babenko, Editable neural networks, in 8th International Conference on Learning Representations, ICLR 2020 . Addis Ababa, Ethiopia: OpenReview.net, [34] Y . Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, Editing large language models: Problems, methods, and opportunities, in Proceedings of the 2023 Conference on Empirical Methods',\n",
       "  'chunk_id': 70},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Problems, methods, and opportunities, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 10 222 10 240. [Online]. Available: [35] Z. Jiang et al. , Active retrieval augmented generation, arXiv preprint [36] D. Sobania, M. Briesch, C. Hanna, and J. Petke, An analysis of the automatic bug fixing performance of chatgpt, in 2023 IEEE/ACM International Workshop on Automated Program Repair (APR) . Los Alamitos, CA, USA: IEEE Computer Society, may 2023, pp. 23 30. [Online]. Available: https://doi.ieeecomputersociety.org/10. [37] A. Radford and K. Narasimhan, Improving language understanding by generative pre-training, 2018. [Online]. Available: https://api. [38] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. GONG, M. Zhou, N. Duan, N.',\n",
       "  'chunk_id': 71},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. GONG, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. LIU, CodeXGLUE: A machine learning benchmark dataset for code understanding and generation, inThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1) , 2021. [Online]. Available: [39] J. Lanchantin et al. , Learning to reason and memorize with self-notes, arXiv preprint arXiv:2305.00833 , 2023. [40] B. Peng et al. , Check your facts and try again: Improving large language models with external knowledge and automated feedback, arXiv preprint arXiv:2302.12813 , 2023. [41] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, Retrieval augmentation reduces hallucination in conversation, in Findings of the Association for Computational Linguistics: EMNLP 2021 , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3784',\n",
       "  'chunk_id': 72},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3784 3803. [Online]. Available: https: [42] A. Madaan et al. , Self-refine: Iterative refinement with self-feedback, arXiv preprint arXiv:2303.17651 , 2023. [43] C. Malaviya et al. , Quest: A retrieval dataset of entity-seeking queries with implicit set operations, arXiv preprint arXiv:2305.11694 , 2023. [44] P. Manakul et al. , Selfcheckgpt: Zero-resource black-box hallucina- tion detection for generative large language models, arXiv preprint [45] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in North American Chapter of the Association for Computational Linguistics , 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID: [46] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, CodeBERT: A pre-trained model for programming and natural',\n",
       "  'chunk_id': 73},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'B. Qin, T. Liu, D. Jiang, and M. Zhou, CodeBERT: A pre-trained model for programming and natural languages, in Findings of the Association for Computational Linguistics: EMNLP 2020 , T. Cohn, Y . He, and Y . Liu, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 1536 1547. [Online]. Available: https://aclanthology.org/2020.findings-emnlp.139[47] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, J. Yin, D. Jiang, and M. Zhou, Graphcodebert: Pre-training code representations with data flow, ArXiv , vol. abs/2009.08366, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID: [48] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, Unified pre-training for program understanding and generation, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani- Tur, I. Beltagy, S. Bethard, R. Cotterell, T.',\n",
       "  'chunk_id': 74},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani- Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y . Zhou, Eds. Online: Association for Computational Linguistics, Jun. 2021, pp. 2655 2668. [Online]. Available: https://aclanthology.org/2021. [49] C. Raffel, N. M. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, J. Mach. Learn. Res. , vol. 21, pp. 140:1 140:67, 2019. [Online]. Available: [50] L. A. Goodman, Snowball sampling, Annals of Mathematical Statis- tics, vol. 32, no. 1, pp. 148 170, 1961. [51] G. Rodriguez-Perez, G. Robles, and J. M. Gonzalez-Barahona, How much time did it take to notify a bug? two case studies: Elasticsearch and nova, in 2017 IEEE/ACM 8th Workshop on Emerging Trends in Software Metrics (WETSoM) , 2017, pp. 29 35. [52] G. Rodr guez-P erez, Gregorio, Robles, A. Serebrenik, Andy, Zaidman, D. M. Germ an, Jesus, and J. M.',\n",
       "  'chunk_id': 75},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Rodr guez-P erez, Gregorio, Robles, A. Serebrenik, Andy, Zaidman, D. M. Germ an, Jesus, and J. M. Gonzalez-Barahona, How bugs are born: a model to identify how bugs are introduced in software components, Empirical Software Engineering , vol. 25, pp. 1294 1340, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID: [53] T. Zhang, D. Han, V . Vinayakarao, I. C. Irsan, B. Xu, F. Thung, D. Lo, and L. Jiang, Duplicate bug report detection: How far are we? ACM Trans. Softw. Eng. Methodol. , vol. 32, no. 4, may 2023. [Online]. [54] Z. Rasool, S. Barnett, S. Kurniawan, S. Balugo, R. Vasa, C. Chesser, and A. Bahar-Fuchs, Evaluating llms on document-based qa: Exact answer selection and numerical extraction using cogtale dataset, ArXiv , vol. abs/2311.07878, 2023. [Online]. Available: [55] C. Wang, X. Liu, Y . Yue, X. Tang, T. Zhang, C. Jiayang, Y . Yao, W. Gao, X. Hu, Z. Qi, Y . Wang, L. Yang, J. Wang, X. Xie, Z. Zhang, and Y . Zhang, Survey on factuality in large language models:',\n",
       "  'chunk_id': 76},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'L. Yang, J. Wang, X. Xie, Z. Zhang, and Y . Zhang, Survey on factuality in large language models: Knowledge, retrieval and domain-specificity, ArXiv , vol. abs/2310.07521, 2023. [Online]. Available: https://api.semanticscholar. [56] M. P. Polak and D. Morgan, Extracting accurate materials data from research papers with conversational language models and prompt engineering, Nature Communications , vol. 15, 2023. [Online]. [57] L. Basyal and M. Sanghvi, Text summarization using large language models: A comparative study of mpt-7b-instruct, falcon-7b-instruct, and openai chat-gpt models, arXiv preprint arXiv:2310.10449 , October 2023. [Online]. Available: https://arxiv.org/abs/2310.10449 [58] LangChain, SQLDatabaseChain, Webpage. [Online]. [59] Hugging Face, all-MiniLM-L6-v2, Webpage. [Online]. Available: [60] F. Rahutomo, T. Kitasuka, and M. Aritsugi, Semantic cosine similarity, 2012. [Online]. Available: https://api.semanticscholar.org/CorpusID:',\n",
       "  'chunk_id': 77},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'Tool-Integrated Reasoning for Bengali Mathematical Department of Computer Science Bangladesh University of Engineering and Technology Department of Computer Science Bangladesh University of Engineering and Technology Abstract In this research paper, we present an innovative approach tailored to solving mathematical problems in Bengali, developed for the DL Sprint 3.0 - BUET CSE Fest 2024 Competition. Our methodology harnesses the power of advanced deep learning models, notably the Qwen 2.5 series, with iter- ative improvements made through prompt engineering, model quantization, and Tool Integrated Reasoning (TIR) to handle complex calculations. Initially, we explored various model archi- tectures, such as fine-tuned Mistral and quantized Qwen models, progressively refining them through translation techniques, RAG (Retrieval-Augmented Generation), and custom dataset curation. Through manual hyperparameter tuning, we optimized param- eters like temperature and top-p to improve model',\n",
       "  'chunk_id': 0},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'manual hyperparameter tuning, we optimized param- eters like temperature and top-p to improve model adaptability and response accuracy. Additionally, the removal of RAG and careful parameter adjustments further contributed to our final models robustness. Our approach demonstrates the potential of advanced NLP techniques in effectively interpreting and solving Index Terms Keywords: Bengali Mathematical Problem Solv- ing, Qwen 2.5, Mathematical Reasoning in Bengali, Tool Integrated Reasoning (TIR), Retrieval-Augmented Generation (RAG), Model Quantization, Prompt Engineering, Self-Consistent The ability to understand and solve mathematical problems is a foundational skill for AI, essential for advancements across fields like science, engineering, and finance. However, while AI models have made strides in various languages, they still face significant challenges when tackling mathematical reasoning in low-resource languages, such as Bengali. This gap becomes particularly evident in tasks',\n",
       "  'chunk_id': 1},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'in low-resource languages, such as Bengali. This gap becomes particularly evident in tasks involving complex problem-solving and precise calculations. To address this, the DL Sprint 3.0 - BUET CSE Fest 2024 Competition [1] intro- duced the unique challenge of building an AI model capable of solving mathematical problems in Bengali, targeting issues akin to those in the Bengali Math Olympiad. This competition not only tests participants technical skills but also aims to push the boundaries of AIs adaptability and performance in Bengali. Our work contributes to this pioneering effort, focusing on enhancing AIs mathematical reasoning in Bengali through advanced NLP and deep learning techniques. Weexplore and iteratively refine state-of-the-art models, such as the Qwen series, alongside strategies like prompt engineering, Tool Integrated Reasoning (TIR), and manual hyperparameter tuning to achieve robust problem-solving capabilities. By contributing to this research, we aim to advance AIs',\n",
       "  'chunk_id': 2},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'robust problem-solving capabilities. By contributing to this research, we aim to advance AIs reach into Bengali language processing, ultimately creating models that can assist students, educators, and researchers in tackling complex problems with precision and reliability. For this task, we aimed to select models capable of effi- ciently solving mathematical problems in Bengali, considering both performance and computational efficiency. Our initial approach involved using a fine-tuned Mistral 7B model [2], which, despite being a strong general-purpose model, did not deliver the desired accuracy for mathematical reasoning tasks. This led us to explore other models better suited for handling mathematical challenges, particularly in a low-resource lan- guage like Bengali. The Qwen series emerged as the most promising option due to its strong performance on mathemat- ical reasoning benchmarks. The Qwen-32B-Instruct model [3], with impressive scores on the MATH benchmark (83.1) and GSM8K',\n",
       "  'chunk_id': 3},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'The Qwen-32B-Instruct model [3], with impressive scores on the MATH benchmark (83.1) and GSM8K benchmark (95.9) [4], was especially appealing for its capability in solving high-level mathematical problems. Given its robust performance, we chose to focus on the Qwen-2.5 series, including the 7B, 14B, and 32B models, each offering different trade-offs between accuracy and computational de- mands. We initially fine-tuned the Qwen-14B-Instruct model for a single epoch, which, though promising, did not provide sufficient improvements in performance. To enhance model ef- ficiency, we implemented VLLM (Variable-Length Language Model) for faster inference, which allowed us to speed up the testing process while maintaining model accuracy. Addition- ally, we employed model quantization techniques to reduce memory requirements, making the models more practical for large-scale inference tasks. To further refine performance, we incorporated Tool Integrated Reasoning (TIR), which enabled the model',\n",
       "  'chunk_id': 4},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'refine performance, we incorporated Tool Integrated Reasoning (TIR), which enabled the model to perform complex calculations using Python. This 8 Nov 2024method improved the models ability to handle mathematical operations effectively. Moreover, manual hyperparameter tun- ing of parameters like temperature and top-p helped optimize the models response accuracy and adaptability. In summary, after exploring various models, we selected the Qwen series, particularly the Qwen-32B-Instruct model, for its exceptional performance in mathematical reasoning. Combined with tech- niques like VLLM, TIR, and hyperparameter optimization, we were able to enhance the models ability to solve mathematical problems in Bengali effectively. In this project, preprocessing was crucial to handling Ben- gali mathematical problems and enhancing the models ability to solve them accurately. Given the complexity of understand- ing Bengali text directly in the initial stage, we leveraged the Qwen-32B-Instruct model',\n",
       "  'chunk_id': 5},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'ing Bengali text directly in the initial stage, we leveraged the Qwen-32B-Instruct model to translate Bengali mathematical questions into English for improved processing. This approach enhanced the performance of our model significantly. Prompt tuning played a key role in optimizing the per- formance of our model as we experimented with various reasoning techniques, including Chain of Thought (COT), Tool Integrated Reasoning (TIR), RAG, self-consistent TIR, and self-consistent COT. We initially designed prompts to guide the model through step-by-step reasoning for COT. For TIR, we adapted the prompts to instruct the model to perform calculations using Python tools for more complex problems. With Self-COT and Self-TIR, the prompts were modified to encourage the model to generate multiple reasoning paths and select the most consistent solution. we experimented with Retrieval-Augmented Generation (RAG). The RAG approach was implemented to provide context and improve the quality of',\n",
       "  'chunk_id': 6},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'Generation (RAG). The RAG approach was implemented to provide context and improve the quality of answers in both Bengali and translated English questions. However , after experimenting with Retrieval-Augmented Generation (RAG), we decided to discontinue its use due to its poor performance, as it did not significantly improve the models accuracy compared to other approaches.Additionally, we fine-tuned hyperparameters such as temperature and topp to control the diversity and confidence of the models responses. Lower temperatures were used for more deterministic answers, while higher values promoted creativity.Adjusting topp helped the model select the most plausible solutions.These prompt tuning and hyperparameter adjustments, particularly with TIR and Self-TIR, significantly improved the models ability to solve complex Bengali math problems. In this section, we present the performance of our deep learning model using various approaches. The model was evaluated on the public leaderboard',\n",
       "  'chunk_id': 7},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'our deep learning model using various approaches. The model was evaluated on the public leaderboard of the DL Sprint 3.0, with the baseline score being 28 out of 100. We tested different configurations, including using various versions of the Qwen model, translation, retrieval-augmented generation (RAG), and Tool Integrated Reasoning (TIR).Translation RAG TIR Self-Consistency Score No No No No 49 Yes(Qwen2.5-14B-Instruct) No No No 61 No Yes No Yes 48 Yes(Qwen2.5-14B-Instruct) Yes No Yes 65 Yes(Qwen2.5-32B-Instruct) Yes No Yes 70 No No No Yes 66 Yes(Qwen2.5-32B-Instruct) No No Yes 73 TABLE I: Performance of Qwen2.5-32B-Instruct with Differ- Temperature Top p Number of Candidates Inference Time(s) Score 0.2 0.9 4 7844.8 68 0.35 0.775 4 7327.0 72 0.4 0.8 4 7391.5 77 0.4 0.8 10 19753.3 77 0.3 0.75 4 7579.1 76 0.3 0.75 5 9721.5 74 0.3 0.75 3 5436.7 71 0.7 0.8 4 7567.7 70 TABLE II: Hyperparameters and performance of the model The experiment began with a baseline score of 28, and the',\n",
       "  'chunk_id': 8},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'and performance of the model The experiment began with a baseline score of 28, and the fine-tuned Mistral 7B model scored only 15, indicating the need for a more powerful model architecture and translation capabilities. Scaling to larger Qwen 2.5 models, such as the 7B and 14B-Instruct, improved performance to 40 points but still showed limitations with certain problem types. The quantized Qwen 2.5-32B-Instruct model performed better, reaching a score of 49 without translation, benefiting from its larger Introducing translation using the 14B-Instruct model boosted the score to 61, allowing the model to leverage pre- existing English knowledge. Further improvement occurred when translation was done using the 32B model. Incorporating Retrieval-Augmented Generation (RAG) with translated En- glish questions raised the score to 70, showing that additional context from external datasets enhanced model performance. However, removing RAG led to a score of 73, suggesting that it may have',\n",
       "  'chunk_id': 9},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'enhanced model performance. However, removing RAG led to a score of 73, suggesting that it may have introduced noise, or the larger model could handle The introduction of Tool Integrated Reasoning (TIR), where Python was used for complex calculations, brought the score to 76. This step significantly enhanced the models efficiency and accuracy, especially with Bengali questions, which saw a notable improvement. The final model, optimized with prompt engineering and hyperparameter tuning, achieved a score of 77, marking the culmination of the models refinement in handling diverse problem types.Model Translation TIR Self-Consistency Score Deepseek-math-7b-Instruct(Baseline) No No Yes 28 mCot(Fine-tuned Mistral 7B) No No No 15 Qwen2.5-14B-Instruct No No No 40 Qwen2.5-Math7B-Instruct No Yes No 39 Yes Yes No 48 NuminaMath-7b-TIR(Finetuned Deepseek-math-7b) Yes(Qwen2.5-14B-Instruct) Yes Yes 66 Yes(Qwen2.5-32B-Instruct) Yes Yes 68 TABLE III: Performance of other tried models with Different',\n",
       "  'chunk_id': 10},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': '66 Yes(Qwen2.5-32B-Instruct) Yes Yes 68 TABLE III: Performance of other tried models with Different Configurations Overall pipeline of our solution Fig. 1: Iterative pipeline Throughout the development of our AI model for solving Bengali mathematical problems, we encountered several key Bengali questions presented without translation consistently scored lower than their translated counterparts. Translating questions into English improved model understanding andproblem-solving, leveraging the models richer pre-trained B. Model Size for Translation Using the Qwen 2.5-32B-Instruct model for translation yielded better results compared to the Qwen 2.5-14B-Instruct model. The larger model demonstrated superior language comprehension, which improved translation quality and con- tributed to overall score improvements.C. RAG Limitations Initial experiments with Retrieval-Augmented Generation (RAG) aimed to enhance the models responses by adding contextual information. However, RAG often',\n",
       "  'chunk_id': 11},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': '(RAG) aimed to enhance the models responses by adding contextual information. However, RAG often introduced noise, leading to a decrease in performance. Removing RAG sub- sequently improved scores, indicating that the inherent ca- pabilities of the larger Qwen models were sufficient without additional retrieved context for this task. D. Scarcity of Bengali Datasets The limited availability of high-quality Bengali mathemati- cal datasets restricted the models exposure to diverse problem E. Inference Optimization with VLLM Integrating Variable-Length Language Modeling (VLLM) enabled faster inference, which was crucial for handling the computational demands of larger models. This optimization helped streamline the testing process within our resource F . Prompt Sensitivity The model exhibited a high sensitivity to prompt wording, with minor adjustments in phrasing significantly affecting output quality and accuracy. Effective prompt engineering became essential to guide the model toward',\n",
       "  'chunk_id': 12},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'quality and accuracy. Effective prompt engineering became essential to guide the model toward optimal solutions, underscoring the importance of precision in prompt construc- G. Quantization for Resource Constraints Due to Kaggles limited GPU memory, we had to quantize the larger Qwen models to make them feasible for testing. This process reduced memory requirements, allowing us to utilize the larger 32B model in a resource-limited environment without compromising performance excessively. H. Fine-Tuning Limitations for Larger Models While fine-tuning smaller Qwen models was manageable, memory constraints in Kaggle prevented us from fine-tuning the Qwen 2.5-32B model. This limitation hindered further customization of the 32B model, restricting our ability to fine- tune it for specific Bengali problem-solving tasks. V. C ONCLUSION AND FUTURE WORK This paper presented a novel approach to solving Bengali mathematical problems by leveraging the Qwen 2.5 series models and optimizing through',\n",
       "  'chunk_id': 13},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'Bengali mathematical problems by leveraging the Qwen 2.5 series models and optimizing through prompt engineering, trans- lation, and Tool Integrated Reasoning (TIR). Our methods demonstrated the efficacy of advanced NLP techniques in mathematical problem-solving, particularly for low-resource languages like Bengali. The Qwen 2.5-32B model, combined with techniques such as self-consistency and TIR, achieved significant improvements over baseline models, highlighting the models potential in handling complex reasoning tasks with minimal Bengali-specific data. Despite these achievements, challenges remain, especially regarding translation dependen- cies, limited Bengali datasets, and memory constraints forlarge models. These factors impacted both model performance and adaptability to a wider range of problem types. Our results also revealed the sensitivity of model outputs to prompt phrasing, underscoring the need for refined prompt engineering To build on this work, we propose the',\n",
       "  'chunk_id': 14},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'underscoring the need for refined prompt engineering To build on this work, we propose the following areas for Enhanced Bengali Data Collection: Increasing the availability and diversity of Bengali mathematical datasets will enable better model training and adaptability to complex problem types. Domain-Specific Fine-Tuning: With sufficient resources, fine-tuning the Qwen 2.5-32B model specifically on Ben- gali math problems could further improve accuracy and Optimized Prompt Engineering: Researching prompt optimization strategies for mathematical reasoning tasks, especially in low-resource languages, could make the model responses more consistent and accurate. Exploration of Lightweight Models: Investigating smaller, efficient models tailored for Bengali could bal- ance memory constraints and inference speed without By addressing these areas, we aim to enhance the utility of deep learning models in Bengali problem-solving, making ad- vanced educational tools more accessible to',\n",
       "  'chunk_id': 15},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'learning models in Bengali problem-solving, making ad- vanced educational tools more accessible to Bengali-speaking learners and educators. This research lays a foundation for further progress in low-resource language applications of AI in mathematical education and reasoning. [1] S. Sushmit, A. Rahman, A. Azad, A. Rahman, M. S. Hossain, N. Tahmid, S. Islam, F. A. Akash, M. Islam, A. Muhtasim, T. Tahsin, and T. T. Dipto, DL Sprint 3.0 Bengali AI Math Olympiad, Kaggle, 2024. [Online]. [2] H. Lai and M. Nissim, mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics , Bangkok, Thailand, Aug. 2024. Association for Computational Linguis- [3] Qwen Team, Qwen2.5: A Party of Foundation Models, Sept. 2024. [4] Qwen Team, Qwen2.5 Overview, Qwen2 Official Website. Available:',\n",
       "  'chunk_id': 16},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'ENHANCING CLUSTER RESILIENCE : LLM- AGENT BASED AUTONOMOUS INTELLIGENT CLUSTER DIAGNOSIS SYSTEM AND EVALUATION FRAMEWORK Honghao Shi1Longkai Cheng1Wenli Wu1Yuhang Wang1Xuan Liu1Shaokai Nie1Weixv Wang1 Xuebin Min1Chunlei Men1Yonghua Lin1 Recent advancements in Large Language Models (LLMs) and related technologies such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have enabled the creation of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have developed an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovations include a knowledge base tailored for cluster diagnostics, enhanced LLM algorithms, practical deployment strategies for agents, and a benchmark specifically designed for evaluating LLM capabilities in this domain. Through extensive experimentation across multiple dimensions, we have demonstrated',\n",
       "  'chunk_id': 0},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'in this domain. Through extensive experimentation across multiple dimensions, we have demonstrated the superiority of our system in addressing the challenges faced in cluster diagnostics, particularly in detecting and rectifying performance issues more efficiently and accurately than traditional methods. Recent advancements in Large Language Models (LLMs) and complementary technologies such as Retrieval- Augmented Generation (RAG) and Diagram of Thought (DoT) have paved the way for the development of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have created an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovative approach includes the establishment of a specialized knowledge base for cluster diagnostics, the enhancement of LLM algorithms to better suit the demands of the domain, practical deployment strategies for agents',\n",
       "  'chunk_id': 1},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'LLM algorithms to better suit the demands of the domain, practical deployment strategies for agents within real-world environments, and the development of a benchmark specifically tailored to evaluate LLM capabilities in the context of cluster diagnostics. These components collectively contribute to a robust framework that addresses the complexities inherent in managing AI clusters, particularly in scenarios involving performance degradation or other operational anomalies. Through rigorous experimentation, we have validated the effectiveness of our LLM-agent system across multiple dimensions. Our benchmark, which consists of 150 manually crafted advanced questions, serves *Equal contribution1Beijing Academy of Artificial Intelli- gence, Beijing, China. Correspondence to: Yonghua Lin <yh- >.as a comprehensive evaluation tool that highlights the performance differences between our enhanced LLM-agent and baseline open-source models. In practical applications, the LLM-agent demonstrates',\n",
       "  'chunk_id': 2},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'LLM-agent and baseline open-source models. In practical applications, the LLM-agent demonstrates its superior capability to identify and resolve performance issues more efficiently than traditional methods, reducing the troubleshooting time significantly. For instance, in a simulated scenario where one GPU was throttled to a much lower frequency, our system identified and resolved the issue within a matter of minutes, whereas conventional approaches would have taken a senior operations engineer nearly an hour to diagnose and rectify using pre-written automated detection Moreover, the LLM-agents ability to detect and initiate corrective actions even before the performance degrada- tion is noticed by human operators marks a significant advancement in proactive system maintenance. This capability not only mitigates immediate issues but also enhances the overall availability and reliability of the cluster by preemptively addressing potential faults. By leveraging the strengths of RAG and',\n",
       "  'chunk_id': 3},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'of the cluster by preemptively addressing potential faults. By leveraging the strengths of RAG and DoT, the LLM-agent can autonomously execute remediation measures, thereby freeing up engineering resources to focus on more complex and value-driven tasks. Our research underscores the transformative potential of combining AI-driven diagnostics with practical deployment strategies, setting the stage for a new era of intelligent cluster management solutions. 8 Nov 2024Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework 2.1 LLMs Alignment and Enhancement In recent years, generative artificial intelligence centered around large language models(LLMs) has seen rapid development, with powerful natural language generating capabilities demonstrated by proprietary models such as the GPT series(Achiam et al., 2023) and Gemini series(Team et al., 2023), as well as open-source models like Llama(Dubey et al., 2024) and Qwen(Yang et',\n",
       "  'chunk_id': 4},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'et al., 2023), as well as open-source models like Llama(Dubey et al., 2024) and Qwen(Yang et al., There are multiple approaches to enhancing the capabil- ities of LLMs across different stages such as training, inference, and deployment, as well as in areas like data, algorithms, and computational resources. In light of the achievements of autoregressive models like GPT- 2(decoder-only transformers)(Radford et al., 2019) and LLaMA(transformer++)(Touvron et al., 2023), enhancing the quality of the data has become a critical method for improving the efficacy of models during the pre-training process(Adler et al., 2024; Liu et al., 2024). For modern LLMs, there exists several training or fine- tuning works between pre-training and the deployment. ChatGPT(Ouyang et al., 2022) describes this process as Su- pervised Fine-Tuning (SFT), Reward Modeling (RM), and Reinforcement Learning with Human Feedback (RLHF), while LLaMA3.1(Dubey et al., 2024) integrates these into a continuous process',\n",
       "  'chunk_id': 5},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Feedback (RLHF), while LLaMA3.1(Dubey et al., 2024) integrates these into a continuous process known as Continue Training. Besides training, LLMs can leverage Retrieval-Augmented Generation (RAG)(Lewis et al., 2020) to utilize knowledge from data distributions that were not part of the training set. We can refer to the above content as the alignment and 2.2 AI-agent based Applications After the model parameters have been frozen, it is possible to enhance the inherent capabilities of the model through mechanisms such as chain-of-thought(CoT) reasoning(Wei et al., 2022), scaling test time(Snell et al., 2024), and com- bining CoT LLM and AI agents(Castelfranchi, 1998) as LLM-agent(Park et al., 2023). CoT is a prompting technique used to guide LLMs to generate intermediate reasoning steps before arriving at a final conclusion. There are extensions to classic CoT, such as Tree of Thought (ToT)(Yao et al., 2024) for tree- like backtracking, Graph of Thought (GoT)(Besta et al., 2024) for',\n",
       "  'chunk_id': 6},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': '(ToT)(Yao et al., 2024) for tree- like backtracking, Graph of Thought (GoT)(Besta et al., 2024) for graph-based reasoning, and Diagram of Thought (DoT)(Zhang et al., 2024) for a propose-critique-summarize approach based on topos theory. The development of CoT and the scaling of test-time are unified, with CoT applications always aiming to maintain optimal results with limited test-time or scaling test-time toachieve extraordinaire results(Snell et al., 2024). The CoT series technics are also one of the foundations for build- ing LLM-agents. LLM-agents can leverage LLMs as the processing core while integrating traditional AI-agent capa- bilities such as memory, planning, and execution, creating semi-autonomous software entities that are highly adaptive and capable(Xi et al., 2023). 2.3 Diagnosis and Repair for AI Clusters Constructing and utilizing LLM applications typically re- quire hardware infrastructure on a scale costing millions of or more dollars. Meta constructed the LLM',\n",
       "  'chunk_id': 7},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'hardware infrastructure on a scale costing millions of or more dollars. Meta constructed the LLM application core LLaMA 3.1 within 54 days, leveraging a cluster that included 16,000 GPUs(Dubey et al., 2024), with just the GPU costs amounting to over billion dollars. However, such complex and expensive systems face significant chal- lenges in terms of reliability and availability. During the 54-day training, the Meta cluster experienced 419 unex- pected interruptions, averaging one disruption every three hours. At such a frequency of interruptions, the cluster, from the operating system to the AI framework and distributed scheduling software, requires the ability to capture, identify, attribute, and repair exceptions to ensure successful and ef- ficient model training. Microsofts Superbench(Xiong et al., 2024) has systematically built a suite of standard test cases to comprehensively assess the availability of clusters. In terms of capture and repair, the Torch(Paszke et al., 2019)',\n",
       "  'chunk_id': 8},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'assess the availability of clusters. In terms of capture and repair, the Torch(Paszke et al., 2019) Elastic solution aims to enable automatic restarts of model training, while works such as FlashCheckpoint- ing in DLRover(Wang et al., 2023) focus on reducing the cost of checkpoint saving and loading during the automatic restart process. Building upon automatic restart capabili- ties, many works at the AI framework level have conducted research and practical implementations to enhance reliabil- ity and availability, particularly those featuring highly cus- tomized solutions based on Megatron(Shoeybi et al., 2019). ByteDances Megascale(Jiang et al., 2024) and Alibabas Pai-Megatron(Qian et al., 2024) both provide toolkits for cluster diagnostics, which are used to check the health of servers and networks, as well as to perform manual or auto- mated error identification and repair. With the advancement of AI technologies, researchers are beginning to explore the use of AI techniques to',\n",
       "  'chunk_id': 9},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'advancement of AI technologies, researchers are beginning to explore the use of AI techniques to address cluster diagnostic issues. Using big data techniques to ana- lyze log files was an typical approach to automating cluster diagnostics(Jung & Chung, 2021). However, such meth- ods primarily involve static or real-time analysis of files produced by the training process, which limits their attribu- tion capabilities and means they lack intelligent autonomy, relying instead on pre-written execution and planning pro- cedure.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework AI computing tasks: refers to programs or processes designed to achieve intelligence, such as training large language models, inference with large language models, world model inference, and LLM-agent inference. AI chips: processors suitable for or dedicated to performing AI computing tasks, such as NVIDIA GPUs, Intel Gaudi AI accelerators, and',\n",
       "  'chunk_id': 10},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'dedicated to performing AI computing tasks, such as NVIDIA GPUs, Intel Gaudi AI accelerators, and Google TPUs(Jouppi et al., 2017). AI servers: computers equipped with AI chips that are suitable for or specifically designed to perform AI computing tasks, such as the NVIDIA DGX H100. AI servers often have requirements beyond those of classic servers in terms of stability, availability, cooling, and power AI cluster: a distributed server cluster composed of two or more AI servers set up to accomplish a single target task, such as Metas cluster containing 16 thousand GPUs. Additionally, AI servers typically require RDMA or higher bandwidth interconnect protocals, such as InfiniBand RDMA(Shanley, 2003) and RDMA over Converged Ethernet(RoCE)(Guo et al., 2016), and do not usually adopt Cluster diagnosis: ensuring that AI computing tasks can run with normal performance on the AI cluster, promptly detecting task failures, identifying the points of failure, clarifying the reasons for failure,',\n",
       "  'chunk_id': 11},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'detecting task failures, identifying the points of failure, clarifying the reasons for failure, repairing the corresponding faults, and ensuring the overall availability of the AI cluster. We incorporate advanced techniques from the field of LLM alignment and enhancement to creatively develop a solution for building a cluster intelligent maintenance system based on LLM-agents. Figure 1 illustrates the overall process of The upper part of the figure represents the core component of solution: the LLM-agent. The LLM-agent consists of an agent program and an LLM. The LLM interprets the input information provided by the agent as external stimuli and task instructions, and responds appropriately. The agent then directly writes code or calls specific software interfaces based on the feedback from the LLM, thereby operating the cluster. For LLM itself, there are two main challenges. First, how does the LLM acquire domain-specific knowledge of cluster diagnostics, and furthermore, where does',\n",
       "  'chunk_id': 12},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'does the LLM acquire domain-specific knowledge of cluster diagnostics, and furthermore, where does this knowledge come from. Second, how can the LLM reason and plan? For the entire LLM-agent, ensuring that the LLMs inputs and outputs match with the actual operations performed by the agent controlling the cluster is another crucial aspect that needs to be addressed. Figure 1. Overview of the Intelligent Maintenance System Based In order to solve the above problems, we have introduced three innovations. First, we use 250 cluster failure records collected from GitHub as a starting point, and treat the cluster operation failure logs actually managed by the LLM-agent as a continuous source of data. We utilize RAG(Lewis et al., 2020) to enable the LLM to capture detailed knowledge corresponding to specific terms within the context. Figure 1 describes the alert, compute cluster, and storage sections, along with their communication with the LLM-agent, which outlines this process. Second, we',\n",
       "  'chunk_id': 13},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'along with their communication with the LLM-agent, which outlines this process. Second, we use DoT(Zhang et al., 2024) enables the model to effectively handle non-natural language information such as symbols, formulas, and code. Similar to vision-text multimodal models, we effectively leverage textual elements that go beyond the inherent meaning of natural language based on DoT. The planning algorithm section at the top of Figure 1 illustrates this innovation. Third, we use self-play technology(Snell et al., 2024) to enable the LLM to autonomously, also intelligently, devides long tasks or challenging reasoning objectives into multiple steps, self-assess the output of each step, and ultimately The lower part of Figure 1 forms the basis of our work. It includes a mature operations alarm troubleshooting and repair process, as well as several mature or advanced software tools. Based on related works, we have developed a unified, multi-level, multi-dimensional cluster diagnostic toolkit',\n",
       "  'chunk_id': 14},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'works, we have developed a unified, multi-level, multi-dimensional cluster diagnostic toolkit as Figure 2. This tool diagnoses the health status of the cluster fromEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Figure 2. Tools for LLM-agent to Diagnose AI Cluster both the supply side and the demand side simultaneously. The bottom part of Figure 2 lists the various components required to build an AI cluster, including the computing component, storage component, network component, and others. AI clusters following different technical routes provide similar capabilities, as shown in the middle part of Figure 2. We inspect all resource supply items affecting AI computing tasks to determine if their content is correct, if their performance is appropriate, and if they are stable. For example, for the feature of RDMA read/write between two GPUs across servers, our tool checks whether the read/write content is correct,',\n",
       "  'chunk_id': 15},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'between two GPUs across servers, our tool checks whether the read/write content is correct, whether the IOPS, bandwidth, latency, and other performance metrics are appropriate, and the stability under complex scenarios such as long-duration or multi-process read/writes. Most of these tools are improved versions of packages provided by chip, server, or operating system vendors. The top part of Figure 2 takes the demand side into consideration, evaluating the metric of concern for AI computing tasks with various characteristics. In summary, we have built an LLM-agent capable of retriev- ing and utilizing vast amounts of external information, with autonomous planning, learning, reasoning, and execution capabilities. This LLM-agent works alongside either custom-written tools or existing mature tools to perform early warning, troubleshooting, and repair tasks for the 4.2 Cluster Diagnosis Domain-specific Knowledge Our knowledge base consists of two sources. One part is logs, monitoring',\n",
       "  'chunk_id': 16},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Domain-specific Knowledge Our knowledge base consists of two sources. One part is logs, monitoring information, or program output content, come from pre-collected, cleaned, and organized GitHub data, carefully selected to address pain points in the cluster diagnostics and troubleshooting domain, incorporating knowledge from issues in the GitHub community, also come from operational data acquired after the initial deployment and operation of the LLM-agent. We call it Diagnosis Dataset. The second part is composed of symbolicreasoning. These reasoning structures use AI computation tasks and hardware specification information as input, and through a bottom-up modeling approach, predict the theoretical performance of the given AI computation tasks, thereby determining the correctness of the performance. We drew on effective practices from Alibabas experience in managing cluster startup operations(Xu et al., 2024) to build a database. We cleaned, organized, and structured the unstructured',\n",
       "  'chunk_id': 17},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'et al., 2024) to build a database. We cleaned, organized, and structured the unstructured data obtained from GitHub, ultimately forming an effective dataset. We collected over a thousand questions and feedback items from the GitHub issue section. Through automated processes and manual review, we filtered out over 200 entries with substantive knowledge content and well-structured Q&A formats. Each piece of organized data contains four fields: problemkey, rawtext, function, and The problemkey is a domain keyword identified either manually or based on openai o1. Rawtext refers to the original content of a website after simple formatting, stored as a long string containing the questions asked on the web page and the developers responses. The function is based on our cluster diagnosis toolkits and is manually correlated by cluster troubleshooting personnel. This part is used as annotation in the portion of the dataset that the model can perceive, it is not perceived by the model for the',\n",
       "  'chunk_id': 18},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'in the portion of the dataset that the model can perceive, it is not perceived by the model for the answers used in the benchmark evaluation part, and it serves as the starting point for knowledge acquisition after the LLM-agent is deployed. The final results are the causes of the faults extracted from the rawtext based on the developers answers. For an LLM capable of driving an agent to perform cluster diagnostics, we expect it to be able to determine the causes of faults based on real-time operational information from the cluster and to call existing tools or write tool code on-the-fly for cluster repairs, without relying on rawtext containing developer replies. We will demonstrate this capability in subsequent experiments. We use a series of progressive methods to model the correct performance of given AI computation tasks, and through the DoT, we convert this special modal data into tokens to feed into the model. In addition to cluster health check, we have included modules in the',\n",
       "  'chunk_id': 19},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'tokens to feed into the model. In addition to cluster health check, we have included modules in the toolkits to determine whether different AI computing tasks exhibit correct performance. These modules can, on one hand, be invoked by the agent to provide results to the LLM for analysis, and on the other hand, they can be called by the LLM to have the agent check We start modeling with the simplest task types. ConsideringEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework that existing AI clusters are composed of computing devices with the von Neumann architecture, AI computing tasks require the use of computing cores, memory, and I/O ports. It is worth noting that what AI computing tasks occupy are not narrowly defined CPU computing cores, main memory, or input/output ports, but rather in a broader sense, such as computing cores dedicated to matrix multiplication, HBM memory composed of multi-level caches, and',\n",
       "  'chunk_id': 20},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'computing cores dedicated to matrix multiplication, HBM memory composed of multi-level caches, and high-speed I/O ports formed by PCIe or RDMA protocols. To build a unified model, we use the concepts of equivalent computing power, equivalent memory bandwidth, and equivalent I/O We refer to computational tasks that occupy or primarily oc- cupy one type of resource as single-resource computational tasks. We construct a single-variable computational task per- formance model and use experiments based on Khinchins law of large numbers to get the results. We assume that for a certain computational task T, the total amount of resource Rirequired is Mi. The hardware running this task can pro- videNiunits of resource Riper second. Assume that the single-variable task Txdepends only on resource R0. We determine M0based on the mathematical formula used for the tasks computation. For N0, we consider it a random variable. Through a large number of repeated experiments after warm-up, we ensure that',\n",
       "  'chunk_id': 21},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'it a random variable. Through a large number of repeated experiments after warm-up, we ensure that the difference between the measured results and the expected value of the random vari- able approaches zero. We define performance as the number of times a specific task can be executed per unit time. For the aforementioned task Tx, we predict its performance to For non-single-variable tasks, we focus on modeling whether the different resources they depend on can oper- ate in parallel. A widely used method in multivariate task modeling is the roofline model(Ofenbeck et al., 2014). The roofline model introduces a new variable: task characteristic CT. The Roofline model introduces a new variable: the task characteristic CT. Consider a task Txdepends on two resources R0andR1, the effective utilization of resource R0is plotted on the Y-axis, and the ratio of effective utiliza- tion of resource R0to resource R1is plotted on the X-axis. By changing CT, a scatter plot can be drawn, forming a',\n",
       "  'chunk_id': 22},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'R0to resource R1is plotted on the X-axis. By changing CT, a scatter plot can be drawn, forming a shape like a roofline. The Roofline model is equivalent to modeling the performance of multivariable tasks under fully parallel scenarios, which does not align with real-world conditions. Additionally, in the context of existing LLM performance modeling, changes in CTare not about varia- tions in the input size of a single task but about the changing proportions of two different primary resource-consuming tasks within the total task. Therefore, we use the proportion of different subtasks as variables to model multivariable tasks for the three main resources provided by AI clusters: equivalent floating- point computing power for matrix multiplication, memory read/write bandwidth, and I/O port bandwidth. The results 0% computeA B CFigure 3. Multi-variable Task Performance Modeling. A shows compute-memory, B shows interconnect-memory, C shows at figure 3 show that computing and memory are in',\n",
       "  'chunk_id': 23},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'B shows interconnect-memory, C shows at figure 3 show that computing and memory are in domains that are completely non-parallelizable, whereas computing, memory, and I/O ports can approach full parallelization. This conclusion and related figures have been compiled and placed in the RAG documentation. 4.3 Create LLM-agent with RAG-DoT-Selfplay 4.3.1 Using RAG to Build an LLM That Can Utilize RAG integrates two core components: retrieval and genera- tion. The retrieval module is responsible for finding context- relevant information from an external knowledge base, a process that typically involves indexing large volumes of documents to quickly locate the most pertinent segments. The retrieved information is then passed to the generation module as additional input. The generation module builds upon a pre-trained language model, leveraging the retrieved context to enhance its generation capabilities, thereby pro- ducing responses that are more accurate and better aligned Considering',\n",
       "  'chunk_id': 24},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'capabilities, thereby pro- ducing responses that are more accurate and better aligned Considering other similar technologies, SFT requires sub- stantial computing resources and may diminish the models inherent generalization capabilities. In-context learning consumes context length and inference time, making it unsuitable for importing datasets with millions of entries. RAG can acquire relevant knowledge during inference with minimal resources and inference time, without altering the weights of the model itself. 4.3.2 Using DoT to Build an Agent That Can Reason and DoT(Diagram of Thoughts)(Zhang et al., 2024) models iter- ative reasoning in LLMs as constructing a Directed Acyclic Graph (DAG) within a single model. The DAG consists of nodes representing propositions, critiques, refinements, and verifications, with edges indicating the logical relationships or dependencies between them. We use XML to handle mul- timodal special symbol data and perform reasoning based Based on the',\n",
       "  'chunk_id': 25},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'We use XML to handle mul- timodal special symbol data and perform reasoning based Based on the principles of DoT, we use XML tags to sep-Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework arate different types of text, including plain text, special symbols, code, formulas, and inference rules. Thanks to the rope positional encoding adopted by LLama3.1, the model can accurately capture the content within XML pairs. Based on the reasoning graph, our experiments confirmed that this application allows the LLM to correctly reason according to specific rules, achieving the capability to support the agent in completing cluster fault attribution and repair tasks. This significantly exceeds the capabilities of pre-trained or 4.3.3 Using Selfplay Techniques to Construct a With the help of RAG and DoT, the LLM can utilize information from outside the training set as well as abstract symbolic reasoning information. However, this',\n",
       "  'chunk_id': 26},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'from outside the training set as well as abstract symbolic reasoning information. However, this still has limitations for an agent designed for intelligent cluster diagnostics. We permit the LLM to generate content over a longer duration. The quality of solutions to challenging problems can be enhanced through multiple rounds of planned selfplay or spontaneous self-questioning and answering by the agent. Spontaneous self-questioning and answering is applied in DoT reasoning. On the planned selfplay process, we transform the complex problem of cluster fault attribution into a three-round process. In the first round, the agent, based on error logs passed from the cluster, prompts the LLM to identify potential keywords from the error items and corresponding solutions from the knowledge base, performing information extraction and RAG. In the second round, the LLM evaluates its own answers, making corrections or accepting them directly, then proceeds to write or call appropriate tools for',\n",
       "  'chunk_id': 27},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'making corrections or accepting them directly, then proceeds to write or call appropriate tools for the Agent to execute. In the final round, the LLM makes an accurate attribution judgment based on the results of the agents interaction with the actual cluster. Compared to existing selfplay work focused on the text side, we integrate it with the agent, granting it the permissions to operate machines and interact with the environment, fully simulating the capabilities of a human engineer to solve problems. We conducted a three-phase experiment to demonstrate the advanced nature of the proposed LLM-agent in the field of cluster intelligent diagnostics. The first phase involves creating a dataset and benchmark for the field of cluster intelligent diagnostics. First, we define the statistical characteristics of the external data knowledge base and introduce the process of generating an evaluation benchmark from this knowledge base. Next, we describe the features of this benchmark and',\n",
       "  'chunk_id': 28},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'evaluation benchmark from this knowledge base. Next, we describe the features of this benchmark and explain its advanced naturein the field of cluster intelligent diagnostics. Throughout this process, we emphasize fairness and impartiality, strictly distinguishing between the parts of the model that can be perceived and the scoring portions of the evaluation. We further elaborate on the benchmark using the results of the mainstream open-source model LLaMA3.1-70B. The second phase involves evaluating the innovative aspects of the three models we proposed RAG, DoT, and selfplay using the aforementioned benchmark for comparative assessment. The experiments in the second phase are aimed at demonstrating the advanced nature of our proposed models in the field of cluster intelligent In the third phase, we expose the LLM-agent to both the training and testing sets in the benchmark, allowing it to operate in its most complete form to address real-world problems encountered in production',\n",
       "  'chunk_id': 29},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'it to operate in its most complete form to address real-world problems encountered in production environments. We demonstrate the accuracy, efficiency, and autonomous intelligence of this solution through two typical cases. Specifically, we found that this solution can provide early warnings for AI clusters, further enhancing the availability Finally, we will conduct a qualitative analysis and discus- sion on the topics of correctness, safety, and reliability, which are at the forefront of the LLM and LLM-agent fields and have yet to be conclusively resolved, to demonstrate the series of work we have undertaken in these areas. 5.1 Statistics and Evaluation for Dataset and The materials provided to the LLM come from three sources. The first source is automatically collected Q&A data from relevant GitHub communities involved in AI cluster troubleshooting, such as the issue sections of repositories like Megatron, PAI, Deepspeed, and NCCL. This serves as our initial dataset. The data has',\n",
       "  'chunk_id': 30},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'like Megatron, PAI, Deepspeed, and NCCL. This serves as our initial dataset. The data has undergone two rounds of filtering, both automatic and manual, retaining parts with clear solutions and logical dialogues. The second source is the program output obtained by the LLM-agent using RAG+DoT technology on several AI clusters running tasks. These tasks are executed on clusters ranging from 4 to 100 A800 AI servers. The third part consists of special modal data such as symbolic representations and formulas processed using XML according to DoT logic, all of which are unified into the text modality. The total amount of pure text material is 200+ items compared with 1.2GB origin files. This also confirms that if more than 200 items consist of pure text content is fully pre-tokenized to serve as the context for LLM inference, it not only poses a significant challenge to the LLMs capability to handle long texts but also increases theEnhancing Cluster Resilience: LLM-agent Based Autonomous',\n",
       "  'chunk_id': 31},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'to handle long texts but also increases theEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework consumption of inference resources, thereby slowing down the execution speed of the LLM-agent. 5.1.2 Benchmarks Source and Statistics for Benchmark We divided the original dataset into two parts, approximately in a 20%-80% ratio. From the 80%, we manually compiled 150 questions to assess the LLMs capabilities in the field of cluster diagnostics. During comparative experiments, unless otherwise specified, we provide only 20% of the original data to all models. During case studies and practical applica- tions, we provide the entire original dataset to the deployed We designed three evaluation metrics. Metric A evaluates the large models information extraction capabilities, in- cluding extracting the cluster IP addresses and SSH port numbers from conversations, as well as the ability to deter- mine whether further execution is',\n",
       "  'chunk_id': 32},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'port numbers from conversations, as well as the ability to deter- mine whether further execution is needed, evaluated through string matching. The challenge here is to assess the models ability to follow instructions and extract information, since logs are derived from user conversations and may contain unnecessary commands that need to be ignored during the determination process. Metric B evaluates the large models code generation capabilities in the diagnostic domain, in- cluding the ability to generate prescribed code based on descriptions given in conversations, control the input and output of the code, and create unseen test cases, imple- mented in a manner similar to human-eval(Chen et al., 2021) but transferred to a real distributed cluster. Metric C eval- uates the large models information attribution capabilities in the diagnostic domain, including the ability to provide attribution based on users error logs and information. This is currently implemented through',\n",
       "  'chunk_id': 33},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'attribution based on users error logs and information. This is currently implemented through multiple-choice questions. 5.1.3 Evaluation of Benchmark on Standard We applied this benchmark to several of the most widely used open-source LLMs, namely LLaMA3.1-70B, nemotron-70B(Adler et al., 2024), mistral-120B(Jiang et al., 2023), and llama3.2 3B. The results is in table 1. Due to the lack of relevant data and information, as well as reasoning logic such as DoT, all models were only able to complete the first task, scoring zero on the second and third tasks. Since the results of llama3.2 3B did not meet the minimum requirements for building the LLM-agent, and the 120B model is difficult to infer on a single AI server, we opted for the better-performing and more widely used LLama3.1-70B out of the two 70B models as the basis for subsequent SFT (Supervised Fine-Tuning) and the application of RAG, DoT, and selfplay.Table 1. Benchmarks Results on Open-source LLMs 70Bno yes 0.8658 0.0 0.0',\n",
       "  'chunk_id': 34},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'of RAG, DoT, and selfplay.Table 1. Benchmarks Results on Open-source LLMs 70Bno yes 0.8658 0.0 0.0 70Bno yes 0.7315 0.0 0.0 120Bno no 0.7383 0.0 0.0 3Byes yes 0.047 0.0 0.0 Table 2. MMLU Benchmarks Results on LLama3.1 and Nemotron Model SFT or notMMLU We conduct two parts of experiments to comprehen- sively evaluate and compare the innovative effects of our work. In the first part, we use the mature and universal MMLU(Hendrycks et al., 2020) benchmark to evaluate the comprehensive ability of the model in basic text understand- ing after it has been enhanced by RAG, DoT, and self-play. In the second part, through ablation and comparison exper- iments, combined with the focus areas of the sub-items in our proposed benchmark, we quantitatively demonstrate the advantages of our three innovations. 5.2.2 General Capability Evaluation Based on MMLU Firstly, we aim to substantiate why SFT is not advisable in this domain. Although the LLM that supports the agent needs to possess extensive',\n",
       "  'chunk_id': 35},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'not advisable in this domain. Although the LLM that supports the agent needs to possess extensive knowledge in cluster diagnostics, performance modeling, and code writing, we discovered that when the LLM reaches a level where this knowledge can be effectively applied, it often lacks the fundamental interaction capabilities required to engage with the agent. We illustrate this point using the MMLU benchmark. We converted the knowledge repository into tokens compatible with the model and constructed an instruction dataset. We iterated through multiple training rounds until the model could respond correctly to instructions. We thenEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Table 3. Multi Comprehensive Benchmarks Results on LLMs 70Bno 0.6246 0.8691 0.8786 0.3720 0.8230 70Byes 0.6032 0.8649 0.8862 0.3680 0.8007 70Bno 0.6280 0.8620 0.8780 0.3680 0.8234 70Byes 0.6126 0.8653 0.8859 0.3580 0.7917 120Bno 0.6544 0.8788',\n",
       "  'chunk_id': 36},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': '0.6280 0.8620 0.8780 0.3680 0.8234 70Byes 0.6126 0.8653 0.8859 0.3580 0.7917 120Bno 0.6544 0.8788 0.9012 0.3980 0.8229 3Bno 0.4352 0.7428 0.7835 0.2800 0.6040 evaluated the SFT model that reached this state against the original open-source model using the Multi-Machine Learning Understanding (MMLU) benchmark. The results are presented in Table 2. From the above results, it can be seen that Supervised Fine-Tuning (SFT) leads to a decline in performance when evaluated using general assessment methods such as MMLU. Subsequently, in our proposed cluster diagnostics benchmark, we further observed adverse consequences of this performance decline in metric C. As a result, we ultimately decided not to use the SFT approach to construct To avoid the potential risks associated with relying solely on MMLU, we further selected three additional LLM benchmarks that are closely related to the problems we aim to solve in our domain or are entirely generalizable: Abstraction and Reasoning',\n",
       "  'chunk_id': 37},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'the problems we aim to solve in our domain or are entirely generalizable: Abstraction and Reasoning Challenge(ARC)(Peter, 2022), BoolQ(Clark et al., 2019), and OpenbookQA(Mihaylov et al., 2018). The results are presented in the table 3. The results of this set of experiments support the conclusions we drew from the MMLU benchmark. 5.2.3 Results of Our Benchmark Table 4 presents all of our experimental results. The second column of the table indicates whether there was cheating. We define experiments that do not participate fairly in the benchmark as cheating. While this is unfair for the benchmark portion, it is clearly meaningful for our core research objective: to build an LLM-agent system that can autonomously and intelligently perform cluster diagnostics and troubleshooting. When evaluating the benchmark section, the cheating items can be considered as ground These experimental results can illustrate several conclusions.Table 4. Benchmarks Results on Open-source LLMs(baselines)',\n",
       "  'chunk_id': 38},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'can illustrate several conclusions.Table 4. Benchmarks Results on Open-source LLMs(baselines) 70BNone None 0.8658 0.0 0.0 plan)None 0.8658 0.4615 0.6470 70BNone SFT 0.0 0.0 0.0 planSFT 0.0 0.9230 0.0 70BNone RAG 0.8658 0.0 0.0 planRAG 0.8658 0.4615 0.7059 70BNone None 0.7315 0.0 0.0 planNone 0.7315 0.4615 0.7059 120BNone None 0.7383 0.0 0.0 planNone 0.7383 0.7692 0.8235 3BNone None 0.047 0.0 0.0 planNone 0.047 0.2307 0.1176 First, we found that a pre-defined plan can help a naive LLM control the agent. However, this plan was specifically written based on the benchmark questions and cannot beEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework used in a production environment. Correspondingly, all experiments utilizing DoT technology and not cheating scored well on metrics B and C for evaluating the agent, although the scores were slightly lower than those achieved with preplanning. This indicates that our proposed',\n",
       "  'chunk_id': 39},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'scores were slightly lower than those achieved with preplanning. This indicates that our proposed knowl- edge processing approach based on DoT and self-play can be used to control cluster troubleshooting agents. Second, we found that SFT significantly improved the scores on metric B, which focuses on evaluating code writing or the invocation of diagnostic tools. However, as a trade-off, all models that underwent SFT, even with preplanning, were unable to control the agent properly, resulting in poor performance on metric C. Third, we found that the results based on LLama3.1-70B were not significantly different from those of Mistral-120B, which has nearly twice the number of parameters. Twice the number of parameters implies double or more inference costs (considering multi-GPU linearity), making it impractical. On the other hand, the 3B smaller model, even with preplanning in a cheating scenario, is still unable to handle the task of We proceeded with subsequent experiments and actual',\n",
       "  'chunk_id': 40},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'scenario, is still unable to handle the task of We proceeded with subsequent experiments and actual deployment using the LLM-agent enhanced with the whole dataset and all of our innovative methods. 5.3 Intelligent Early Warning and Troubleshooting: A To demonstrate the superiority of the LLM-agent system we have built in the context of intelligent cluster diagnostics, we can present a concrete example to illustrate how the system operates and how it is more efficient and accurate compared to traditional methods. In the production environment of AI clusters, abnormal events or interruptions are not the most challenging problems to resolve. Clear information about anomalies or interruptions can effectively guide senior engineers in diagnosing the causes of issues. Current research is also progressively integrating technologies such as automatic restarts and automatic scheduling into the procedures for handling anomalies or interruptions in AI computing tasks. However, once an AI',\n",
       "  'chunk_id': 41},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'the procedures for handling anomalies or interruptions in AI computing tasks. However, once an AI computing task exhibits slow performance, it becomes difficult to quickly identify the problem, and it is even harder to pinpoint the cause of the slowdown. Assume there is an AI training cluster composed of dozens of servers, where one of the servers suddenly experiences a performance drop. This could be due to various reasons, such as increased network latency, memory leaks, high CPU load, or insufficient storage space. Traditionally, administrators or engineers would check the log files of the cluster to manually identify possible issues. This would involve reviewing logs from different nodes, monitoring system metrics, attempting to reproduce the problem, andso on. This method is time-consuming and labor-intensive and may require multiple attempts to pinpoint the root cause. In our system, the LLM-agent automatically gathers relevant log information, performance metrics, and other',\n",
       "  'chunk_id': 42},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'the LLM-agent automatically gathers relevant log information, performance metrics, and other necessary data from the nodes of the cluster. Leveraging the LLM-agents capabilities assessed through the benchmark, the system extracts useful information from the collected data, such as cluster IP addresses, SSH ports, and other crit- ical diagnostic details. Using its diagnostic capabilities in code generation and information attribution, the LLM-agent identifies the root cause of the issue based on the collected data and information. This may include generating new test cases to validate hypotheses. Once the problem is identified, the LLM-agent generates corresponding remediation scripts and requests human review. After approval, the LLM-agent executes the remediation measures in the cluster. Following the execution of remediation measures, the system collects data again to assess the outcome, forming a closed loop of data, algorithm, and hardware to optimize future diagnostic We manually',\n",
       "  'chunk_id': 43},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'forming a closed loop of data, algorithm, and hardware to optimize future diagnostic We manually constructed a scenario. This scenario would lead to slow performance in AI model training tasks and has repeatedly occurred in the development environment. We simulated an extreme heat situation with HV AC failure, throttling the frequency of one of the dozens of GPUs to approximately 200 MHz, rather than the 1410 MHz that the A800 GPUs should operate at. Observing the actual logs shows that the speed of this AI computing task decreased to approximately one-third of its normal performance. Our LLM-system initially flagged the slow AI task through power consumption monitoring and performance modeling results, triggering an automatic alert. Following this, through three rounds of self-play, it recommended checking the GPU core frequencies, a suggestion that the agent then dispatched for execution across all GPUs. Based on the execution results, the LLM accurately pinpointed the GPU with the',\n",
       "  'chunk_id': 44},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'across all GPUs. Based on the execution results, the LLM accurately pinpointed the GPU with the low core frequency that we had specifically altered. The entire troubleshooting process took less than 10 minutes. In contrast, a senior operations engineer would typically need about one hour to correctly identify the problem and then use a pre-written automated detection software tool created by engineers to determine the specific GPU with the low-frequency fault. More importantly, our LLM-agent can identify the fault before algorithm engineers or operations engineers detect the slow-down phenomenon and automatically complete the repair. This achieves resolving the issue before the fault occurs, thereby enhancing the overall availability of the cluster.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework 5.4 Qualitative Analysis of Correctness, Safety, and Based on the existing research that is not yet fully mature, and in',\n",
       "  'chunk_id': 45},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'of Correctness, Safety, and Based on the existing research that is not yet fully mature, and in the context of this specific field of study, we provide reasonable definitions for correctness, safety, and reliability. In this study, we define correctness as whether the process and results of the LLM-agent executing tasks are correct. Compared to evaluating the output of the LLM, assessing the correctness of the LLM-agents actions is more challenging. An apparently incorrect operation process may produce the correct result, whereas seemingly perfect output at the textual level might lead to an erroneous result when executed. Since we focus on the field of cluster diagnostics with the actual output being the execution of procedures by the agent, we do not investigate the potential harmfulness or bias in the textual content generated by the LLM. Instead, we examine the ability of our LLM-agent to avoid performing harmful operations on the cluster when the information fed back to the agent',\n",
       "  'chunk_id': 46},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'to avoid performing harmful operations on the cluster when the information fed back to the agent changes, or even when malicious content is inserted by an attacker, such as deleting files, shutting down, overclocking, or modifying critical system configurations. Regarding reliability, we define it as the overall quality of fault handling by the LLM-agent compared to human engineers or expert human engineers. In addition to whether the attribution is correct, we also consider factors such as the time taken to complete typical fault handling, the resources consumed, and the ability to We incorporate the assessment of correctness into the benchmark evaluation. For the potential risks associated with the LLM-agent, we implement a whitelist plus human review approach. Initially, we ensure the safety of the existing toolkit, followed by creating a whitelist for the program interfaces included in the toolkit and conducting human reviews for the LLM-agents requests to execute self-authored',\n",
       "  'chunk_id': 47},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'in the toolkit and conducting human reviews for the LLM-agents requests to execute self-authored code. Finally, we observed that the LLM-agent can attribute faults with an average of fewer than three test cases across multiple rounds of self-play, which is more efficient than the twelve cases typically required by human experts. However, regarding communication abilities, the LLM-agent currently does not possess such capabilities. The qualitative analysis described above is mainly aimed at reducing the probability of harmful incidents. Quantitative analysis or a comprehensive model still necessitates further advancements in the field of 6 CONCLUSION AND DISCUSSION 6.1 Work Summary and Further Plan Based on our experience and research in the fields of cluster diagnostics, LLM enhancement, and LLM-agent construc-tion, we innovatively proposed a system solution utilizing LLM-agents to autonomously and intelligently perform clus- ter troubleshooting. In terms of LLM algorithms, we intro-',\n",
       "  'chunk_id': 48},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'and intelligently perform clus- ter troubleshooting. In terms of LLM algorithms, we intro- duced a benchmark consisting of 150 advanced problems manually crafted, demonstrating the performance differ- ences between our constructed LLM-agent and the original open-source LLMs under fair data conditions. In the realm of LLM-agent construction, we innovatively proposed inte- grating DoT reasoning mathematics and the ability to handle special symbols and formulas into the agent, enabling the LLM to operate machines at the software level and receive feedback. Ultimately, we applied our innovative achieve- ments to cluster diagnostics, exploring the potential in this field, and were pleasantly surprised to find that the LLM- agent systems, despite being in their extremely early stages, are already capable of handling repetitive and low-end tasks, thus freeing industry practitioners to tackle more challeng- ing and valuable problems. In the future, we will continue our work in four aspects.',\n",
       "  'chunk_id': 49},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'more challeng- ing and valuable problems. In the future, we will continue our work in four aspects. In terms of LLM algorithms, we will expand and upgrade the existing benchmark and build a more comprehensive and valuable metrics system. In the Agent field, we will further unlock the potential of DoT and make self-written code by the LLM gradually become the main execution body, re- ducing reliance on preset tools. At the system application level, we will form a closed loop of data, algorithm, and hardware, enriching the database with results from actual deployments. Finally, in terms of safety and reliability, we will continue to work with researchers in related fields to ensure and evaluate the safety and reliability of the agents. 6.2 Shortcomings and Limitations Our research still has shortcomings and limitations. In terms of shortcomings, our agent currently relies on a mechanism of human review to ensure safety, depends on pre-written tools for code, and relies on data sourced',\n",
       "  'chunk_id': 50},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'of human review to ensure safety, depends on pre-written tools for code, and relies on data sourced from GitHub as a starting point. An ideal LLM-agent system should form a self-sustained relationship with the AI cluster, maintaining In terms of limitations, our work depends on the LLM within the LLM-agent, but smaller models like llama3.2-3B cur- rently cannot support the capabilities of the agent. There- fore, our work can only be applied to data centers or large- scale distributed clusters and cannot be deployed in edge computing or personal computer scenarios. We need to continuously monitor the development of smaller models and explore the possibility of teaching the capabilities of the LLM-agent to smaller models in the form of DoT when appropriate.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman,',\n",
       "  'chunk_id': 51},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Co- hen, J., et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704 , 2024. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Pod- stawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. InProceedings of the AAAI Conference on Artificial In- telligence , volume 38, pp. 17682 17690, 2024. Castelfranchi, C. Modelling social action for ai agents. Artificial intelligence , 103(1-2):157 182, 1998. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint',\n",
       "  'chunk_id': 52},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint Guo, C., Wu, H., Deng, Z., Soni, G., Ye, J., Padhye, J., and Lipshteyn, M. Rdma over commodity ethernet at scale. InProceedings of the 2016 ACM SIGCOMM Conference , Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring mas- sive multitask language understanding. arXiv preprint Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint Jiang, Z., Lin, H., Zhong, Y ., Huang,',\n",
       "  'chunk_id': 53},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint Jiang, Z., Lin, H., Zhong, Y ., Huang, Q., Chen, Y ., Zhang, Z., Peng, Y ., Li, X., Xie, C., Nong, S., et al. {MegaScale }: Scaling large language model training to more than 10,000 {GPUs }. In 21st USENIX Symposium on Net- worked Systems Design and Implementation (NSDI 24) , pp. 745 760, 2024.Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual inter- national symposium on computer architecture , pp. 1 12, Jung, H. and Chung, K. Social mining-based clustering process for big-data integration. Journal of Ambient In- telligence and Humanized Computing , 12(1):589 600, Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K uttler, H., Lewis, M., Yih, W.-t., Rockt aschel, T., et al. Retrieval-augmented generation for knowledge-',\n",
       "  'chunk_id': 54},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'H., Lewis, M., Yih, W.-t., Rockt aschel, T., et al. Retrieval-augmented generation for knowledge- intensive nlp tasks. Advances in Neural Information Pro- cessing Systems , 33:9459 9474, 2020. Liu, Y ., Tao, S., Zhao, X., Zhu, M., Ma, W., Zhu, J., Su, C., Hou, Y ., Zhang, M., Zhang, M., et al. Coachlm: Automatic instruction revisions improve the data quality in llm instruction tuning. In 2024 IEEE 40th International Conference on Data Engineering (ICDE) , pp. 5184 5197. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint Ofenbeck, G., Steinmann, R., Caparros, V ., Spampinato, D. G., and P uschel, M. Applying the roofline model. In2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS) , pp. 76 85. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to',\n",
       "  'chunk_id': 55},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems , 35:27730 27744, 2022. Park, J. S., OBrien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology , pp. 1 22, 2023. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems , 32, 2019. Peter, E. Abstraction and reasoning challenge. 2022.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Qian, K., Xi, Y ., Cao, J., Gao, J., Xu, Y ., Guan, Y ., Fu, B., Shi, X., Zhu, F., Miao, R., et al.',\n",
       "  'chunk_id': 56},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Qian, K., Xi, Y ., Cao, J., Gao, J., Xu, Y ., Guan, Y ., Fu, B., Shi, X., Zhu, F., Miao, R., et al. Alibaba hpn: a data center network for large language model training. In Proceedings of the ACM SIGCOMM 2024 Conference , Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. Shanley, T. InfiniBand network architecture . Addison- Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multi- billion parameter language models using model paral- lelism. arXiv preprint arXiv:1909.08053 , 2019. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test- time compute optimally can be more effective than scal- ing model parameters. arXiv preprint arXiv:2408.03314 , Team, G., Anil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models.',\n",
       "  'chunk_id': 57},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971 , 2023. Wang, Q., Sang, B., Zhang, H., Tang, M., and Zhang, K. Dlrover: An elastic deep training extension with auto job resource recommendation. arXiv preprint Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824 24837, Xi, Z., Chen, W., Guo, X., He, W., Ding, Y ., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864 , 2023. Xiong, Y ., Jiang, Y .,',\n",
       "  'chunk_id': 58},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'model based agents: A survey. arXiv preprint arXiv:2309.07864 , 2023. Xiong, Y ., Jiang, Y ., Yang, Z., Qu, L., Zhao, G., Liu, S., Zhong, D., Pinzur, B., Zhang, J., Wang, Y ., et al. {SuperBench }: Improving cloud {AI}infrastructure reli- ability with proactive validation. In 2024 USENIX Annual Technical Conference (USENIX ATC 24) , pp. 835 850, Xu, Y ., Chen, Y ., Zhang, X., Lin, X., Hu, P., Ma, Y ., Lu, S., Du, W., Mao, Z., Zhai, E., et al. Cloudeval-yaml: Apractical benchmark for cloud configuration generation. Proceedings of Machine Learning and Systems , 6:173 Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y ., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems , 36, 2024. Zhang, Y ., Yuan, Y ., and Yao, A. C.-C. On',\n",
       "  'chunk_id': 59},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'in Neural Information Processing Systems , 36, 2024. Zhang, Y ., Yuan, Y ., and Yao, A. C.-C. On the diagram of thought. arXiv preprint arXiv:2409.10038 , 2024. APLEASE ADD SUPPLEMENTAL MATERIAL Put anything that you might normally include after the refer- ences as an appendix here, not in a separate supplementary file. Upload your final camera-ready as a single pdf, includ-',\n",
       "  'chunk_id': 60},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification Yohei Seki1, Hakusen Shu2, Ana s Lhuissier3, Hanwool Lee4, Juyeon Kang3,Min-Yuh Day5,Chung-Chi Chen6 1Institute of Library, Information, and Media Science, University of Tsukuba, Japan, 2College of Knowledge and Library Sciences, School of Informatics, University of Tsukuba, Japan, 33DS Outscale, France,4Shinhan Securities Co., Korea, 5Graduate Institute of Information Management, National Taipei University, Taiwan6AIST, Japan Promises made by politicians, corporate lead- ers, and public figures have a significant impact on public perception, trust, and institutional rep- utation. However, the complexity and volume of such commitments, coupled with difficul- ties in verifying their fulfillment, necessitate innovative methods for assessing their credi- bility. This paper introduces the concept of Promise Verification, a systematic approach in- volving steps such as promise identification, evidence assessment, and the',\n",
       "  'chunk_id': 0},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'systematic approach in- volving steps such as promise identification, evidence assessment, and the evaluation of tim- ing for verification. We propose the first mul- tilingual dataset, ML-Promise, which includes English, French, Chinese, Japanese, and Ko- rean, aimed at facilitating in-depth verification of promises, particularly in the context of En- vironmental, Social, and Governance (ESG) reports. Given the growing emphasis on corpo- rate environmental contributions, this dataset addresses the challenge of evaluating corpo- rate promises, especially in light of practices like greenwashing. Our findings also explore textual and image-based baselines, with promis- ing results from retrieval-augmented generation (RAG) approaches. This work aims to foster further discourse on the accountability of pub- lic commitments across multiple languages and In a world where promises shape perceptions and drive decisions, the integrity of commitments made by politicians, corporate leaders, and',\n",
       "  'chunk_id': 1},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'and drive decisions, the integrity of commitments made by politicians, corporate leaders, and public fig- ures must be scrutinized. These promises, ranging from environmental sustainability to social respon- sibility and governance ethics, significantly influ- ence the general publics and stakeholders trust, as well as government and corporate reputations. Yet, the complexity and abundance of such commit- ments, coupled with the challenge of verifying their fulfillment, create a pressing need for innovative approaches to assess their strength and verifiabil- ity. Recognizing the critical role of transparencyand accountability in todays society, we propose a groundbreaking task: Promise Verification. To perform promise verification, several steps are required, including (1) identifying the promise, (2) linking the promise with supporting evidence, (3) assessing the clarity of the promise-evidence pair, and (4) inferring the timing for verifying the promise. For example, after',\n",
       "  'chunk_id': 2},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'promise-evidence pair, and (4) inferring the timing for verifying the promise. For example, after identifying a promise, the availability of evidence to support that the speaker is taking action to fulfill the promise could serve as a coarse-grained evaluation of the promises quality. However, the clarity of the ev- idence may also affect the quality of the promise, which would be a fine-grained evaluation. Addi- tionally, whether the speaker provides a clear time- line for verifying the promise is an important crite- rion. For instance, we will achieve net zero carbon emissions within five years is a stronger promise than we will achieve net zero carbon emissions. Following this line of thought, this paper proposes the first multilingual dataset for in-depth promise verification, including Chinese, English, French, In recent years, increasing emphasis has been placed on companies environmental contributions, especially in addressing climate change, deforesta- tion, and compliance',\n",
       "  'chunk_id': 3},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'contributions, especially in addressing climate change, deforesta- tion, and compliance with labor conditions and gov- ernance, when evaluating their investment value. In the evolving landscape of ESG (environmental, social, and governance) criteria, the ability to accu- rately assess a companys promises and adherence to its ESG promises has become paramount. How- ever, unlike traditional financial statements, ESG reports still lack clear standards regarding corporate promises. This allows some companies to use mis- leading information to project an overly positive environmental image, a practice known as green- washing. As Gorovaia and Makrominas (2024) points out, companies involved in environmental misconduct tend to produce longer, more positive, and more frequent reports. We hypothesize that 7 Nov 2024Task Label English French Chinese Japanese Korean Promise IdentificationYes 84.5 80.5 40.2 74.9 77.5 No 15.5 19.5 59.8 25.1 22.5 Supporting EvidenceYes 20.1 71.6 20.1 66.4 75.6 No',\n",
       "  'chunk_id': 4},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': '80.5 40.2 74.9 77.5 No 15.5 19.5 59.8 25.1 22.5 Supporting EvidenceYes 20.1 71.6 20.1 66.4 75.6 No 79.9 28.4 79.9 33.6 24.4 Clarity of Promise-Evidence PairClear 53.3 56.6 64.7 61.2 94.8 Not Clear 42.9 41.9 35.3 34.7 5.2 Misleading 3.8 1.5 0.0 4.1 0.0 Timing for VerificationWithin 2 years 1.9 12.4 37.5 7.3 45.5 2-5 years 14.1 15.0 10.0 9.3 8.4 Longer than 5 years 9.0 21.6 15.0 18.7 17.5 Other 75.0 51.0 37.5 64.7 28.7 Table 1: Label distribution in each language. (%) such reports may lack substantive evidence, or the information presented may be irrelevant or ambigu- ous, leading to misinterpretation. To this end, the proposed dataset, ML-Promise, focuses on ESG reports released by corporations in five countries: the U.K., France, Taiwan, Japan, and Korea. In addition to exploring text-based baselines, we also provide pilot results on image-based experi- ments, as most reports are released in PDF format. Our experiment further shows that the retrieval- augmented generation (RAG)',\n",
       "  'chunk_id': 5},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'released in PDF format. Our experiment further shows that the retrieval- augmented generation (RAG) approach (Lewis et al., 2020) can help in some language cases. Al- though we do not find a silver bullet for all lan- guages and tasks, we hope the proposed dataset can open a new chapter in discussions on the respon- sibility of promises, especially those from public Recent studies have sought to improve the anal- ysis of ESG or sustainability reports for estimat- ing company values using contextual embedding approaches. For example, Gutierrez-Bustamante and Espinosa-Leal (2022) evaluated sustainability reports from publicly listed companies in Nordic countries using latent semantic analysis (LSA) and the global vectors for word representation (GloVe) model, enhancing document retrieval performance based on similarity. Garigliotti (2024) explored the integration of sustainable development goals (SDGs) into environmental impact assessments (EIAs) using a RAG framework powered by large',\n",
       "  'chunk_id': 6},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'goals (SDGs) into environmental impact assessments (EIAs) using a RAG framework powered by large language models (LLMs). Their work focused on two tasks: detecting SDG targets within EIA reports and identifying relevant textual evidence, specifically in European contexts. Hillebrand et al. (2023) introduced sustain.AI, a context-aware rec- ommender system designed to analyze sustainabil- ity reports in response to increasing corporate so-cial responsibility (CSR) regulations. The system, based on a BERT architecture, identified relevant sections of lengthy reports using global reporting initiative (GRI) indicators and demonstrated strong performance on datasets from German companies. Previous studies have a few shortcomings. First, most of them focus solely on reports from one country. Second, none of them attempt to analyze corporate promises, despite the abundance of sus- tainability reports. To address these problems, our study extends these works by focusing on multi- lingual',\n",
       "  'chunk_id': 7},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'reports. To address these problems, our study extends these works by focusing on multi- lingual companies from both European and Asian regions, including Taiwan, the UK, France, Japan, and Korea. With the proposed new task, we aim to highlight the importance of anti-greenwashing by evaluating corporate promises in ESG reports. We collect ESG reports from five countries: the UK, France, Taiwan, Japan, and Korea. The annotators are native speakers of the target language or are familiar with the language at the work level. The task designs are as follows when given a paragraph in the ESG reports. 1.Promise Identification : This is a boolean label (Yes/No) based on whether a promise exists. 2.Supporting Evidence : This is a boolean label (Yes/No) based on whether supporting evidence exists. 3.Clarity of the Promise-Evidence Pair : We designed three labels (Clear/Not Clear/Misleading) for this task, which should depend on the clarity of the given evidence in relation to the promise.',\n",
       "  'chunk_id': 8},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'for this task, which should depend on the clarity of the given evidence in relation to the promise. 4.Timing for Verification : Following the MSCI guide- lines and previous work (Tseng et al., 2023), we set timing labels (within 2 years/2-5 years/longer than 5 years/other) to indicate when readers/investors should return to verify the promise. Here, other denotes the promise has already been verified or doesnt have a specific timing to verify it.Approach Task English French Chinese Japanese Korean w/o RAGPromise Identification (PI) 0.842 0.816 0.521 0.670 0.849 Supporting Evidence (SE) 0.680 0.746 0.163 0.720 0.792 Clarity of Promise-Evidence Pair (CPEP) 0.411 0.443 0.569 0.450 0.897 Timing for Verification (TV) 0.636 0.523 0.317 0.632 0.406 w/ RAGPromise Identification 0.866 0.798 0.540 0.659 0.807 Supporting Evidence 0.757 0.732 0.503 0.850 0.774 Clarity of Promise-Evidence Pair 0.467 0.487 0.628 0.465 0.939 Timing for Verification 0.693 0.601 0.469 0.684 0.571 Table 2: Experimental',\n",
       "  'chunk_id': 9},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': '0.487 0.628 0.465 0.939 Timing for Verification 0.693 0.601 0.469 0.684 0.571 Table 2: Experimental Results (F1-Score). The best performance in each language is denoted in bold . Finally, we obtained 3,010 instances, i.e., 600 for each language and 10 additional instances in the Chinese dataset. The Cohens agreement (Co- hen, 1960; McHugh, 2012) for these tasks is ap- proximately 0.65-0.96, 0.71-0.88, 0.62-0.80, and 0.60-0.89, respectively. Table 1 presents the distri- bution of the proposed ML-Promise dataset. First, we observe that around 35-40% of the evidence is not clear in supporting the associated promises in four out of five languages. This highlights the necessity of the proposed task for evaluating the quality of the promise-evidence pairs from corpora- tions. Furthermore, about 4% of instances contain (potentially) misleading evidence in the English and Japanese datasets. It is crucial for corporations to re-examine this evidence, and it is also essen- tial for supervisory',\n",
       "  'chunk_id': 10},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'is crucial for corporations to re-examine this evidence, and it is also essen- tial for supervisory authorities to monitor these instances. Second, we noted that corporations in Taiwan and Korea tend to make more short-term promises (within 2 years), whereas corporations in the remaining countries tend to make longer-term promises. This finding shows the need for a multi- lingual comparison of ESG reports across different countries, as the narrative styles vary among them. RAG (Lewis et al., 2020) was introduced as a method to enhance LLMs by integrating external knowledge sources. This approach combines re- trieval mechanisms with generative models, pro- ducing more accurate and contextually relevant outputs. Yu et al. (2024) highlights the advantages of RAG systems, particularly their ability to extract domain-specific information. By incorporating ex- ternal retrieval processes, RAG enables generative models to access a broader, field-specific knowl- edge base, improving the',\n",
       "  'chunk_id': 11},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'RAG enables generative models to access a broader, field-specific knowl- edge base, improving the accuracy and relevance of responses. This capability is especially impor- tant for handling domain-specific queries, an areaRAG TaskChinese Korean Image-Based Text-Based Image-Based Text-Based w/oPI 0.530 0.521 0.837 0.849 SE 0.124 0.163 0.812 0.792 CPEP 0.510 0.569 0.922 0.897 TV 0.202 0.317 0.201 0.406 w/PI 0.580 0.540 0.843 0.807 SE 0.512 0.503 0.845 0.774 CPEP 0.618 0.628 0.893 0.939 TV 0.297 0.469 0.330 0.571 Table 3: Image-based experimental results. Bolded denotes the best performance in each language. Underlined denotes performance with RAG better than where existing LLMs often encounter difficulties. Fan et al. (2024) discusses training strategies for RAG, including independent, sequential, and joint methods, which can be tailored to optimize retrieval and generation for specific domains. For Chinese language applications, Wang et al. (2024b) empha- sizes the importance of',\n",
       "  'chunk_id': 12},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'domains. For Chinese language applications, Wang et al. (2024b) empha- sizes the importance of domain-specific corpora over general knowledge sources. Ardic et al. (2024) applied RAG to analyze sustainability reports from ten Turkish companies, focusing on ESG factors. The study found BM25 outperformed BERTurk in retrieving relevant sections, highlighting the effec- tiveness of advanced retrieval techniques. Following the findings of previous studies, we also explore and design the RAG approach for the proposed tasks. Specifically, when given a para- graph, we first retrieve the six most similar samples in the training set. We leveraged Multilingual E5 Text Embeddings (Wang et al., 2024a) to calcu- late the cosine similarity between target paragraphs and instances from the training set. Then, we pro- vide the top-six examples for the LLM to perform in-context learning (Dong et al., 2022). In our ex- periment, we use GPT-4o as the base LLM. In the experiment, we randomly select 200 in-',\n",
       "  'chunk_id': 13},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'In our ex- periment, we use GPT-4o as the base LLM. In the experiment, we randomly select 200 in- stances from each language as the test set, and the remaining instances are used for training. We use the F1 score to evaluate the performance of eachtask. Table 2 shows the performance of each task in each language. First, the performance of most tasks improves when adopting RAG. Specifically, for En- glish and Chinese, all tasks perform better when using RAG. Second, RAG enhances performance in estimating the clarity of the promise-evidence pair and inferring the timing for verification, regard- less of the language used. These results suggest the usefulness of RAG in these two novel tasks. Addi- tionally, the findings demonstrate the value of the proposed annotations. With the proposed dataset, the performance of fine-grained promise evaluation can be improved. Third, although the performance in promise identification and supporting evidence identification tasks may slightly decrease',\n",
       "  'chunk_id': 14},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'in promise identification and supporting evidence identification tasks may slightly decrease in French, Japanese, and Korean, the declines are minimal (less than 2% in most cases). These results suggest that the method for retrieving and suggesting sam- ples similar to the paragraph requires refinement for imbalanced boolean datasets. In future work, we will focus on improving the RAG approach by extracting balanced samples, particularly for minor We noticed a significant difference between Tai- wan/Korea reports and the reports from other coun- tries.1The reports from these two countries uti- lize a large number of graphs instead of textual descriptions. This observation raises the question of whether we could use multimodal LLMs to read PDF files directly instead of relying on extracted text. To explore this, we align the annotations with a PDF page and employ GPT-4o to reassess the tasks using an image as input. For RAG, we lever- aged E5-V Universal Embeddings (Jiang et al., 2024)',\n",
       "  'chunk_id': 15},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'using an image as input. For RAG, we lever- aged E5-V Universal Embeddings (Jiang et al., 2024) to calculate the cosine similarity between target pages and instances from the training set. Table 3 presents the performance. First, using GPT-4o with image input reduces performance in three out of four tasks in the Chinese dataset and in two out of four tasks in the Korean dataset. Sec- ond, RAG improves the performance of most tasks when using image input. Third, with RAG, the per- formance in promise identification and supporting evidence identification tasks improves with Chi- nese image input, and the performance of support- ing evidence identification improves with Korean image input. However, for estimating the clarity of 1We provide some examples in Appendix A.Input RAG Task ROUGE-L Table 4: Results of promise and evidence extraction. the promise-evidence pair and inferring the timing for verification, using text input with RAG remains superior. In summary, our experimental',\n",
       "  'chunk_id': 16},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'timing for verification, using text input with RAG remains superior. In summary, our experimental results sug- gest that image input should be used for PI and SE tasks, while text input is preferable for CPEP and TV tasks. Additionally, RAG performs well regardless of input type. 5.2 Promise and Evidence Extraction In the previous section, we explored the promise and evidence identification tasks. However, the task can also be formulated in an extractive man- ner. Instead of only outputting a yes or no, we can also ask models to extract the promise and evidence from the report. We provide additional annotations in the Chinese dataset and experiment in multimodal settings with and without RAG. The ROUGE-L (Lin, 2004) score is used to evaluate ex- traction performance. Table 4 presents the results. These results indicate that the best performance is achieved in the image-based setting with RAG for both promise and evidence extraction. This empha- sizes the importance of exploring',\n",
       "  'chunk_id': 17},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'with RAG for both promise and evidence extraction. This empha- sizes the importance of exploring multimodal input for ESG report understanding. In this paper, we introduce the concept of Promise Verification, a novel task aimed at evaluating the credibility and fulfillment of promises made by corporations, particularly in the context of ESG reports. We propose the first multilingual dataset, ML-Promise, to emphasize the importance of assessing corporate environmental and social promises. Our results demonstrate that RAG im- proves performance, while also showing the po- tential of multimodal approaches in promise ver- ification. Our annotations will be released under the CCBY-NC-SA 4.0 license. We hope this work serves as a foundation for the robustness of promise verification systems and contributes to greater ac- countability in corporate and public disclosures.Limitation Several limitations warrant discussion. First, al- though the ML-Promise dataset includes five lan-',\n",
       "  'chunk_id': 18},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'Several limitations warrant discussion. First, al- though the ML-Promise dataset includes five lan- guages?Chinese, English, French, Japanese, and Korean?its scope is still limited to a few countries and may not fully capture the diversity of corpo- rate promise communication styles globally. The dataset focuses on ESG reports from specific re- gions, which may limit the generalizability of the findings to other languages and cultural contexts. Future studies can follow our design to expand the dataset to include more regions and languages, which could enhance the robustness and applicabil- ity of the proposed methods. Second, although the study uses RAG to improve performance, the re- sults show that this approach does not consistently outperform baseline models across all languages and tasks. These inconsistencies suggest that RAG may require further optimization or task-specific adjustments, particularly in handling the nuances of each language and dataset structure. These',\n",
       "  'chunk_id': 19},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'adjustments, particularly in handling the nuances of each language and dataset structure. These limitations and our findings highlight ar- eas for future research, including expanding the dataset, refining the RAG approach, enhancing mul- timodal learning, and addressing the inherent am- biguities in corporate ESG reporting. Ozgur Ardic, Mahiye Uluyagmur Ozturk, Irem Demir- tas, and Secil Arslan. 2024. Information Extraction from Sustainability Reports in Turkish through RAG Approach. In 2024 32nd Signal Processing and Com- munications Applications Conference (SIU) , pages Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhi- fang Sui. 2022. A Survey on In-context Learning. arXiv preprint arXiv:2301.00234 . Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs: To-',\n",
       "  'chunk_id': 20},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs: To- wards Retrieval-Augmented Large Language Models. Dario Garigliotti. 2024. SDG target detection in envi- ronmental reports using Retrieval-augmented Gener- ation with LLMs. In Proceedings of the 1st Work- shop on Natural Language Processing Meets Cli- mate Change (ClimateNLP 2024) , pages 241 250, Bangkok, Thailand. Association for Computational Linguistics.Nina Gorovaia and Michalis Makrominas. 2024. Identi- fying greenwashing in corporate-social responsibility reports using natural-language processing. European Marcelo Gutierrez-Bustamante and Leonardo Espinosa- Leal. 2022. Natural Language Processing Meth- ods for Scoring Sustainability Reports? A Study of Nordic Listed Companies. Sustainability , 14(15). Lars Hillebrand, Maren Pielka, David Leonhard, To- bias Deu er, Tim Dilmaghani, Bernd Kliem, R diger Loitz, Milad Morad, Christian Temath, Thiago Bell, Robin Stenzel, and Rafet Sifa.',\n",
       "  'chunk_id': 21},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'Kliem, R diger Loitz, Milad Morad, Christian Temath, Thiago Bell, Robin Stenzel, and Rafet Sifa. 2023. sustain.AI: a Recommender System to analyze Sustainability Re- ports. In Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law , ICAIL 23, page 412?416, New York, NY , USA. Association Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. 2024. E5-V: Universal Embeddings with Multimodal Large Language Mod- Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich K ttler, Mike Lewis, Wen-tau Yih, Tim Rock- t schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge- intensive NLP tasks. In Proceedings of the 34th International Conference on Neural Information Pro- cessing Systems , Red Hook, NY , USA. Curran Asso- Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text',\n",
       "  'chunk_id': 22},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'Curran Asso- Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out , pages 74 81, Barcelona, Spain. Association for Computational Linguistics. Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica , 22(3):276 282. Yu-Min Tseng, Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen. 2023. DynamicESG: A Dataset for Dynamically Unearthing ESG Ratings from News Articles. In Proceedings of the 32nd ACM Interna- tional Conference on Information and Knowledge Management , pages 5412 5416. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024a. Multi- lingual E5 Text Embeddings: A Technical Report. arXiv preprint arXiv:2402.05672 . Shuting Wang, Jiongnan Liu, Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yu- tao Zhu, and Zhicheng Dou. 2024b. Domain- RAG: A Chinese Benchmark for Evaluating Domain- specific Retrieval-Augmented Generation. Preprint , Hao Yu, Aoran',\n",
       "  'chunk_id': 23},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'Benchmark for Evaluating Domain- specific Retrieval-Augmented Generation. Preprint , Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of Retrieval- Augmented Generation: A Survey. Preprint , We provide five ESG report exam- ples in this section, and please refer to our training set for more instances:',\n",
       "  'chunk_id': 24}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "GOOGLE_API_KEY = \"AIzaSyDLBN8OD_rJg93yVAyFP9mNKZQ0YX3IPL4\"  # You'll need to set this\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\",  # or \"text-embedding-004\" depending on access\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the metadata structure when creating the FAISS index\n",
    "texts = [doc[\"content\"] for doc in documents]\n",
    "metadata = [{\n",
    "    \"source\": f\"Paper_{doc['paper_id']}_chunk_{doc['chunk_id']}\", # Add source field\n",
    "    \"paper_id\": doc[\"paper_id\"],\n",
    "    \"title\": doc[\"title\"],\n",
    "    \"chunk_id\": doc[\"chunk_id\"]\n",
    "} for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vector Embeddings of the Chunks\n",
    "\n",
    "vectorstore = FAISS.from_texts(texts, embeddings, metadatas=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index for reuse\n",
    "vectorstore.save_local(\"data/RAG/embeddings/top5_papers_faiss_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining RAG components and implementing the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"filter\": lambda metadata: metadata[\"paper_id\"] == current_paper_id  # We'll set this dynamically\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the LLM using ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Create the RAG pipeline with the correct chain type\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"map_reduce\"  # or  if dealing with longer texts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a structured prompt for each paper\n",
    "\n",
    "def create_paper_prompt(question, title):\n",
    "    return f\"\"\"You are an expert at analysing research papers. \n",
    "    For the paper with the title {title}, please answer this query.\n",
    "    {question}.\n",
    "    Be specific to this paper only.\"\"\"\n",
    "\n",
    "# Define the base questions\n",
    "questions = [\n",
    "    \"Provide a comprehensive summary of the paper from the info that you have\",\n",
    "    \"what specific problem does this paper solve?\",\n",
    "    \"how does the paper solve this problem?\",\n",
    "    \"what are the next steps or future work suggested in this paper?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chunking, embedding and RAG pipeline for each paper separately\n",
    "\n",
    "results = {}\n",
    "for paper in top_5_rag_papers:\n",
    "    paper_id = paper[\"paper_id\"]\n",
    "    title = paper[\"title\"]\n",
    "    paper_text = paper[\"text\"]\n",
    "    results[paper_id] = {}\n",
    "    \n",
    "    # Create a temporary vector store just for this paper\n",
    "    temp_chunks = text_splitter.split_text(paper_text)\n",
    "    temp_metadata = [{\"source\": f\"Paper_{paper_id}_chunk_{i}\", \"paper_id\": paper_id, \"title\": title, \"chunk_id\": i} \n",
    "                    for i in range(len(temp_chunks))]\n",
    "    \n",
    "    temp_vectorstore = FAISS.from_texts(\n",
    "        temp_chunks, \n",
    "        embeddings, \n",
    "        metadatas=temp_metadata\n",
    "    )\n",
    "    \n",
    "    # Create a new retriever and chain for this paper\n",
    "    temp_retriever = temp_vectorstore.as_retriever(\n",
    "        search_type=\"similarity\", \n",
    "        search_kwargs={\"k\": 10}\n",
    "    )\n",
    "    \n",
    "    temp_qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=temp_retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type=\"map_reduce\"\n",
    "    )\n",
    "    \n",
    "    for question in questions:\n",
    "        # Create a specific prompt for this paper\n",
    "        specific_prompt = create_paper_prompt(question, title)\n",
    "        \n",
    "        # Query the RAG system\n",
    "        result = temp_qa_chain({\"question\": specific_prompt})\n",
    "        results[paper_id][question] = {\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"sources\": result[\"sources\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'Provide a comprehensive summary of the paper from the info that you have': {'answer': 'The paper titled \"Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation\" explores the integration of multiple subsystems in dynamic environments using OpenAPI descriptions. The study focuses on the challenges of preprocessing API documentation to fit within the input token limitations of Large Language Models (LLMs) while maintaining relevant information. The authors propose the use of Retrieval-Augmented Generation (RAG) for endpoint discovery, employing various chunking strategies to optimize the retrieval process.\\n\\nThe paper introduces two main approaches: the OpenAPI RAG and the Discovery Agent. The OpenAPI RAG applies RAG for endpoint discovery using seven chunking strategies, validated through the RestBench benchmark. The Discovery Agent further enhances the process by breaking down queries into smaller tasks, improving precision and F1 scores while reducing recall. The study demonstrates that LLM-based and format-specific preprocessing methods outperform naive chunking strategies, and the use of an agent improves overall performance.\\n\\nThe research highlights the effectiveness of endpoint split-based chunking strategies and the need for further research to enhance retrieval performance. The paper provides open-source prototypes and results, emphasizing the potential of automated service composition using LLMs for integrating Information Systems.\\n\\n',\n",
       "   'sources': 'Paper_1_chunk_0, Paper_1_chunk_24, Paper_1_chunk_35, Paper_1_chunk_6, Paper_1_chunk_2, Paper_1_chunk_19, Paper_1_chunk_34, Paper_1_chunk_1'},\n",
       "  'what specific problem does this paper solve?': {'answer': 'The paper titled \"Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation\" addresses the problem of improving preprocessing methods for retrieval-augmented generation (RAG) by using LLM-based and format-specific approaches. It specifically focuses on how to optimally preprocess, or chunk, OpenAPIs for RAG endpoint discovery to reduce input token length while preserving relevant information. The paper also proposes the use of a Discovery Agent to further enhance performance metrics such as token count, precision, and F1 score by splitting tasks into finer subtasks and retrieving specification details on demand.\\n\\n',\n",
       "   'sources': 'Paper_1_chunk_0, Paper_1_chunk_2, Paper_1_chunk_4, Paper_1_chunk_1'},\n",
       "  'how does the paper solve this problem?': {'answer': 'The paper titled \"Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation\" addresses the problem by implementing an OpenAPI RAG (Retrieval-Augmented Generation) system that uses various chunking strategies to manage the integration of multiple subsystems. The system applies token-based and LLM-based chunking strategies to split service descriptions into manageable parts, such as endpoints, to fit within the input token limitations of Large Language Models (LLMs). The paper evaluates these strategies based on retrieval quality using the RestBench benchmark. The LLM-based strategies include generating summaries and queries for each OpenAPI endpoint to create embeddings, while the chunk content remains the original endpoint information. The Discovery Agent further enhances the system by breaking down queries into smaller tasks, retrieving relevant chunks, and improving precision and F1 scores, although it may reduce recall.\\n\\n',\n",
       "   'sources': 'Paper_1_chunk_0, Paper_1_chunk_35, Paper_1_chunk_24, Paper_1_chunk_19, Paper_1_chunk_6, Paper_1_chunk_2, Paper_1_chunk_4, Paper_1_chunk_23'},\n",
       "  'what are the next steps or future work suggested in this paper?': {'answer': 'The paper suggests several next steps or future work, including:\\n\\n1. Further research to improve retrieval performance and address the decline in recall due to processing through the LLM (Large Language Model).\\n2. Evaluation in larger real-world scenarios remains open for future research.\\n3. Exploring the automatic generation of service documentation using an LLM.\\n4. Creating a custom chunking strategy for specific kinds of service documentation.\\n5. Further research is necessary to retrieve all requisite endpoints.\\n\\n',\n",
       "   'sources': 'Paper_1_chunk_35, Paper_1_chunk_37, Paper_1_chunk_36, Paper_1_chunk_2'}},\n",
       " 66: {'Provide a comprehensive summary of the paper from the info that you have': {'answer': 'The paper titled \"ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet?\" investigates the challenges and solutions related to the inaccuracies of ChatGPT when processing technical reports, specifically software bug reports. These reports often contain a mix of descriptive text, technical terminology, and code snippets, which can be complex for AI models to interpret accurately. The study identifies two main limitations of ChatGPT: its difficulty in understanding complex technical content like stack traces and its challenges in integrating context from technical terms and texts.\\n\\nTo address these issues, the authors developed CHIME (ChatGPT Inaccuracy Mitigation Engine), which preprocesses technical reports and guides the query validation process in ChatGPT. CHIME uses context-free grammar to parse stack traces and applies metamorphic testing and query transformation to verify and correct ChatGPT\\'s responses. The study found that CHIME improved the accuracy of ChatGPT\\'s responses by 30.3% over the baseline. The paper highlights the importance of preprocessing and guided prompting to mitigate hallucinations and improve the reliability of AI models in technical domains.\\n\\n',\n",
       "   'sources': 'Paper_66_chunk_0, Paper_66_chunk_1, Paper_66_chunk_2, Paper_66_chunk_4, Paper_66_chunk_5'},\n",
       "  'what specific problem does this paper solve?': {'answer': 'The paper \"ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet?\" addresses the problem of inaccuracies, specifically hallucinations, in ChatGPT\\'s responses when processing technical reports. The paper identifies two main limitations of ChatGPT: 1) difficulty in understanding complex technical content, such as code snippets and stack traces, and 2) challenges in integrating contexts denoted by technical terms. To solve these issues, the paper introduces CHIME (ChatGPT Inaccuracy Mitigation Engine), which preprocesses technical reports and guides the query validation process to improve the accuracy of ChatGPT\\'s responses. CHIME uses context-free grammar to parse stack traces and applies metamorphic testing and query transformation to verify and correct ChatGPT\\'s outputs, resulting in a 30.3% improvement in accuracy over standard ChatGPT responses.\\n\\n',\n",
       "   'sources': 'Paper_66_chunk_0, Paper_66_chunk_1, Paper_66_chunk_4, Paper_66_chunk_5'},\n",
       "  'how does the paper solve this problem?': {'answer': 'The paper \"ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet?\" addresses the problem of inaccuracies in ChatGPT\\'s understanding of technical reports by introducing CHIME (ChatGPT Inaccuracy Mitigation Engine). CHIME uses context-free grammar (CFG) to parse stack traces in technical reports and applies metamorphic testing and query transformation to verify and correct ChatGPT responses. The approach involves preprocessing technical reports to store relevant information as metadata and guiding the query validation process with iterative prompting. This method shows a 30.3% improvement in correction over standard ChatGPT responses.\\n\\n',\n",
       "   'sources': 'Paper_66_chunk_0, Paper_66_chunk_1, Paper_66_chunk_5'},\n",
       "  'what are the next steps or future work suggested in this paper?': {'answer': \"In the future, the paper suggests focusing on expanding CHIME's grasp of more technical terminologies and other documents. This includes improving the issue preprocessor module to separate code and textual contents, adapting the CFG to handle code snippets/traces/crash dumps, and using a static partial program analyzer to handle code examples in API documentation. The expectation is that other modules in CHIME can be used with minimal changes.\\n\\n\",\n",
       "   'sources': 'Paper_66_chunk_61'}},\n",
       " 77: {'Provide a comprehensive summary of the paper from the info that you have': {'answer': 'The paper titled \"Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving\" presents an innovative approach to solving mathematical problems in Bengali, specifically developed for the DL Sprint 3.0 - BUET CSE Fest 2024 Competition. The methodology utilizes advanced deep learning models from the Qwen 2.5 series, with enhancements through prompt engineering, model quantization, and Tool Integrated Reasoning (TIR) to manage complex calculations. The research involved exploring various model architectures, including fine-tuned Mistral and quantized Qwen models, and refining them through translation techniques, Retrieval-Augmented Generation (RAG), and custom dataset curation. Manual hyperparameter tuning was employed to optimize parameters like temperature and top-p, improving model adaptability and response accuracy.\\n\\nThe Qwen 2.5-32B model, combined with techniques such as self-consistency and TIR, showed significant improvements over baseline models, demonstrating its potential in handling complex reasoning tasks with minimal Bengali-specific data. However, challenges such as translation dependencies, limited Bengali datasets, and memory constraints for large models were noted, impacting model performance and adaptability. The paper highlights the sensitivity of model outputs to prompt phrasing, emphasizing the need for refined prompt engineering.\\n\\nThe research also involved translating Bengali mathematical questions into English to enhance processing, with the Qwen-32B-Instruct model showing superior performance in translation and problem-solving compared to smaller models. Despite the achievements, the paper acknowledges the limitations due to Kaggle\\'s limited GPU memory, which restricted fine-tuning of larger models like Qwen 2.5-32B.\\n\\n',\n",
       "   'sources': 'Paper_77_chunk_0, Paper_77_chunk_1, Paper_77_chunk_14, Paper_77_chunk_6, Paper_77_chunk_13, Paper_77_chunk_5, Paper_77_chunk_11, Paper_77_chunk_4, Paper_77_chunk_12'},\n",
       "  'what specific problem does this paper solve?': {'answer': 'The paper \"Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving\" addresses the specific problem of solving mathematical problems in Bengali, particularly those akin to the Bengali Math Olympiad. It tackles the challenge of mathematical reasoning in low-resource languages like Bengali by leveraging the Qwen 2.5 series models, optimizing through prompt engineering, translation, and Tool Integrated Reasoning (TIR). The approach includes translating Bengali mathematical questions into English to improve processing and using advanced NLP techniques to enhance problem-solving capabilities in this context.\\n\\n',\n",
       "   'sources': 'Paper_77_chunk_0, Paper_77_chunk_1, Paper_77_chunk_2, Paper_77_chunk_14'},\n",
       "  'how does the paper solve this problem?': {'answer': 'The paper \"Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving\" addresses the problem by utilizing the Qwen 2.5 series models, particularly the Qwen-32B-Instruct model, to solve mathematical problems in Bengali. The approach involves several key techniques:\\n\\n1. **Translation**: Bengali mathematical questions are translated into English using the Qwen-32B-Instruct model to improve processing and understanding, leveraging the model\\'s richer pre-trained capabilities.\\n\\n2. **Prompt Engineering**: The paper employs prompt tuning to optimize model performance, experimenting with various reasoning techniques such as Chain of Thought (COT), Tool Integrated Reasoning (TIR), and self-consistent versions of these techniques. Prompts are designed to guide the model through step-by-step reasoning and to perform calculations using Python tools for complex problems.\\n\\n3. **Tool Integrated Reasoning (TIR)**: This involves adapting prompts to instruct the model to use external tools for calculations, enhancing its ability to handle complex mathematical problems.\\n\\n4. **Self-Consistency**: The model generates multiple reasoning paths and selects the most consistent solution, improving accuracy and robustness.\\n\\n5. **Model Quantization and Hyperparameter Tuning**: These techniques are used to optimize the model\\'s adaptability and response accuracy, with manual tuning of parameters like temperature and top-p.\\n\\n6. **Removal of RAG**: The paper found that removing Retrieval-Augmented Generation (RAG) improved scores, indicating that the inherent capabilities of the larger Qwen models were sufficient without additional retrieved context.\\n\\nThese methods demonstrate the efficacy of advanced NLP techniques in mathematical problem-solving, particularly for low-resource languages like Bengali, despite challenges such as translation dependencies and limited datasets.\\n\\n',\n",
       "   'sources': 'Paper_77_chunk_0, Paper_77_chunk_1, Paper_77_chunk_6, Paper_77_chunk_11, Paper_77_chunk_14'},\n",
       "  'what are the next steps or future work suggested in this paper?': {'answer': 'The next steps or future work suggested in the paper include increasing the availability and diversity of Bengali mathematical datasets, domain-specific fine-tuning of the Qwen 2.5-32B model on Bengali math problems, optimizing prompt engineering for mathematical reasoning tasks in low-resource languages, and exploring lightweight models tailored for Bengali to balance memory constraints and inference speed.\\n\\n',\n",
       "   'sources': 'Paper_77_chunk_15'}},\n",
       " 80: {'Provide a comprehensive summary of the paper from the info that you have': {'answer': 'The paper titled \"Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework\" discusses the development of an autonomous intelligent system using Large Language Models (LLMs) for diagnosing and troubleshooting AI clusters. The system leverages recent advancements in LLMs, Retrieval-Augmented Generation (RAG), and Diagram of Thought (DoT) technologies, integrating them with self-play methodologies to create an LLM-agent capable of autonomously diagnosing and resolving cluster issues. Key innovations include a knowledge base tailored for cluster diagnostics, enhanced LLM algorithms, and practical deployment strategies for agents. The paper also introduces a benchmark with 150 manually crafted questions to evaluate the LLM-agent\\'s capabilities, demonstrating its efficiency in identifying faults faster than human experts. The research aims to enhance cluster availability by detecting and rectifying performance issues more efficiently than traditional methods, although it acknowledges the need for further advancements in communication abilities and quantitative analysis.\\n\\n',\n",
       "   'sources': 'Paper_80_chunk_0, Paper_80_chunk_2, Paper_80_chunk_45, Paper_80_chunk_48'},\n",
       "  'what specific problem does this paper solve?': {'answer': 'The paper titled \"Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework\" addresses the problem of cluster troubleshooting by proposing a system that utilizes LLM-agents to autonomously and intelligently diagnose and resolve issues within AI clusters. This system enhances cluster resilience by preemptively addressing potential faults, thereby freeing up engineering resources to focus on more complex tasks. It is capable of detecting task failures, identifying points of failure, clarifying reasons for failure, and repairing corresponding faults, ensuring the overall availability of the AI cluster. The system demonstrates efficiency and accuracy in diagnosing and rectifying performance issues more effectively than traditional methods.\\n\\n',\n",
       "   'sources': 'Paper_80_chunk_4, Paper_80_chunk_45, Paper_80_chunk_0, Paper_80_chunk_1, Paper_80_chunk_48'},\n",
       "  'how does the paper solve this problem?': {'answer': 'The paper \"Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework\" addresses the problem by developing an LLM-agent system that autonomously diagnoses and resolves issues within AI clusters. This system leverages recent advancements in Large Language Models (LLMs) and related technologies such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT). The LLM-agent can autonomously execute remediation measures, freeing up engineering resources for more complex tasks. It accurately identifies faults, such as a GPU with low core frequency, before they are detected by engineers, and automatically completes repairs, enhancing the overall availability of the cluster. The system includes a specialized knowledge base for cluster diagnostics, enhanced LLM algorithms, practical deployment strategies, and a benchmark for evaluating LLM capabilities in this domain. Through extensive experimentation, the system has demonstrated superiority in detecting and rectifying performance issues more efficiently and accurately than traditional methods.\\n\\n',\n",
       "   'sources': 'Paper_80_chunk_0, Paper_80_chunk_1, Paper_80_chunk_2, Paper_80_chunk_4, Paper_80_chunk_12, Paper_80_chunk_45, Paper_80_chunk_48'},\n",
       "  'what are the next steps or future work suggested in this paper?': {'answer': 'The paper suggests that future work will focus on continuously monitoring the development of smaller models and exploring the possibility of teaching the capabilities of the LLM-agent to these smaller models in the form of DoT when appropriate.\\n\\n',\n",
       "   'sources': 'Paper_80_chunk_51'}},\n",
       " 87: {'Provide a comprehensive summary of the paper from the info that you have': {'answer': 'The paper titled \"ML-Promise: A Multilingual Dataset for Corporate Promise Verification\" introduces the concept of Promise Verification, a novel task aimed at evaluating the credibility and fulfillment of promises made by corporations, particularly in the context of ESG (Environmental, Social, and Governance) reports. The authors propose the first multilingual dataset, ML-Promise, which includes languages such as English, French, Chinese, Japanese, and Korean, to facilitate in-depth verification of corporate promises. This dataset addresses challenges like greenwashing by evaluating corporate environmental and social promises.\\n\\nThe study highlights that around 35-40% of the evidence is unclear in supporting the associated promises in four out of five languages, emphasizing the necessity of the proposed task. Additionally, about 4% of instances contain potentially misleading evidence in the English and Japanese datasets. The authors explore textual and image-based baselines, with promising results from Retrieval-Augmented Generation (RAG) approaches, although RAG does not consistently outperform baseline models across all languages and tasks.\\n\\nThe paper also discusses the limitations of the dataset, noting its focus on specific regions and the need for future studies to expand its scope to include more regions and languages. The authors plan to release their annotations under the CCBY-NC-SA 4.0 license, hoping their work will contribute to greater accountability in corporate and public disclosures.\\n\\n',\n",
       "   'sources': 'Paper_87_chunk_0, Paper_87_chunk_1, Paper_87_chunk_3, Paper_87_chunk_5, Paper_87_chunk_8, Paper_87_chunk_10, Paper_87_chunk_18, Paper_87_chunk_19'},\n",
       "  'what specific problem does this paper solve?': {'answer': 'The paper \"ML-Promise: A Multilingual Dataset for Corporate Promise Verification\" addresses the problem of evaluating the credibility and fulfillment of promises made by corporations, particularly in the context of Environmental, Social, and Governance (ESG) reports. It introduces the concept of Promise Verification and proposes the first multilingual dataset, ML-Promise, to facilitate the verification of corporate promises and to address challenges such as greenwashing. The dataset includes languages such as English, French, Chinese, Japanese, and Korean, and focuses on ESG reports from corporations in the U.K., France, Taiwan, Japan, and Korea.\\n\\n',\n",
       "   'sources': 'Paper_87_chunk_0, Paper_87_chunk_18, Paper_87_chunk_1, Paper_87_chunk_5'},\n",
       "  'how does the paper solve this problem?': {'answer': 'The paper \"ML-Promise: A Multilingual Dataset for Corporate Promise Verification\" addresses the problem of verifying corporate promises, particularly in the context of Environmental, Social, and Governance (ESG) reports, by introducing a novel task called Promise Verification. This involves evaluating the credibility and fulfillment of promises made by corporations. The paper proposes the first multilingual dataset, ML-Promise, which includes languages such as English, French, Chinese, Japanese, and Korean, to facilitate in-depth verification of these promises. The dataset is designed to address challenges like greenwashing by evaluating corporate promises in ESG reports from companies in the UK, France, Taiwan, Japan, and Korea. The task involves several steps: Promise Identification, Supporting Evidence, and Clarity of the Promise-Evidence Pair. The study also explores textual and image-based baselines, with promising results from retrieval-augmented generation (RAG) approaches, although RAG does not consistently outperform baseline models across all languages and tasks, indicating a need for further optimization.\\n\\n',\n",
       "   'sources': 'Paper_87_chunk_0, Paper_87_chunk_1, Paper_87_chunk_8, Paper_87_chunk_18'},\n",
       "  'what are the next steps or future work suggested in this paper?': {'answer': 'Future studies can follow the design of the paper to expand the dataset to include more regions and languages, which could enhance the robustness and applicability of the proposed methods.\\n\\n',\n",
       "   'sources': 'Paper_87_chunk_19'}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a JSON file\n",
    "with open(\"data/RAG/results/top_5_paper_analysis_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enefit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
