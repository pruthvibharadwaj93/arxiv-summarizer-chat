{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries, Set keys, Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.api.arxiv import load_data\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "from google.generativeai import GenerativeModel\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "# Count chunks per paper\n",
    "from collections import Counter\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "from fpdf import FPDF\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from data/RAG/text/top_5_rag_papers.json\n"
     ]
    }
   ],
   "source": [
    "top_5_rag_papers = load_data(file_path=\"data/RAG/text/top_5_rag_papers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'text': 'Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation Robin D. Pesl1, Jerin G. Mathew2, Massimo Mecella2, and Marco Aiello1 1University of Stuttgart, Stuttgart, Germany 2Sapienza Universit di Roma, Rome, Italy Abstract Integrating multiple (sub-)systems is essential to create ad- vanced Information Systems (ISs). Difficulties mainly arise when integrat- ing dynamic environments across the IS lifecycle, e.g., services not yet existent at design time. A traditional approach is a registry that provides the API documentation of the systems endpoints. Large Language Mod- els (LLMs) have shown to be capable of automatically creating system integrations (e.g., as service composition) based on this documentation but require concise input due to input token limitations, especially regard- ing comprehensive API descriptions. Currently, it is unknown how best to preprocess these API descriptions. Within this work, we (i) analyze the usage of Retrieval Augmented Generation (RAG) for endpoint discovery and the chunking, i.e., preprocessing, of state-of-practice OpenAPIs to reduce the input token length while preserving the most relevant informa- tion. To further reduce the input token length for the composition prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that only receives a summary of the most relevant endpoints and retrieves specification details on demand. We evaluate RAG for endpoint discovery using the RestBench benchmark, first, for the different chunking possibil- ities and parameters measuring the endpoint retrieval recall, precision, and F1 score. Then, we assess the Discovery Agent using the same test set. With our prototype, we demonstrate how to successfully employ RAG for endpoint discovery to reduce the token count. While revealing high values for recall, precision, and F1, further research is necessary to retrieve all requisite endpoints. Our experiments show that for preprocessing, LLM-based and format-specific approaches outperform na ve chunking methods. Relying on an agent further enhances these results as the agent splits the tasks into multiple fine granular subtasks, improving the overall RAG performance in the token count, precision, and F1 score. Keywords: Retrieval augmented generation Large language models OpenAPI Endpoint discovery RestBench. OpenAPI is the state-of-practice for describing interfaces for integrating Informa- tion Systems (ISs). It contains formal elements like paths and natural language 29 Nov 20242 Pesl et al. constituents like descriptions. For integrating these systems automatically, au- tomated service composition using Large Language Models (LLMs) has been proposed [ 27,28,29]. These approaches exploit the capabilities of LLMs to process formal and natural language input, combining them with the inherent nature of automated service composition of decoupling and independent lifecycle manage- ment. While prohibiting any manual modeling effort by relying on already broadly available OpenAPIs, the approaches face the challenge of limited input token length [29]. This bounds the quantity and extent of the input service description. Even for proprietary models with a large input token context, e.g., OpenAIs GPT4 with a context size of 128,000 tokens [ 23], an economic constraint emerges as these models are paid by input and output token count. Therefore, a smaller prompt length is beneficial to (i) insert further service documentation and (ii) reduce costs for proprietary models. To address these challenges, Retrieval Augmented Generation (RAG) [ 16] has emerged as a promising resort. In this approach, the external information is collected in a database, typically structured as a set of documents or document chunks. The primary goal is retrieving only a small subset of the most relevant documents or document chunks, which is then inserted into the prompt [ 16]. How to optimally apply RAG for endpoint discovery in IS is open to investigation, leading to the following research questions: RQ1.How best to preprocess, i.e., chunk, OpenAPIs for RAG endpoint discovery? RQ2.Can LLM agents be employed to reduce token count further and improve To answer RQ1, we develop an OpenAPI RAG system that takes as input service descriptions. We apply different token-based and LLM-based chunking strategies to split the documentation and evaluate them based on retrieval quality. The token-based strategies process the document using a classical parser and then split the parts, e.g., endpoints, into equal-sized chunks. The LLM-based strategies let an LLM create a description, i.e., a summary or a question, for each endpoint and then use these descriptions for similarity matching. We employ mainstream open-source and proprietary embedding models for similarity matching, which can create an embedding vector for an input. The similarity between two inputs can then be determined by comparing their embedding vectors using, e.g., the cosine similarity. We evaluate the OpenAPI RAG and the different chunking strategies by relying on the already available RestBench benchmark [ 35] for LLMs agents, measuring recall, precision, and F1 score for each chunking strategy. The benchmark consists of the OpenAPI descriptions of Spotify and TMDB and corresponding queries, each with a set of endpoints as the sample solution. To address RQ2, we propose an LLM agent called Discovery Agent . As LLM agents allow the usage of external tools, we first investigate using one tool that simply inputs the results of the RAG to the prompt. Then, we experiment with using two tools: the first tool filters and enters the LLM endpoint summaries to the prompt using RAG, while the second tool allows the retrieval of the endpoint details on demand. We resort to the same RestBench benchmark for evaluation and measure recall, precision, F1 score, and additional token count.Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 3 As the chunking strategy, we rely on the LLM-based summary strategy with OpenAIs text-embedding-3-large embedding model [24]. The remainder of the paper is structured as follows. First, we provide an overview of related works regarding service discovery and LLMs in Section 2. Then, we present how to use RAG for endpoint discovery and the OpenAPI chunking strategies in Section 3. We evaluate and discuss the RAG and the different chunking strategies in Section 4. Finally, we conclude with Section 5. Regarding endpointdiscovery,we provide abrief overview ofthe essential concepts of the various service discovery approaches. Additionally, we provide relevant insights into LLMs and the novel approach of integrating LLMs with tools, known as LLM agents, and how they relate to our approach. The most common service discovery implementation is a service registry, which collects information about available services and offers search facilities. This service registry is usually backed by a component residing at the middleware or application level [ 15]. It is characterized by the syntax used to describe the services and their invocation and the expressive power of the available query language. The typical integration model is a pull model where service consumers search for the required services. Less is a push model as used in the UPnP protocol, where service providers regularly advertise their services [32]. In the early days of XML-based services, the infrastructure for service discov- ery was the Universal Description, Discovery, and Integration (UDDI) specifi- cation [7]. UDDI had a global incarnation called the UDDI Business Registry (UBR), intended to offer an Internet-wide repository of available web services and promoted by IBM, Microsoft, and SAP. Unfortunately, UBR never gained widespread adoption and was short-lived (2000-2006). Significant research in the early days focused on enhancing service discovery on UDDI, improving search capabilities, and creating federated registries, e.g., [ 3,4,11]. Alternatively, WS-Discovery is a multicast protocol that finds web services on a local network. Nowadays, OpenAPI is the de facto standard for describing services. While not offering a discovery protocol and mechanism, given its popularity, OpenAPI would also benefit from discovery [ 34]. So, additional infrastructure for discovery has been proposed, such as centralized repositories (SwaggerHub or Apiary), service registry integration (Consul, Eureka), API Gateways (Kong, Apigee), or Populating registries of services requires effort from service providers, which often hinders the success of such approaches, especially if the service provider is expected to provide extensive additional information beyond the service endpoints. This additional effort has often been the reason for the failure of some of these technologies, most notably UBR. Approaches confined to specific applications,4 Pesl et al. domains, or enterprises have been more successful, e.g., Eureka. Developed by Netflix as part of its microservices architecture [ 36], Eureka helps clients find service instances described by host IP, port, health indicator URL, and home page. Developers can add additional data to the registry for extra use cases. extensive modeling, e.g., as semantic annotations. Hence, our approach relies on already broadly available state-of-practice OpenAPI specification and their 2.2 Large Language Models (NLP) and machine learning field [ 1,2,13]. Often containing billions of parameters, these models are trained on extensive text corpora to generate and manipulate human-like text [ 30]. They are primarily based on an encoder-decoder architecture tasks using decoder-only models such as GPT [ 31]. Usually, the input is a natural language task called prompt, which first needs to be translated to a sequence of input tokens. The model processes this prompt and returns an output token sequence, which can then be translated back to a natural language answer. As these models can, in general, capture intricate linguistic nuances and semantic contexts, they can be applied to a wide range of tasks, e.g., in software engineering [ 10]. LLMs can be used to create integration based on endpoint documentation automatically [ 27,28,29]. Yet, these face strict input token limitations, e.g., 128,000 tokens for current OpenAI models [23,29]. With this paper, we analyze how RAG can be used to preprocess API documentation to mitigate this issue. Another approach is encoder-only models such as BERT [ 8], often referred to as embedding models. They allow condensing the contextual meaning of a text into a dense vector, termed embedding. Using similarity metrics such as dot product, cosine similarity, or Euclidean distance allows for assessing the similarity of two input texts. Embedding models are usually used for the similarity search in RAG systems [6], which we also do in our implementation. LLMs have shown remarkable capabilities in solving complex tasks by decompos- ing them in a step-by-step fashion [ 38] or by exploring multiple solution paths simultaneously [ 41]. Typically, these plans are generated iteratively by using the history of the previously generated steps to guide the generation of the next step. Additionally, recent studies have shown the potential of providing LLMs access to external tools to boost their reasoning capabilities and add further knowledge. This approach consists of prompting the LLM to interact with external tools to solve tasks, thus offloading computations from the LLM to specialized functions. Notable examples of such tools include web browsers [ 20], calculators [ 5], andAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 5 Python interpreters [ 12]. In practice, a tool is usually a Python function, which can be called during the interaction with the LLM. The LLM agent paradigm [ 19,22,40] combines the concepts of (i) external tool usage, (ii) the planning capabilities of LLMs, and adds a shared (iii) memory, to solve complex tasks. Given an input task, an LLM agent uses its reasoning capabilities to decompose the task into a set of simpler subtasks. For each subtask, the LLM finds and interacts with the set of tools to solve the subtask. Then, based on the outcome of the current task and the history of previously executed subtasks, the LLM agent generates a new subtask and repeats the steps mentioned above or terminates if the original task is solved. To instruct the processing, the outcome of the tool invocations and the history of the subtasks are stored in the memory, typically consisting in the LLM agents own context. Within this work, we apply the LLM agent paradigm to create the Discovery Agent as an LLM agent for endpoint discovery. A critical challenge for LLM agents is the accessibility to a set of common APIs and tasks for their evaluation, e.g., tested using benchmarks like API Bank [17] or RestBench [ 35]. API Bank is a benchmark to evaluate the tool use of an LLM consisting of a set of APIs exposed through a search engine. Unfortunately, the available code of the benchmark is incomplete. The RestBench benchmark contains a collection of tasks and endpoints expressed using the OpenAPI specification of Spotify and TMDB [ 35]. As the currently most extensive available benchmark, we employ RestBench to validate our results. OpenAPIs within LLM agents have been used in RestGPT [ 35] and Chain of Tools [ 33]. The former combines multiple LLM agents to solve complex tasks by interacting with a set of tools exposed using the OpenAPI specification. The latter solves an input query by framing the problem as a code generation task and interacts with the set of tools to generate Python code to solve the query. In contrast, our Discovery Agent does not directly interact with the endpoints stated in the OpenAPIs. Instead, it filters and returns matching endpoints that can be used for subsequent processing. Even when considering the similarity to the tool selection within LLM agents, the task of selecting a set of tools from a larger pool to solve a specific problem remains relatively underexplored [ 42]. Existing research primarily focuses on the a priori selection of human-curated tools [ 25], heuristic-based methods for tool selection [ 18], choosing the relevant tool by scoring each query against every tool based semantic retrieval using a combination of different vector databases [ 42]. With our work, we contribute the analysis of preprocessing OpenAPIs into this We first introduce the general architecture to employ RAG for endpoint discovery. As state-of-practice for service documentation, we then investigate how to chunk OpenAPIs as preprocessing for RAG.6 Pesl et al. 3.1 RAG for Endpoint Discovery RAG comprises a preprocessing step ahead of the answer generation of an LLM to a semantic search based on some knowledge sources. Usually, the semantic search is done by embedding similarity, and the data from the knowledge sources is reduced to small chunks to allow fine-grained information retrieval [16]. Figure1. RAG for Endpoint Discovery Figure 1 depicts the application of RAG for endpoint discovery, i.e., the OpenAPI RAG. Initially, the chunking strategy determines how the chunks are created from the OpenAPIs, i.e., how many chunks are created and what they contain. Each chunk has an embedding as metadata for similarity search in addition to its content. The chunking strategy specifies which data is used as input to the embedding model to create the embedding. This input does not have to match the chunk content, e.g., it can be a summary instead of the entire content. The chunks are finally stored in the chunk database. For retrieval, the user submits in 1a natural language query qto the chunk retriever, which converts qinto the embedding eusing the same embedding model as for the chunk creation. In 2, the chunk retriever queries the chunk database using e. The chunk database compares eusing a similarity metric with the embeddings of the service chunks contained in the database. The results are the top kmost similar chunks according to the metric, which are then returned to the chunk retriever in 3. Finally, in 4, the chunk retriever forwards the retrieved results to the user, who can add them to their prompt either manually or automatically through integration into their tooling. The benefit of employing RAG is the insertion of only the gist of the available information, which allows picking more and only the most relevant information for the fix LLM context size. A drawback is that, based on the retrieval algorithm, not all relevant information may be retrieved. Further, fixing kreveals the advantage of controlling the result size. An alternative would be to return all chunks about a certain similarity threshold, introducing the question about the optimal cutoff. Figure 2 shows how the Discovery Agent extends on the RAG from Figure 1 shown in yellow hued. Instead of passing qto the RAG, the user submits it in1to the Discovery Agent, which then iteratively decomposes qinto a set of fine-grained tasks in 2. Breaking down the query into smaller, more manageable tasks can potentially fill the gap between the coarse semantics of the query and the specificities in the services documentation. In 3, the Discovery AgentAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 7 Figure2. Overview of the Discovery Agent Approach for Endpoint Discovery submits each task to the RAG to retrieve the set of relevant chunks to solve the current task specifically. Finally, in 4, the Discovery Agent collects the retrieval results of each individual task, filters them, and repeats 2ifqneeds further processing or returns the results to the user in 5. 3.2 OpenAPI Chunking Strategies A critical step in the RAG workflow is creating the chunks for the chunk database. registries can contain tens of thousands of services, each containing multiple potentially lengthy endpoints due to detailed descriptions or extensive input and output schemas. So, a single service might not fit into the context size of the embedding model or even exceed the limit of the LLM that further processes the output of the RAG system. In addition, service documentation can also feature additional metadata that, while valuable for understanding service details, is not necessarily relevant for composing services to solve a query. Table 1. Implemented Chunking Strategies Category Splitting Refinement Meta-Parameters Token-based No split Token chunking m(model), s(chunk size), l(overlap) Endpoint split Token chunking m(model), s(chunk size), l(overlap) Endpoint split Remove examples m(model) Endpoint split Relevant fields m(model) JSON split Token chunking m(model), s(chunk size), l(overlap) LLM-based Endpoint split Query m(model) Endpoint split Summary m(model) To determine advantageous chunking strategies, we employ the seven different and output is a list of chunks. The chunking strategies can be categorized into token-based andLLM-based strategies. Each strategy consists of a splitting method, which dissects the OpenAPI specification into a list of intermediate chunks, and another refinement step, which converts the intermediate chunks to the final list of chunks. In addition, there is the meta-parameter for the used embedding model m. For the token chunking refinement step, there is also the8 Pesl et al. chunk size sin tokens and their overlap l, i.e., how many tokens two consecutive chunks share, in tokens. For the token-based approaches, we consider three main splitting methods. Theno split method returns a single intermediate chunk for each OpenAPI containing the whole specification. The endpoint split divides the OpenAPI into one chunk per endpoint. The JSON split is a built-in LlamaIndex3splitting strategy tailored to files in the JSON format. This strategy parses the JSON file and traverses it using depth-first search, collecting leaf nodes, i.e., key-value pair where the value is a primitive type, e.g., strings, numbers, etc.. During this traversal, the parser concatenates keys and values into single lines of text to create a comprehensive textual representation of each leaf node. For the refinement, we implemented token chunking ,remove example , and relevant field . Thetoken chunking splits each intermediate chunk into a list of fixed-size chunks of stokens respecting an overlap of ltokens with the previous node. The remove example removes the requestBody and recursively all examples fields for each endpoint as these are typically lengthy but contribute little information. The relevant field extracts representative fields, i.e., service title, service description, endpoint verb, endpoint path, and endpoint description, which contribute much information but few tokens. For the LLM-based processing strategies, we apply the endpoint split and a summary (similar to [ 21]) andqueryapproach for refinement. In the summary approach, we prompt an LLM to generate a summary for each OpenAPI endpoint. For thequeryapproach, we instruct the LLM to generate a possible query matching the OpenAPI endpoint, as this might be closer to a possible input query than the summary. For both approaches, we only consider the LLM output for the embedding creation. The chunk content remains the original OpenAPI endpoint information. The no split and JSON split splitting methods can only be used with token chunking since all other refinement strategies rely on exactly one endpoint as an intermediate chunk. To evaluate the OpenAPI RAG and the Discovery Agent, we implement it as a fully operational prototype. Then, we employ the RestBench [ 35] benchmark to validate it in a real-world setting. We implement the OpenAPI RAG and Discovery Agent approaches as open- source prototypes based on the LlamaIndex library.3For the prototypes, we rely solely on OpenAPIs as the state-of-practice for service descriptions. All sources and results are available online.4 4https://doi.org/10.18419/darus-4605Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 9 For the OpenAPI RAG, we focus on the components presented in Figure 1. At the first start, the system loads the OpenAPIs and applies a chunking strategy to create chunks and their embeddings for their later retrieval. The chunks contain thereby the information from the OpenAPIs, e.g., a whole endpoint or a part of it. A chunk embedding does not necessarily have to match the chunks content; for example, the content can be the endpoint, and the embedding is created using a natural language summary of the endpoint. Thus, the matching is performed based on the embedding, and the result returned is the chunks content, which can include additional information not required for the matching process. As the service database, we use FAISS, which allows the storage and the similarity search of chunks [ 9]. We use a so-called QueryEngine from LlamaIndex for the chunk retriever, which allows us to query a chunk database based on textual We realize the Discovery Agent from Figure 2 using a LlamaIndex Ope- nAIAgent, which implements the LLM agent pattern for OpenAIs LLMs. An OpenAIAgent takes a list of tools, i.e., Python functions with a name and a description as parameters, and interacts with these using the OpenAI API. We implement two strategies for the tools. In the first strategy ( Query), we use the OpenAPI RAG as input for a LlamaIndex QueryEngineTool, which allows the agent to interact with the RAG on demand. This has the advantage of being a simple, straightforward implementation but may increase the token count as the results of the RAG are fed into the chat history, which is transferred to the LLM for the reasoning on this data. The second strategy ( Summary ) uses a RAG with chunks of the endpoints verb, path, and summary as contents and for their embeddings. We create the summary by instructing an LLM to create it based on the endpoint information, i.e., as in the summary chunking strategy. This should reduce the token count, as the chunks are much smaller, as not all endpoint details are returned and processed. To account for the same function as the first approach with the OpenAPI RAG and provide all information, we introduce a second tool, which takes the endpoint verb and path as input parameters and returns the whole endpoint information. The complete data is only inserted into the history for indispensable endpoints. To enable measuring the retrieved endpoints, we attach the endpoint infor- mation, i.e., verb and path, to each chunk as metadata. For the endpoint split splitting strategies, we take the information from the endpoint. For the other strategies, we first attach a list of all endpoints to the nodes before splitting and then filter on the endpoint paths in the final chunks after splitting. So, for each chunk, we know to which endpoint or endpoints it relates to. 4.2 Dataset and Metrics We evaluate our approach using the RestBench benchmark, covering the Spotify and TMDB OpenAPI specifications [ 35]. With 40 endpoints for Spotify and 54 for TMDB, this benchmark is much more complex than usual Service-Oriented Com- puting (SOC) case studies containing usually just three to seven endpoints [28].10 Pesl et al. Nevertheless, a holistic benchmark covering various domains is still missing (see RestBench contains 57 queries for Spotify and 100 for TMDB. For each of these queries, a solution set of endpoints is given, i.e., one to four endpoints that must be called to fulfill the query. For example, one query is Who directed the top-1 rated movie? The solution contains the endpoints GET /movie/top_rated As embedding models, we employ OpenAIs text-embedding-3-large [24] as one of the currently leading proprietary models. As open-source models, we utilize BAAI/ bge-small-en-v1.5 [39], which is relatively small while still producing reasonable results, allowing the model to be executed on commonly available hardware like laptops, and Nvidias NV-Embed-v1 [14] as one of the leading open-source models. For the LLM, we use OpenAIs gpt-4o-2024-05-13 . We evaluate the quality of the retrieved information in terms of accuracy and the token count of the returned result. We measure the accuracy using standard information retrieval metrics, namely recall =TP To correctly solve the query q, a service discovery approachs recall should ideally be maximal. However, this could result in retrieving several irrelevant services, turning into a drop in precision. The F1 score represents a balance between recall and precision. Table 2 shows the RestBench results for the OpenAPI RAG on the Spotify API. In recall, the JSON split method performs exceptionally well, especially with a high chunk size s, as this approach densely packs the information from the JSON into the chunks by removing all formatting. For precision and F1, the endpoint splitting approaches perform best because each chunk corresponds to precisely one endpoint. Differences between the models are minor, except that thebge-small-en-v1.5 performs worse for the no split approach. We also tested s= 2048and s= 4096, which are not reported here for space reasons. We show s= 1024because it is the default chunk size of LlamaIndex and s= 8191 because it is the maximum input token count for the OpenAI model. It is worth mentioning that with an increasing chunk size, the token size of the returned result also increases. Generally, a higher recall seems to correlate with a higher token count, e.g., no splitting with s= 1024, and l= 0has 4717tokens output on average. In contrast, the JSON split has 10056with the same parameters, but this needs further analysis. Due to length limitations, we cannot show the token count comparison and other values for top khere. We also tested top k= 5and topk= 20. Recall increases with a higher top k, but precision drops. Additional data is available in the complementary material.4 Table 3 presents the OpenAPI RAG RestBench results for the TMDB API. The TMDB OpenAPI is more complex in length and extent than the Spotify OpenAPI. In this case, the endpoint split-based approaches performs best inAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 11 Table 2. Results for the OpenAPI RAG for top k= 10with the Spotify API. The bge-small-en-v1.5 and NV-Embed-v1 , respectively. TC is for token chunking, RE for remove examples, and RF for relevant fields. Recall, precision, and F1 are in percent. The highest values per column are marked in bold. Category Splitting Refinement s lOSNOSNOSN Character No TC 1024 0674062161219261829 No TC 8191 0887191777121313 Endpoint TC 1024 0707576191920293131 Endpoint TC 8191 0737576191920293031 JSON TC 1024 08184859810171519 JSON TC 8191 09795100555101010 LLM Endpoint Query N/AN/A715758181515292324 Table 3. Results for the RAG for top k= 10with the TMDB API. Schema as in Category Splitting Refinement s lOSNOSNOSN Character No TC 1024 0114733173821511 No TC 8191 0301715195923811 Endpoint TC 1024 0404046201518272126 Endpoint TC 8191 0664759191214291923 JSON TC 1024 0444446181216261924 JSON TC 8191 0616050886141411 LLM Endpoint Query N/AN/A566546131510202417 Endpoint Summary N/AN/A69596516131529222412 Pesl et al. Table 4. Results of the Discovery Agent experiments. We set topk= 10and use OpenAIs text-embedding-3-large as the embedding model. Spotify and TMDB are the two test sets from the RestGPT benchmark. RAG are the results for the summary chunking strategy from the Tables 2 and 3. Query is the standard LlamaIndex QueryEngineTool to retrieve data from a RAG system. The summary is our approach with a QueryEngineTool for summaries and a details-on-demand fetcher. Accuracy values are in percent. #Token is the number of tokens per query averaged over all queries in the test set. The best value per row is marked in bold. Accuracy Recall71.92 63.70 66.4469.33 43.11 46.67 Precision 18.4267.3970.29 15.60 45.9750.97 #Token Prompt4233.658606.873125.21 41001.46 65699.75 4544.57 Completion 0.00262.30 256.26 0.00242.65 231.73 Total4233.658869.183411.47 41001.46 65942.40 4776.30 precision and F1. The no split approaches achieve high values in precision due to their low value of true positives. Overall, the endpoint split tends to outperform no splitting. The JSON splitting benefits Spotify as the endpoints are already very dense, i.e., the endpoints do not contain examples, and schemas are only referenced. Therefore, many endpoints can be condensed into one chunk. This approach performs much worse for the lengthier endpoints in the TMDB API. The summary refinement outperforms the query refinement, leading to the Discovery Agent. We present the RestBench results of the Discovery Agent in Table 4. For accuracy, we measure recall, precision, and F1 equally to the OpenAPI RAG experiments. For the token count, we measure the actual tokens sent from the agent to the LLM from the agent as prompt, the tokens received as completion , and their sum astotal. For the RAG approach, we accumulate the tokens of the retrieved The results show that both agent approaches improve precision and F1 but reduce recall. The Query approach increases the tokens in the prompt. Contrarily, in the total token count. The completion token count is by a magnitude smaller than the prompt token count for the agent approaches, which is relevant as completion tokens are usually more expensive than prompt tokens. No LLM is invoked in the RAG approach, so the completion tokens are zero.Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 13 We demonstrated the effectiveness of the OpenAPI RAG and the Discovery Agent using our implementation. They are able to retrieve large portions of relevant data while not revealing all relevant information in all cases. To address RQ1, we implemented the OpenAPI RAG to apply RAG for endpoint discovery with seven chunking strategies and numerous parameter combinations. We showed its effectiveness using the RestBench benchmark. Overall, the ability to adequately reduce the token size to fit into the LLM context size while maintaining most of the relevant information is exhibited by the prototype. Regarding the chunking strategies, endpoint split-based chunking strategies achieve favorable accuracies. Limitations are primarily that the RAG results may not contain all relevant information, and the precision is low due to the retrieval of exactly kchunks. Additional research is needed to improve the retrieval performance further and prove the results in a generalized setting across For RQ2, we introduced the Discovery Agent, which transfers the LLM agent pattern to endpoint discovery. Especially using Summary approach, the Discovery Agent showed strong improvement over the OpenAPI RAG in terms of precision, F1, and token count. Further research is needed to improve the decline in recall due to the processing through the LLM. While we rely on the research benchmark RestBench for our results, which covers two extensive OpenAPIs, queries, and ground truth, it is still limited to these two services. OpenAPI RAG systems in practice may operate on much larger datasets. For the data processing, we rely on standard RAG implementations like LlamaIndex, which are already designed to operate on large amounts of data. The performance evaluation, especially in larger real-world scenarios, remains open for future research. The applicability of the OpenAPI RAG depends on the availability of service documentation. We try to mitigate this issue by relying on widely adopted OpenAPI specifications, but this might not be valid for all domains. A solution to consider is automatically generating service documentation using an LLM. Another factor influencing the discovery is the quality of the OpenAPIs. The discovery may fail if no descriptions, meaningful naming, or erroneous information is given. This is not an issue of the approach, as a human developer would face the same problem, but it highlights the importance of high-quality documentation. strategies, e.g., CRAFT [ 42], could be added to the OpenAPI RAG. These could improve retrieval performance by combining multiple strategies or by creating a custom chunking strategy for a specific kind of service documentation. Another advancement could also be creating a custom embedding model tailored explicitly to service descriptions and service description chunks. This model may also be trained for one specific chunking strategy or intended use case. Additionally, the RAG output may be trimmed to boost precision. This could be done by, e.g., employing a similarity threshold.14 Pesl et al. The presented Discovery Agent could be further improved to handle whole service compositions. In this case, the agent would be extended by an additional component for the service composition, and the user would only submit their ser- vice composition task to the agent to retrieve the executable service composition Besides capabilities of the RAG system, resource consumption is a major issue in LLM-based systems. The OpenAPI RAG only uses embedding models. These are much more efficient than LLMs, resulting in costs in fractions of a cent per query. In contrast, the Discovery Agent requires significantly more resources, i.e., running RestBench in our experiments resulted in about $50 of API fees. Further work is needed to reduce this resource footprint. different ISs. With the application of automated LLM-based service composition approaches, the LLM input context limitations have become prominent, as the the preselection of relevant information. To address this issue, we proposed an OpenAPI RAG, which facilitates semantic search based on state-of-the-practice OpenAPIs and reduces the input token size. Further, we show an advanced integration through a Discovery Agent, which can retrieve service details on demand to reduce the input token count further. Our evaluation based on the RestBench benchmark shows that our approach is viable and performing. Limitations are especially in the restriction of RestBench to two services of the entertainment domain. We will address this in an extended version of this work. Further improvements are in optimizing the implementation and extending the agent for additional tasks, e.g., whole service compositions. We leave this for Acknowledgments. This work was partially funded by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) project Software-Defined Car (SofD- Car) (19S21002). The authors acknowledge support by the state of Baden-W rttemberg Disclosure of Interests. The authors Pesl and Aiello are listed as inventors of a patent [27], which covers automated service composition using LLMs for the automotive 1.Achiam, J., et al.: GPT-4 technical report (2023), https://arxiv.org/abs/2303. 2.AI@Meta: Llama 3 model card (2024), https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.mdAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 15 3.Baresi, L., Miraz, M.: A distributed approach for the federation of heterogeneous registries. In: ICSOC 2006. pp. 240 251. Springer (2006). https://doi.org/10. 4.Bohn, H., Golatowski, F., Timmermann, D.: Dynamic device and service discovery extensions for WS-BPEL. In: ICSSSM 2008. pp. 1 6. IEEE (2008). https://doi. 5.Cobbe, K., et al.: Training verifiers to solve math word problems (2021), https: 6.Cuconasu, F., et al.: The power of noise: Redefining retrieval for RAG systems. In: SIGIR. vol. 47, pp. 719 729 (2024). https://doi.org/10.1145/3626772.3657834 7.Curbera, F., et al.: Unraveling the web services web: an introduction to SOAP, WSDL, and UDDI. IEEE Internet Computing 6(2), 86 93 (2002). https://doi. 8.Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: NAACL-HLT 2019. pp. 9. Douze, M., et al.: The Faiss library (2024), https://arxiv.org/abs/2401.08281 10.Fan, A., et al.: Large language models for software engineering: Survey and open 11.Fikouras, I., Freiter, E.: Service discovery and orchestration for distributed service repositories. In: ICSOC 2003. pp. 59 74. Springer (2003). https://doi.org/10. 12. Gao, L., et al.: Pal: Program-aided language models. In: International Conference on Machine Learning. pp. 10764 10799. PMLR (2023) 13.Kim, M., Stennett, T., Shah, D., Sinha, S., Orso, A.: Leveraging large language models to improve REST API testing. In: ICSE. vol. 44, pp. 37 41 (2024). https: 14.Lee, C., et al.: Nv-embed: Improved techniques for training llms as generalist embedding models (2024), https://arxiv.org/abs/2405.17428 15.Lemos, A.L., Daniel, F., Benatallah, B.: Web service composition: A survey of techniques and tools. ACM Comput. Surv. 48(3) (dec 2015). https://doi.org/10. 16.Lewis, P., et al.: Retrieval-augmented generation for knowledge-intensive NLP tasks. In: NeurIPS. vol. 33, pp. 9459 9474. Curran Associates (2020) 17.Li, M., et al.: API-Bank: A comprehensive benchmark for tool-augmented LLMs. In: EMNLP. Association for Computational Linguistics (2023). https://doi.org/ 18.Liang, Y., et al.: Taskmatrix.AI: Completing tasks by connecting foundation models with millions of APIs. Intelligent Computing 3, 0063 (2024). https://doi.org/ 19.Mialon, G., et al.: Augmented language models: a survey (2023), https://arxiv. 20.Nakano, R., et al.: WebGPT: Browser-assisted question-answering with human 21.Nogueira, R., Yang, W., Lin, J., Cho, K.: Document expansion by query prediction 22.OpenAI: Function calling and other API updates (Jun 2024), https://openai.com/ index/function-calling-and-other-api-updates/ , last accessed 2024-07-18 23.OpenAI: GPT-4 Turbo in the OpenAI API. https://help.openai.com/en/ articles/8555510-gpt-4-turbo-in-the-openai-api (2024), last accessed 2024- 11-1916 Pesl et al. 24.OpenAI: New embedding models and API updates (Jan 2024), https://openai. com/blog/new-embedding-models-and-api-updates , last accessed 2024-07-18 25.Parisi, A., Zhao, Y., Fiedel, N.: Talm: Tool augmented language models (2022), 26.Patil, S.G., Zhang, T., Wang, X., Gonzalez, J.E.: Gorilla: Large language model connected with massive APIs (2023), https://arxiv.org/abs/2305.15334 27.Pesl, R.D., Klein, K., Aiello, M.: Verfahren zur Nutzung von unbekannten neuen Systemdiensten in einer Fahrzeuganwendung (2024), Patent DE102024108126A1 28.Pesl, R.D., St tzner, M., Georgievski, I., Aiello, M.: Uncovering LLMs for service- composition: Challenges and opportunities. In: ICSOC 2023 WS. Springer (2024). 29.Pesl, R.D., et al.: Compositio Prompto: An architecture to employ large language models in automated service computing. In: ICSOC 2024. Springer (2024) 30.Radford, A., Wu, J., Amodei, D., Amodei, D., Clark, J., Brundage, M., Sutskever, I.: Better language models and their implications. OpenAI blog 1(2) (2019), https: //openai.com/index/better-language-models/ , last accessed 2024-11-28 31.Radford, A., et al.: Improving language understanding by generative pre-training 32.Santana, J.M.S., Petrova, M., Mahonen, P.: UPnP service discovery for heteroge- neous networks. In: IEEE PIMRC. vol. 17, pp. 1 5. IEEE (2006) 33.Shi, Z., et al.: Chain of tools: Large language model is an automatic multi-tool 34.Soki, A.T., Siqueira, F.: Discovery of RESTful Web services based on the OpenAPI 3.0 standard with semantic annotations. In: AINA. pp. 22 34. Springer (2024). 35.Song, Y., et al.: RestGPT: Connecting large language models with real-world applications via restful APIs (2023), https://arxiv.org/abs/2306.06624 36.Th nes, J.: Microservices. IEEE software 32(1), 116 116 (2015). https://doi.org/ 37. Vaswani, A., et al.: Attention is all you need. NeurIPS 30(2017) 38.Wei, J., et al.: Chain-of-thought prompting elicits reasoning in large language models. NeurIPS 35, 24824 24837 (2022) 39.Xiao, S., Liu, Z., Zhang, P., Muennighoff, N.: C-pack: Packaged resources to advance general chinese embedding (2023), https://arxiv.org/abs/2309.07597 40.Yao, S., et al.: React: Synergizing reasoning and acting in language models (2023), 41.Yao, S., et al.: Tree of thoughts: Deliberate problem solving with large language 42.Yuan, L., et al.: CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets (2024), https://arxiv.org/abs/2309.17428'},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'text': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet? Abstract Hallucinations, the tendency to produce irrelevan- t/incorrect responses, are prevalent concerns in generative AI- based tools like ChatGPT. Although hallucinations in ChatGPT are studied for textual responses, it is unknown how ChatGPT hallucinates for technical texts that contain both textual and technical terms. We surveyed 47 software engineers and produced a benchmark of 412 Q&A pairs from the bug reports of two OSS projects. We find that a RAG-based ChatGPT (i.e., ChatGPT tuned with the benchmark issue reports) is 36.4% correct when producing answers to the questions, due to two reasons 1) limita- tions to understand complex technical contents in code snippets like stack traces, and 2) limitations to integrate contexts denoted in the technical terms and texts. We present CHIME ( ChatGPT Inaccuracy Mitigation Engine) whose underlying principle is that if we can preprocess the technical reports better and guide the query validation process in ChatGPT, we can address the observed limitations. CHIME uses context-free grammar (CFG) to parse stack traces in technical reports. CHIME then verifies and fixes ChatGPT responses by applying metamorphic testing and query transformation. In our benchmark, CHIME shows 30.3% more correction over ChatGPT responses. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT Index Terms ChatGPT, Hallucination, Software Issue Reports The reliability of LLMs is often questioned due to their tendency to produce nonsensical or incorrect outputs, a phe- nomenon commonly referred to as hallucination [1], [2], [3]. Like any LLM, ChatGPT can also suffer from hallucination issues like inconsistency in responses [4], [5] or factual inaccuracies. These problems can arise even when the model is provided with the context as a document/paragraph. While progress is made to assess hallucinations in textual data [6], [7], we are not aware of how hallucinations can be detected and mitigated for software technical reports that contain both textual and technical terms (e.g., crash dumps, code snippets, This paper studies the detection and mitigation of ChatGPT inaccuracies in technical reports. We pick software bug reports for our study, because bug reports often contain a blend of descriptive text, technical terminology, code references, and snippets of crash/system dumps [8]. These documents are crucial for tracking and resolving software issues but can be overwhelming due to their volume and complexity [9]. An AIchatbot, trained to understand these reports, may streamline the process by extracting information. But for that, first we need to ensure that the responses from the chatbot are correct. In the first phase of our study , we conducted a survey of 47 software engineers to understand the types of questions they ask while exploring bug reports and for which they wish for an automated Q&A tool like a chatbot. We found that developers ask diverse questions during bug exploration, which we could group into five types: 1) issue analytics, 2) issue trends, 3) issue summary, 4) issue labeling, and 5) issue backlogs. Based on the survey findings, we produced a benchmark of 412 Q&A pairs by consulting our industry partner (with whom we conducted regular bi-weekly sessions) and the literature. The Q&A pairs are collected by analyzing the issue reports of two popular open-source software (OSS). In the second phase of our study , we tuned ChatGPT with issue reports from the two studied OSS based on the Retrieval Augmented Generation (RAG) techniques [10], [11], [12], [13]. We then evaluated the correctness of ChatGPT re- sponses against our benchmark. Each question was asked and compared automatically to its expected answer. Correctness was assessed as the ratio of questions whose answers were found as correct. We found that our RAG-based ChatGPT was correct in only 36.4% cases. For the rest of the questions, it hallucinated by producing incorrect or irrelevant answers. We manually examined each hallucination case and identified two limitations in ChatGPT to process technical documents like bug reports: 1) limitations to understand complex technical contents in code snippets like stack traces (e.g., when a partial code snippets/crash dump is provided and the question is about determining the cause of the crash by assessing both the crash dump and the textual contents), and 2) limitations to integrate contexts denoted in the technical terms and texts (e.g., when ChatGPT was required to assess the relationships among multiple metadata and the technical terms). In the third phase of our study , we designed CHIME (ChatGPT Inaccuracy Mitigation Engine) to address the above two limitations. The underlying principle in CHIME is that (1) if we can preprocess the technical reports better and store information relevant to an issue report as a combination of metadata and actual contents and (2) then guide the query validation process in ChatGPT with guided iterative prompting approaches, we can address the observed limitations. The 11 Nov 2024TABLE I: Demography of Survey Participants Current Role 0-3 4-5 6-10 11-15 16-20 Total Developer 13 20 4 2 - 39 QA Engineer 1 1 - - - 2 Project Manager - 1 - - - 1 Other 2 - 2 - 1 5 Total 16 22 6 2 1 47 (a)Resolution Triaging Reporting Management89.36% Fig. 1: (a) Frequency & (b) Reasons of Bug Report Exploration usage of metadata is found to improve LLM search capabilities [14]. For us, such metadata could be generated by organizing the mix of textual and technical contents into a structured form. A challenge was how to separate the textual and technical contents and process the code terms within a crash dump and then organize those within a structure. We introduce a novel context-free grammar (CFG) in CHIME to efficiently parse stack traces in technical reports. As for the second principle (i.e., guided prompting for verification), we extended recent similar work on textual content. CHIME verifies and fixes ChatGPT responses by using query transformation [15] and by extending CoVe [16] with metamorphic testing (MT) [17]. CoVe is a zero-shot iterative prompting-based query verifi- cation technique. We evaluated CoVes response by further mutating the question using MT, because CoVe may discard correct responses or promote incorrect responses. We evaluated CHIME using our benchmark. CHIME shows on average 30.3% improvement over ChatGPT responses by offering more correct answers. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT without CHIME. Our replication package (https://bit.ly/4fyaMIP) contains all the data and code developed in the study. II. R ELATED WORK A. Hallucinations in Large Language Models Extensive studies in the literature identified the causes of hallucinations as sub optimal training, inference [18], [19], [20], [21], [22], and insufficient/low-quality data [23], [24], [25]. Techniques such as bidirectional auto-regressive models [26] and attention-sharpening mechanisms [27] have been developed to address training-related hallucinations. Inference issues, primarily due to decoding strategies, often result in in- accurate outputs. Strategies like factual-nucleus sampling [28]and in-context pretraining [29] are implemented to mitigate these inaccuracies. Challenges posed by flawed data sources introduce biases and inaccuracies into models, stemming from misinformation, duplication biases, and social biases in the training datasets. Mitigating data biases involves manual dataset creation [30], integrating high-quality sources such as the Pile [31], and up-sampling factual data [32]. Furthermore, knowledge editing [33], [34] and Retrieval-Augmented Gen- eration (RAG) [35], [11], [12], [13] are employed to bridge knowledge gaps, utilizing external sources for more accurate Our study utilizes RAG-based ChatGPT for technical bug report understanding. We enhance ChatGPTs knowledge base by integrating it with a database of bug reports through RAG methods. We then develop CHIME, which refines both the preprocessing of input data and the validation of RAG-based B. LLMs for Software Engineering In recent years, the application of LLMs has been widely utilized in Software Engineering (SE) tasks, ranging from code analysis to bug detection [36], [37], [38], [39], [40], [41], [42], [43], [44]. Encoder-only models like BERT [45] and its derivatives, including CodeBERT [46] and GraphCodeBERT [47], excel in processing code. While encoder-decoder models, such as T5 and PLBART excel in understanding semantics of code for tasks like code summarization [48], [49]. Decoder- only models like the GPT series and specialized versions like CodeGPT and Codex generate direct responses from prompts. However, the specific challenge of mitigating inaccuracies in software technical reports remains unexplored. Addressing this gap, we introduce CHIME to reduce inaccuracies in ChatGPT- generated responses during bug report exploration. III. AI C HATBOT NEEDS FOR BUGREPORT EXPLORATION To evaluate a chatbot on software technical documents, we needed a benchmark, which at the time of our study was not available. We thus adopted a systematic approach to create such a benchmark. First, we conducted a survey of software developers to produce a catalog of questions that they ask during bug reports. Second, we used the catalog to produce our benchmark (see Section IV). This section discusses the survey, which answers the following research question (RQ): RQ1. What types of questions would software practitioners like to ask a chatbot during bug report exploration? We employed the snowball sampling approach [50] to recruit participants, resulting in 47 software practitioners. The majority of respondents (83%) held developer roles. The largest proportion of participants (55%) reported having 4- 5 years of experience in the field. Table I illustrates the distribution of participants across their roles in the software industry and years of experience. We ensured that all se- lected participants regularly use Issue Tracking Systems (ITS), such as Jira, GitHub, or in-house systems for several keyTABLE III: Key Questions (KQ) of the Survey. 1 How would you like to utilize a chatbot during bug 2 Would you want to use the chatbot for T#? C 3 Rate the usefulness of T# for your work C 4 Rate the potential usefulness of the following bench- mark questions of T#C TABLE II: Identified Task Types for Bug Report Exploration T# Task Title Task Description T1 Issue Analytics Explores complex details within/across bug reports, including technical jargon, error codes, and contextual nuances, etc. T2 Issue Trends Patterns and trends in bug occurrences. T3 Issue Summaries Summaries of key topics discussed in an issue or across multiple issues. T4 Issue Labeling Inquires labels for bug reports to help orga- nize and categorize them effectively. T5 Issue Backlog Explores whether the issue remains open for purposes. Around 89.36% of respondents used ITS for issue resolution addressing and for solving bugs; followed by issue triaging which involves prioritizing and assigning issues. Project Management and documenting issues for records or stakeholder communication were also noted. A vast majority of respondents (70.21%) reported engaging with issue reports daily. Figure 1a presents the distribution of issue report inter- action frequency and Figure 1b illustrates the survey responses for the primary reason for issue report exploration. Before the main survey, we consulted two software profes- sionals from a reputed software company to get insights on the potential tasks that an AI chatbot can support during bug report understanding. Each had 16 and 35 years of experience respectively. We finally settled on five distinct tasks as shown in Table II). For our survey, we formulated questions to address During the survey, participants were prompted to envision an AI chatbot with comprehensive access to bug reports. Subsequently, they were asked whether they would like to utilize the chatbot for the specified tasks (T#) and to rate the perceived usefulness of each task for their work. We condensed the key survey questions into Table III for clarity. All survey questions can be found in our online appendix. C. Understanding Preferences of Software Practitioners on Identified Task Types (RQ1) In Table III, with Key Question 1 (KQ1), we tried to identify the desired chatbot roles during bug report explorations. With KQ2-4, we examined how participants perceived the useful- ness of each task type. The questions are summarized in TableIV. For each question, we show quantitative evidence from the survey about what the participants thought of the usefulness 1) KQ1. Desired Chatbot Roles: We asked KQ1 as an open- ended question to check whether the participants considered the same five task types that we identified in Table II. As such, we did not show them KQ2-4 until they answered KQ1. Participants desired a tool whose capabilities closely aligned with our predefined tasks (T1 T5). They emphasized the chat- bots potential to analyze issues (T1), such as extracting pivotal details like exceptions or log entries from similar bug reports. According to R43: Queries for potential duplicate issues could be really helpful. The capability to analyze trends (T2) within reported issues to uncover recurring problems was seen as crucial. Participants also noted the importance of the chatbots ability to summarize (T3) and categorize (T4) issues. The survey responses indicated a significant interest in features that would allow users to query backlogged items (T5), as R43 stated: Queries for issues with very little recent activity and no clear resolution would be super helpful. Apart from these tasks, participants also expressed desires for additional capabilities such as sorting issues based on priority, severity, or difficulty and forecasting resolution times using historical data. We leave support for those tasks as our future work. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 2: (a) Interest for T1 (b) Usefulness Perception of T1 2) KQ2-4. Issue Analytics (T1): The survey results reveal a strong preference for the chatbots analytical capabilities, particularly in the context of analyzing multiple issues, with more than 80% of participants expressing interest in utilizing the chatbot for detecting similar or duplicate issues and finding it useful; while 72.34% participants value the chatbots utility in analyzing individual issues as illustrated in Figure 2. In Table IV, we show 10 questions under T1 that each par- ticipant assessed. On average, 73.62% of participants marked these useful. The capability to identify and summarize stack traces (Q1.1) within the single issue analysis domain was highly valued, evidenced by a utility score of 78.72% and remark from respondent R11, Summary of stack trace is a good idea . In comparison, the importance of determining the environment linked to an exception (Q1.2) was rated lower, at 57.45%. For the analysis of multiple issues, the ability to find similar issues (Q1.3) was highly valued at 82.98%, as quoted by R09, They are all extremely time-consuming when done manually. A chatbot will definitely help with this. 0 20% 40% 60% 0No Maybe Yes 0 20% 40% 60% 80% 0Not Useful Neutral Useful Fig. 3: (a) Interest for T2 (b) Usefulness Perception of T2TABLE IV: Perceived Usefulness of Benchmark Questions Presented in the Survey. Not Useful Neutral Useful Q# Question Perceived Usefulness of T# (KQ4) T1 - Issue Analytics Extracting Information from Issue Details or Find Similarities Among Issues Q1.1 Is there a stack trace provided in issue 123, and can you summarize it? Q1.2 Where in the code does the exception in issue 123 occur? Q1.3 What is the exception reported in issue 123? Q1.4 How many tests failed as reported in issue 123? Q1.5 Which environment is associated with the exception reported in issue 123? Q1.6 Are there any issues similar to issue 123? Q1.7 Find duplicate reports of the X error (or other) in A module Q1.8 Find all similar issues related to X failures] Q1.9 Identify any performance degradation issues reported on last month Q1.10 Has there been a report of a crash on a X machine running the A Test recently? T2 - Issue Trend Detect and Analyze the Trends and Patterns among Issues Q2.1 What are the frequently encountered errors in the nightly builds? Q2.2 What are the recurring themes in bug reports post the latest OS update? T3 - Issue Summary Obtain a Comprehensive Overview of Reports Selected by Different Criterion Q3.1 List all issues related to an X feature and their current status Q3.2 Generate a report detailing the distribution of issues across different project modules Q3.3 Compile a summary of unresolved issues not older than 60 days Q3.4 Can you generate a summary of all issues tagged as bug in the last 30 days? Q3.5 Create a summary of user-reported issues versus internally identified issues T4 - Issue Label Provide Suggestions for Categorizing and Tagging Issues with Appropriate Labels Q4.1 Suggest existing labels to tag issue 123 Q4.2 Can you recommend labels for performance-related issues? T5 - Issue Backlog Analyze Unresolved Issues Reported but not yet Addressed Q5.1 Are there any long-standing issues that have been consistently postponed? Q5.2 Find issues that have not been assigned to any milestone but are older than 60 days Q5.3 List issues that have missed two or more release cycles Q5.4 Identify issues with no activity in the last 30 days. 3) KQ2-4. Issue Trend (T2): 87.2% of participants ex- pressed interest in using this feature and 85.1% found the corresponding questions on identifying and analyzing trends within bug reports useful (see Figure 3). In Table IV, we show two questions that we asked under this task. Both received a favorable response. Respondent R31 noted, By focusing on recurring errors and themes, these questions provide valuable insights that can guide decision-making, resource allocation, and issue resolution efforts. When participants were asked about the utility of chatbots in identifying frequently encoun- tered errors in the nightly builds of their development envi- ronment (Q2.1), e.g., in CI/CD pipelines, 76.60% perceived this functionality as useful. 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 4: (a) Interest for T3 (b) Usefulness Perception of T3 4) KQ2-4. Issue Summary (T3): 80.6% of participants were keen on a chatbot to produce summaries of issues and 70.2% considered the asked questions useful for efficiently understanding and resolving software issues. In Table IV, regarding the chatbots ability to report on how issues are distributed across different project modules (Q3.1), 76.6% found this function useful. On the other hand, the feature for distinguishing between user-reported and internallyidentified issues (Q3.2) was seen as useful by 51.06% of participants, indicating a notable but more moderate interest in differentiating the sources of issues. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 5: (a) Interest for T4 (b) Usefulness Perception of T4 5) KQ2-4. Issue Labeling (T4): 70.2% of participants are interested in leveraging chatbots for the task of issue labeling (see Figure 5). However, its worth noting that this task re- ceived the lowest percentage of perceived usefulness (61.70%) compared to others. Regarding the chatbots ability to suggest appropriate labels for an issue (Q4.1) and to recommend labels for performance-related issues (Q4.2), about 68% of respondents considered these features to be useful. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 6: (a) Interest for T5 (b) Usefulness Perception of T5 6) KQ2-4. Issue Backlog (T5): The management of Issue Backlogs is an essential aspect of software development. R16 highlighted the challenge: Sometimes change of prioritiespushes issue out of find and stay unresolved for days. So it is good to find out long-running or inactive issues. Despite its importance, this task garnered the least interest (63.8%) among all tasks for potential chatbot utilization. Notably, T5 records the highest percentage of No responses (17.02%) regarding interest and Not Useful perceptions (19.15%). 70.2% of respondents see value in identifying long-standing, postponed issues (Q5.1) but interest slightly drops to 61.70% for detecting issues with no recent activity over the last 30 Summary of RQ1. When examining software practition- ers preferences for AI chatbot capabilities in exploring bug reports, the identification of similar issues and the analysis of recurring error trends were highly favored. In contrast, capabilities related to categorizing issues and handling pending bugs were deemed less critical. IV. A B ENCHMARK OF Q&A P AIRS TO EVALUATE AI CHATBOTS FOR BUGREPORT EXPLORATION In Table IV, we showed a catalog of 23 questions that we validated with our survey participants and for which they wished for chatbot support. Each question is a template, which can be used to produce multiple similar questions.Based on the question templates in Table IV, we produced a total of 412 questions from the issue reports of two popular OSS repos, OpenJ9 and ElasticSearch. OpenJ9 was chosen due to its alignment with our industrial partner. ElasticSearch [51], [52], [53] is frequently referenced in academic studies. We then produced an answer to each question by assessing the two OSS repos and by consulting among the authors. Four authors (the first two and last two) engaged in many hours of discussions that spanned over six months (both in- person and over formal presentations). The last two authors are also among the maintainers of OpenJ9. Given the benchmark was created via mutual discussion, we did not compute any standard agreement analysis metrics. We created the benchmark by selecting 80 complex issues (40 from each repository). Following Deeksha et al. [9], we define an issue as complex if it is excessively long and/or it has stack traces. We picked issues within the last year of our analysis because those are likely to be explored more by developers. We sorted issues by length and selected 40 issues with stack traces (per repo). Following the standard chatbot evaluation process, we contained three types of answers: binary [54], factual [55], [56], and summary [57]. The binary (i.e., Query Type = Y/N) queries have ques- tions with answers as Y/N. These are designed to assess the chatbots accuracy in identifying clear-cut, definitive binary decisions based on information available in bug reports; such as the presence of a particular error code or the applicability of a specific scenario. To verify a chatbot response for these queries, we simply need to check for Y/N in their responses and match those against the benchmark answer.Q1. Type: Y/N. Source: ElasticSearch Question: Is there any issue similar to issue 100071? The factual (i.e., Query Type = Factual) queries assessed the chatbots ability to extract concrete information from bug reports, such as identifying, retrieving, and presenting specific details from the dataset, such as error messages, stack traces, configuration settings, etc. Like binary queries, this method also allows for a straightforward assessment of the chatbots accuracy, and thus direct matching can be used for verification. Q2. Type: Factual. Source: ElasticSearch Question: What existing label is recommended for issues that need immediate triaging? The summary-based (i.e., Query Type = Summary) queries challenge the chatbot to engage in deeper analysis and syn- thesis of data. These queries require the chatbot to identify patterns and even to propose potential solutions based on the analysis of multiple data points. Since these queries demand a synthesis of information and provide insights or summaries, we need a similarity analysis between a response and the expected answer for verification. Q3. Type: Summary. Source: ElasticSearch Question: Summarize similarities between issues 103072 Expected Answer: Issues 103072 and 103344 both involve test failures within the LearningToRankRescorerIT class. The root cause of these failures stems from a named object notfound exception and xcontent parse exception, resulting in ... TABLE V: Distribution of Benchmark Questions over Survey- identified Tasks from Table IV. T# Y/N Fact Summarization T1 - Issue Anlys(S)48 140 24 212 T1 - Issue Anlys(M)12 20 8 40 T2 - Issue Trend16 16 8 40 T3 - Issue Summary8 32 40 T4 - Issue Labeling12 20 8 40 T5 - Issue Backlog12 24 440 Table V shows the distribution of question types Yes/No, Fact, and Summarization across OpenJ9 and ElasticSearch, totaling 206 questions per project. OpenJ9 and ElasticSearch have a similar overall structure, with a strong emphasis on factual questions (114 for OpenJ9, 114 for ElasticSearch), followed by binary (yes/no) and summarization questions. Our online appendix contains details about each of the 412 questions and how each question maps to our catalog of 23 survey questions.V. E FFECTIVENESS OF CHATGPT ON THE BENCHMARK In this section, we answer the following research question: RQ2. How effective is a RAG-enhanced ChatGPT to answer to the benchmark questions while exploring the corre- The RAG architecture combines ChatGPT with an external knowledge retriever to provide responses to queries. This framework utilizes external database sources, primarily issue reports with structural data and metadata fetched by the GitHub API. Figure 7 illustrates the pipeline for this. It functions by first retrieving pertinent information from the database based on the input query. This step is crucial as it aligns the models focus with the most relevant data. Then, the augmented data from the retrieval step are combined with the inherent generative capabilities of ChatGPT to help ChatGPT provide high-quality responses. We used ChatGPT 3.5-turbo within LangChain framework [58] to implement this pipeline. We used a temperature setting of 0. A temperature value above 0 produces slightly different answers to a prompt across multiple runs, which is unnecessary when we expect consistent answers from ChatGPT. We ran it multiple times on our benchmark dataset to ensure that the answers were indeed consistent across multiple runs. Fig. 7: Pipeline of the RAG-based ChatGPT We measure the effectiveness of the above RAG-based ChatGPT on our benchmark by calculating correctness (C): C=Number of Correct Responses Total Number of Queries 100% (1) Correctness analysis involved comparing the generated re- sponses against the predefined correct answer for a query in the benchmark. We adopted the following automated approaches to measure the correctness of the responses for the three types of queries in our benchmark (i.e., Y/N, Fact, and Summaries). For evaluating yes/no responses, we used a zero-shot clas- sification approach, enabling us to automatically determine if detailed answers from the chatbot align with a simple Yes or No expectation. For evaluating factual query responses, we combine two approaches: direct comparison of extracted information (such as issue numbers) and semantic similarity assessment for non-listed facts. First, we extract and compare key factual elements. If the response and expectation directly match or share common elements, the correctness is assessed accordingly. For responses without explicit factual elements, we utilize the SentenceTransformer library, employing the all- MiniLM-L6-v2 model [59] to encode the actual and expected answers into embeddings. Subsequently, we gauge the seman- tic similarity between these embeddings by computing thecosine similarity [60] and assessing how closely the actual answer aligns with the expected fact, considering nuances in phrasing and context. For evaluating Summarization queries, we compute the semantic similarity like before between the actual summary provided by ChatGPT and the expected sum- mary. Based on empirical observations (see Section VIII-B), we used a similarity threshold of 0.7. The RAG-based pipeline achieved 36.4% accuracy in our benchmark (see Table VI). We manually assessed each of the 262 incorrect answers to determine the causes of its incorrectness. Given that ChatGPT is a black-box model, our assessment is based on the nature of the questions asked and the provided answers. We observed two limitations in L1. Limitations in Understanding Complex Technical Content L2. Contextual Understanding Challenges (57.3% cases). TABLE VI: Correctness of RAG Based ChatGPT T# Y/N Fact Sum Total Total 49.0% 30.7% 36.9% 36.4% T1 - Issue Anlys(S) 41.7% 32.1% 33.3% 34.4% T1 - Issue Anlys(M) 50.0% 15.0% 0.0% 22.5% T2 - Issue Trend 50.0% 12.5% 37.5% 32.5% T3 - Issue Summary 0.0% 75.0% 43.8% 50.0% T4 - Issue Labeling 58.3% 40.0% 62.5% 50.0% T5 - Issue Backlog 66.7% 25.0% 25.0% 37.5% The challenge in understanding technical contents (L1) occurred when ChatGPT faced challenges in deeply under- standing and accurately processing highly technical content, particularly when it involved intricate programming/technical concepts. For example, when asked to provide the root location of the exception encountered in issue 18151; it gives a non- useful answer; even though the issue provides a stack trace. Q4. Asking ChatGPT - OpenJ9 Question: Where in code exception of issue 18151 occur? Incorrect Answer: The exception of issue 18151 occurs in the body of the issue. Again, for asking to find the line number where the error occurs, ChatGPT just gives an irrelevant answer by showing Q5. Asking ChatGPT - OpenJ9 Question: Which line in CharacterDataLatin1 class trig- gered ArrayIndexOutOfBoundsException in issue 17063? Irrelevant Answer: The line of code that is The Contextual Understanding Challenges category (i.e., L2) shows a fundamental difficulty in ChatGPT to processand respond to queries about technical issues. This category primarily involves the models struggles with: Integrating and interpreting the context in which queries are made. Some context is explicitly stated within the query or the referenced issue, such as a specific error message or stack trace. Other times, the context is implicit, requiring the model to infer based on its broader knowledge or related data points. Handling ambiguous or insufficiently detailed queries necessitates the chatbot to fill in the gaps with assumptions or inferred knowledge. Not all contextual in- formation holds equal relevance to a given query, requiring the chatbot to prioritize the most pertinent context based on the nuances of the query. Adapting to the technical conventions of specific domains. Technical domains often have their own conventions for documentation, communication, and issue tracking. For instance, understanding that a particular label in an issue tracking system denotes the responsible team, requires domain-specific knowledge that the AI must possess. On asking to find a responsible team for an issue, ChatGPT searches on the assignee list, but the team details are on the issue labels. Due to this lack of contextual information, it fails to answer the question. Q6. Asking ChatGPT - ElasticSearch Question: Which team is responsible for issue 104160? Incorrect Answer: The team responsible for issue 104160 is not specified in the database. Even when the relevant context is identified, retrieving and applying it accurately to generate a response is challenging. This includes understanding the specific ways in which information is structured or presented within data sources and how it relates to the users query. Effectively bridging this gap is crucial for generating accurate and contextually Summary of RQ2. A RAG-based ChatGPT showed an average correctness of 36.4% on our benchmark. The pipeline encountered challenges in comprehending com- plex technical content and grasping contextual nuances, leading to inaccuracies in its responses. VI. CHIME: C HATGPT I NACCURACY MITIGATION Our observations in Section V of ChatGPT limitations contributed to the design of CHIME, as a suite of techniques to detect and fix incorrectness in ChatGPT responses. The underlying principle of CHIME is that by offering ChatGPT with a more structured representation of bug reports and by applying a systematic approach to assess ChatGPT responses, we can address the two limitations we observed in Section V-B. A more structured representation of bug reports can be achieved if we can process the different technical and textual terms properly and store those in a structured way, e.g., in a database with metadata offering more information about thoseterms. A systematic approach to verify the responses can be achieved by applying/adapting the techniques of guided itera- tive prompting of LLM responses that are used in the literature for textual content. As such, we designed to preprocess the inputs (both the bug report and the query) and to verify the CHIME treats ChatGPT as an API, where the inputs (ques- tions) and outputs (answers) are processed for inaccuracy detection and mitigation. We can use another LLM as an API in CHIME and apply all the techniques we developed. Doing so would simply require changing the API endpoints to point to the other LLM within the LangChain toolkit. CHIME will need to be updated significantly while using multi-modal LLMs, e.g., to process/validate modalities other than texts, etc. Fig. 8: The Architecture of CHIME In Figure 8, we show the architectural diagram of CHIME. We apply an Issue Preprocessor component to parse crash dumps and other technical terms. Issue Preprocessor employs Context-Free Grammars (CFGs) to interpret complex technical data, addressing ChatGPTs limitations in understanding com- plex technical content. To address the second limitation (i.e., Contextual understanding challenges), we introduce two more components in CHIME. First, we apply Query Preprocessor component to decipher users intents. Second, we designed the Response Validator component, which evaluates the accuracy in a response using a combination of two techniques: Chain of Verification (CoVe) [16] and Metamorphic Testing (MT) [17]. A query goes through each of these components. We discuss We developed a CFG as shown in Listing 1 which sup- ports parsing stack traces to the level of individual code elements. Stack traces can also contain auxiliary information like timestamps and memory addresses (e.g., crash dumps). We preprocessed these auxiliary information and applied our CFG to parse the code elements in a Java stack trace. Finally, our issue preprocessor stored the code blocks in the database by separating the code blocks from the textual contents. The issue preprocessor saves the following information by parsing an issue report: title, body, stack trace, and other metadata(creation date, status, etc). When ChatGPT, enhanced with Issue Preprocessor, is presented with the query regarding the type of exception in issue 18151 (Q4) or 17063 (Q5), it correctly identifies the location of the exception detailed in Listing 1: CFG Grammar for Stack Traces 1 Root ::= StackTraceElems 2 StackTraceElems ::= StackTraceElem StackTraceElems | 3 StackTraceElem ::= ExceptionElems |CodeDetails 4 ExceptionElems ::= ExceptionElem ExceptionElems | 5 ExceptionElem ::= ExceptionType |ExceptionMessage 6 CodeDetails ::= ClassElem |MethodElem |FileElem | Asking CHIME with Q4 from Section V. OpenJ9 Question: Where in the code exception of issue 18151? Correct Answer: The exception in issue 18151 can be triggered at line 98 in the JvmErgonomics.java file. Asking CHIME with Q5 from Section V. OpenJ9 Question: Which code in CharacterDataLatin1 triggered the ArrayIndexOutOfBoundsException in issue 17063? Correct Answer: Line 72 in the CharacterDataLatin1 A user query may not provide enough details, leading ChatGPT to generate responses based on faulty assumptions or context. We leveraged the query transformation algorithm from [15] that strategically rephrases and modifies a query. Each query goes through this component for error correction as follows. We prompt ChatGPT with instructions on how to transform a query based on the question type. For example, for a Yes/No query type, we instruct ChatGPT as follows Change is/are/have there issues to check if there are any issues with the provided condition. For a summarization type question, the instruction is Summarize the contents from issue title, exceptions, body, and labels. We then provide some example transformations (in a few-shot setting) to clarify the instruction, like the following. Original Query: Are issue 18102 and 18669 similar? Transformed Query: Compare the exceptions, stack traces, and descriptions of issues 18102 and 18669 to Finally, GPT answers based on the transformed query. For example, when presented with the query, Q6 from Section V, the Query Preprocessor directs the question with additional in- structions: Summarize the team responsible for issue 104160 based on the assignee and labels. This instruction results in generating a more precise SQL query to our database (we used LangChain SQL engine + RAG ChatGPT pipeline).Asking CHIME with Q6 from Section V. ElasticSearch Question: Which team is responsible for issue 104160? Correct Answer: The team responsible for issue 104160 is the ML team, with no specific assignee and labels indicating it is related to ML. The third component of CHIME is added to validate ini- tial responses from RAG-based ChatGPT. We employed two strategies sequentially, CoVe [16] and Metamorphic Testing (MT) [17]. Drawing inspiration from investigative interroga- tion techniques, CoVes methodology is designed to test the consistency and authenticity of ChatGPTs responses through a series of challenge questions. CoVes process can be unfolded into three distinct stages 1) Upon receipt of a query, ChatGPT formulates an initial response based on its interpretation of the query within the context of available data. 2) CoVe then autonomously generates a set of follow-up questions related to the initial response. CoVe dynamically determines the number of follow-up questions. These questions aim to clarify its preceding statements. 3) The subsequent responses are ana- lyzed for consistency with the initial response. Any observed contradictions signal the need for a review and adjustment of the initial response, leading to the refinement or generation of a new, more accurate answer. Listing 2: How CoVe validates in CHIME Initial Question: Is issue 18102 and 18669 similar? Response: No, issue 18102 and 18669 are not similar. Follow-up Questions Generated by CoVe 1. Can issue 18102 and 18669 be considered similar based on their descriptions and reported problems? 2. Have there been any interactions or dependencies between the teams working on issue 18102 and 18669 that could 3. Are the issues 18102 and 18669 related to the same error? ChatGPT Responses to Follow-up Questions 1. Yes, based on the descriptions and reported problems, they can be considered similar as both involve 2. Yes, there have been interactions between the teams working on issue 18102 and 18669. 3. No, issues 18102 and 18669 are not related to same error. Final Response: Yes, issue 18102 and 18669 are similar. However, CoVe can discard correct responses (if not found in responses to challenge questions) or promote incorrect responses (if responses are found both in base and challenge questions). We thus refine CoVe with MT as follows. Using CoVe to verify ChatGPTs initial responses, we get an intermediate response which is then passed for consistency verification to MT. Central to MT is the concept of metamor- phic relations (MRs), which involve applying transformationsto input queries to anticipate changes in output. In CHIME, this approach is leveraged to introduce nuanced variations to the queries for testing the consistency of its responses under diverse conditions. A specific implementation of MRs in CHIME involves Sentence-Level Mutation, where equivalent questions are generated to test. This process includes 1) using predefined MRs to subtly alter the phrasing or focus of the original queries. In our implementation, MT generates three mutated questions each time in CHIME. For instance, the original query might be What causes error X in module Y?, and its mutated equivalent could be Why does error X occur in module Y?. 2) The responses ChatGPT provides for both the original and mutated queries are compared to assess consistency with MR expectations. If an MR implies that adding specific details should refine the response, the eval- uation checks for this level of refinement. After the refinement via MR, CHIME generates the final response for the users. The user in CHIME does not see the mutated questions from CoVe or the mutated questions from MRs. Instead, the user only sees the final response from CHIME. However, CHIME can print the mutated questions to the user if needed. Listing 3: How CoVe + MT validates in CHIME Initial Question: Does Elasticsearch require the UseG1GC option to be present during issue 18151 startup stage? Initial ChatGPT Response: No, Elasticsearch does not require the UseG1GC option to be present during its startup stage in issue 18151. Follow-up Questions Generated by CoVE : 1. Is Elasticsearch the software mentioned in the response? 2. Is the UseG1GC option not required? 3. Is issue 18151 not requiring the UseG1GC option? ChatGPT Responses to Follow-up Questions : 1. Yes, Elasticsearch is mentioned in the response. 2. No, the UseG1GC option is not required. 3. No, issue 18151 is not requiring the UseG1GC option. CoVE : No, the UseG1GC option is not required during Mutated Questions Generated by MT : 1. Is the UseG1GC option necessary for Elasticsearch to be present during its startup stage in issue 18151? 2. Must the UseG1GC option be included during Elasticsearch s startup stage for issue 18151? 3. Is it required to have the UseG1GC option present during Elasticsearchs startup stage for issue 18151? ChatGPT Responses to Mutated Questions : 1. No, the UseG1GC option is not necessary for Elasticsearch s to be present during its startup stage in issue 18151. 2. Yes, the UseG1GC option must be included during Elasticsearchs startup stage for issue 18151. 3. Yes, it is required to have the UseG1GC option present during Elasticsearchs startup stage for issue 18151. Final Response: Yes, it is required to have the UseG1GC option during Elasticsearchs startup stage .VII. E FFECTIVENESS OF CHIME We evaluate CHIME by answering three RQs: RQ3. How well can CHIME fix inaccuracies in ChatGPT RQ4. How well do individual components in CHIME per- RQ5. Would responses from CHIME be favored like those from ChatGPT when both are correct? RQ3 and RQ4 investigate whether CHIME and its components can fix inaccuracies in ChatGPT while we use our benchmark. Similar to RQ2, we use the correctness metric from Equation 1 to answer RQ3 and RQ4. RQ5 assesses the usability of CHIME when it is used by developers instead of a RAG-based ChatGPT. We conduct a user study to answer RQ5. TABLE VII: Correctness of CHIME. Column Improv shows percent improvement over RAG-based ChatGPT T# Y/N Fact Sum Total Improv Total 80.0% 61.4% 65.5% 66.7% +30.3% T1 - Issue Anlys(S) 83.3% 67.1% 66.7% 70.8% +36.3% T1 - Issue Anlys(M) 58.3% 30.0% 50.0% 42.5% +20.0% T2 - Issue Trend 68.8% 43.8% 50.0% 55.0% +22.5% T3 - Issue Summary 0.0% 87.5% 68.8% 72.5% +22.5% T4 - Issue Labeling 83.3% 60.0% 87.5% 72.5% +22.5% T5 - Issue Backlog 100.0% 58.3% 50.0% 70.0% +32.5% A. How well can CHIME fix ChatGPT inaccuracies? (RQ3) Table VII presents the assessments of the correctness of CHIME in our benchmark by offering overall results, across the three types of queries and also across the five task types in our benchmark. Overall, CHIME offers around 30.3% improvement over the RAG-based ChatGPT pipeline from Section V. The improvement is consistent across all five task types, with issue analytics and backlog tasks benefiting the most from CHIME. CHIME showcases enhancements over ChatGPT across all tasks for both OpenJ9 and ElasticSearch: 29.6% and 31.1% improvement over ChatGPT for OpenJ9 and ElasticSearch respectively. The detailed result for each project is provided in our online appendix. We manually assessed the responses where CHIME was inaccurate and observed three main reasons as follows. Query-Directed Retrieval Failure (60.6%) : CHIME relies on its ability to query a database of stored data and generate SQL queries based on the provided questions. However, when user or verifying queries lack clarity, the query fails to provide clear instructions for formulating SQL queries. In such cases, CHIME produces incorrect or irrelevant responses. Logical Inference Errors (27%) : This pertains to cases where CHIME fails to accurately apply logical inference principles. It occurs when CHIME incorrectly deduces information from the data or makes faulty assumptions during reasoning. Semantic Discrepancy (10.9%) : CHIME relies on similarity scores to match user queries with existing data or responses. However, discrepancies in semantic similarity assessments can lead to incorrect matches or associations.TABLE VIII: Correctness of components of CHIME. Column Improv shows percent improvement over RAG-based Chat- Component Y/N Fact Sum Total Improv Issue Preprocessor 59.0% 43.9% 46.4% 48.1% +11.7% Query Preprocessor 57.0% 35.1% 45.2% 42.5% +6.1% Response Validator 55.0% 36.0% 47.6% 43.0% +6.6% CoVe 58.0% 28.9% 38.1% 37.9% +1.5% MT 69.0% 37.7% 53.6% 48.5% +12.1% Summary of RQ3. CHIME achieves an average correct- ness of 66.7% and an improvement of 30.3% over a RAG- based ChatGPT on our benchmark of bug report questions. B. How do individual components in CHIME perform? (RQ4) We ran each component of CHIME individually and deter- mined the contribution of the component within the pipeline. In Table VIII, we show the performance of each component per query type and also show whether the component offered an improvement over a RAG-based ChatGPT. We discuss how we ran each component while analyzing the results below. Issue Preprocessor. In our CHIME pipeline, we kept this component and removed the other two components (i.e., Query Processor and Response Validator). Hence, issue reports are preprocessed by this component and then stored in the database. From here, we utilize RAG-based ChatGPT for Q&A. The integration of the Issue Processor enhances the accuracy of the baseline GPT model by 11.7% on average for both projects. This improvement is particularly notable in technical question comprehension and analysis tasks. Query Preprocessor. Similar to the above setup, we only kept this component and removed the other two components in our CHIME pipeline (i.e., Issue Preprocessor and Response Val- idator). On average, this process demonstrates an improvement of 6.1% over a RAG-based ChatGPT. Response Validator. We used it to validate responses from an RAG-based ChatGPT. Overall, this component contributed to a 6.6% improvement over a RAG-based ChatGPT. The bottom two rows in Table VIII further illustrate the performance of the two modules in the Response Validator, i.e., CoVe and MT. Interestingly, MT as an individual module worked even better than the Response Validator component. MT offered a 12.1% improvement over RAG-ChatGPT while CoVe offered a 1.5% improvement. However, we kept the combinations of CoVe and MT in the response validator, because CoVe + MT may become more useful for other repos where responses may need a sequence of challenges via both CoVe and MT. For example, when the responses from CoVe contain references to the fact (but with incorrect summarization), MT can double- check those facts via follow-up mutated questions. As we can see from Tables VII and VIII, CHIME as an end-to-end pipeline offers 30.3% improvement over RAG-based ChatGPT, while none of the individual components in CHIME could offer more than 12% improvement over RAG-based ChatGPT. This means that the ensemble of all the components in CHIMEs pipeline helped the fixing of ones mistake by others. For instance, when a user queries List all pending issues rather than simply providing the count of pending issues, the transformed query from Query Preprocessor prompts the system to generate a list of issue numbers, which increases its accuracy. Summary of RQ4. Each component in CHIME can offer an improvement over a RAG-based ChatGPT by correcting the inaccuracies in ChatGPT responses. The components work best when they are all put together in CHIME as an C. Would responses from CHIME favored like those from ChatGPT when both are correct? (RQ5) A comparative study was conducted to assess the practical efficacy of CHIME, involving 31 participants. The majority (93%) had 0-5 years of experience in the software industry, with 57% being software developers and 33% researchers. 1) Survey Setup: Participants were presented with two random questions from each task in our benchmark dataset. The questions include a summarization of the failure of an issue, similarities between multiple issues, recurring themes in a component, pending issues, identification of error-prone components, unresolved or blocker issues, guidelines for label- ing, and label suggestions. For these questions, responses from both CHIME and ChatGPT, along with links to associated bug reports, were provided for evaluation. To ensure a fair comparison, only questions with correct responses from both systems were selected. Participants were then asked to rate the correctness and perceived usefulness of the responses in addressing software bug-related queries. The survey questions are provided in our online appendix. 2) Survey Result: Participant feedback in Table IX indi- cates that CHIME was the preferred choice for the majority of tasks when the answers were correct and selected more frequently for 6 out of 10 questions. It was favored in issue analysis (T1) with a 79% participant preference. On average, for this task, 63.6% of participants found the responses to be comprehensive and covering all necessary aspects, 45.1% felt that they provided additional information helpful for a better understanding of the problem, and 33.3% thought the responses were clear and easy to follow. Their preference also extended to issue summarization (T3) with a 65% preference and to issue labeling (T4), with a 63% preference. However, for the issue trending task (T2), there was a slight preference for ChatGPT. Nonetheless, for two questions, participants seemed undecided, indicating a comparable level of usefulness between CHIME and ChatGPT when the responses were correct.0.60.65 0.70.75 0.80.85 0.90.9510%20%30%40%50%60%70%80%90% ThresholdGPT Issue Pre. Query Pre. RV Fig. 9: Impact of Threshold on Similarity Analysis. Here, Issue Pre. = Issue Preprocessor, Query Pre. = Query Preprocessor, RV = Response validator TABLE IX: Selection Preference of CHIME and ChatGPT Provided Correct Responses across Tasks T1 - Issue Analytics21% 79% T2 - Issue Trend58% 42% T3 - Issue Summary35% 65% T4 - Issue Labeling37% 63% T5 - Issue Backlog48% 52% Summary of RQ5. In a comparative study with 31 partici- pants, CHIME responses were preferred over a stand-alone ChatGPT for the majority of tasks when both provided correct answers. This preference was particularly evident for tasks related to issue analysis, summarization, and A. Accuracy of our CFG We evaluated the CFG-based stack trace parsing by as- sessing the 80 issue reports that we used to create our benchmark dataset. The CFG is designed to identify key elements, such as exception types, messages, and code details (e.g., class/method/file names, etc.). For each stack trace, we checked whether the parser found all key elements as expected. We used three metrics to compute accuracy: precision, recall, and F1-score. Precision is the ratio of correctly identified elements to the total elements identified by the parser. Recall is the ratio of correctly identified elements to the total actual elements in the stack trace. F1-score ( F1) is the harmonicmean of precision and recall. We manually created a list of the expected elements for accurate comparison for each of the 80 issue reports. We observed an average precision of 0.99 and recall of 0.91 (F1-score = 0.93). The few errors in parsing were mainly due to the limitations in our regular expressions used in the CFG parser, and the variations in stack trace formats across issue reports. Our replication package contains the details of B. Threats to Validity Concerns regarding construct validity arise from the bench- marks design. However, we derived our benchmark queries from survey responses. The selection of issues from OpenJ9 and ElasticSearch may affect the generalizability of the find- ings across various software engineering contexts. The partic- ipant pool in surveys might not comprehensively represent the diverse perspectives in the broader software engineering com- munity. Finally, the methodology used for evaluating CHIME could affect the accuracy and objectivity of our effectiveness assessment and may introduce methodological bias . However, we have analyzed the accuracy rates of summary queries across various similarity threshold values of CHIME, ranging from 0.60 to 0.95, and chose the threshold of 0.7 as it gives the highest accuracy (see Figure 9), an optimal trade-off between capturing relevant information and minimizing false positives. We have introduced CHIME to mitigate the inaccuracy of ChatGPT response during bug report exploration. CHIME demonstrates 30.3% improvements over ChatGPT in terms of providing more correct responses for bug exploration tasks. Our industrial partner is working on deploying CHIME as a Slack bot. The conceptualization of CHIME originated from an internal demo of a similar chatbot created by our partner one year ago. Their initial chatbot lacked the required accuracy, which we sought to address by developing CHIME. To further motivate the need for such a chatbot beyond our industrial partner, we conducted a survey of 47 software practitioners (see Section III). The survey findings highlight the necessity of such chatbots in the real world. Feedback from 31 industry participants, presented in Section VII-C shows that CHIME is preferred for its ability to analyze, summarise, and label issues. Our industrial partner was involved in the design and evalu- ation of CHIME, which was crucial for advancing CHIME from the proof-of-concept stage to the current deployment stage within the company. Like any innovation, we expect to improve CHIME in an agile manner, i.e., based on user In the future, we will also focus on expanding CHIMEs grasp of more technical terminologies and other documents. To handle other documents, in CHIME we will improve the issue preprocessor module e.g., to separate code and textual contents, and to adapt the CFG to handle code snippet- s/traces/crash dumps in those documents or using a static partial program analyzer to handle code examples in API documentation. We expect that the other modules in CHIME can be used with minimal changes.DATA AVAILABILITY The code and data used for this study can be found here: [1] Z. Ji et al. , Survey of hallucination in natural language generation, ACM Computing Surveys , vol. 55, no. 12, pp. 1 38, 2023. [2] K. Filippova, Controlled hallucinations: Learning to generate faithfully from noisy data, in Findings of the Association for Computational Linguistics: EMNLP 2020 , T. Cohn, Y . He, and Y . Liu, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 864 870. [3] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, ArXiv , vol. abs/2311.05232, 2023. [Online]. Available: [4] M. Jang, D. S. Kwon, and T. Lukasiewicz, BECEL: Benchmark for consistency evaluation of language models, in Proceedings of the 29th International Conference on Computational Linguistics , N. Calzolari, C.-R. Huang, H. Kim, J. Pustejovsky, L. Wanner, K.-S. Choi, P.-M. Ryu, H.-H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim, Y . Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S.-H. Na, Eds. Gyeongju, Republic of Korea: International Committee on Computational Linguistics, Oct. 2022, pp. 3680 3696. [Online]. [5] M. Jang and T. Lukasiewicz, Consistency analysis of chatgpt, in Conference on Empirical Methods in Natural Language Processing , 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID: [6] R. Cohen, M. Hamri, M. Geva, and A. Globerson, LM vs LM: Detecting factual errors via cross examination, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 12 621 12 640. [Online]. Available: https://aclanthology.org/2023.emnlp-main. [7] B. A. Galitsky, Truth-o-meter: Collaborating with llm in fighting its [8] OpenJ9. (2024, February) Issue 18151: To accept openjdk option +useg1gc to enable startup of elasticsearch application]. [Online]. [9] D. Arya, W. Wang, J. L. C. Guo, and J. Cheng, Analysis and detection of information types of open source software issue discussions, inProceedings of the 41st International Conference on Software Engineering , ser. ICSE 19. IEEE Press, 2019, p. 454 464. [Online]. [10] Z. Jiang, F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang, J. Callan, and G. Neubig, Active retrieval augmented generation, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 7969 7992. [Online]. Available: https: [11] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K uttler, M. Lewis, W.-t. Yih, T. Rockt aschel, S. Riedel, and D. Kiela, Retrieval-augmented generation for knowledge-intensive nlp tasks, in Proceedings of the 34th International Conference on Neural Information Processing Systems , ser. NIPS20. Red Hook, NY , USA: Curran [12] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang, Realm: retrieval-augmented language model pre-training, in Proceedings of the 37th International Conference on Machine Learning , ser. ICML20. [13] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, Retrieval augmentation reduces hallucination in conversation, in Findings of the Association for Computational Linguistics: EMNLP 2021 , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3784 3803. [Online]. Available: https: [14] K. Beelen and D. van Strien, Metadata might make language models better, 2022. [Online]. Available: https://arxiv.org/abs/2211.10086[15] X. Ma, Y . Gong, P. He, hai zhao, and N. Duan, Query rewriting in retrieval-augmented large language models, in The 2023 Conference on Empirical Methods in Natural Language Processing , 2023. [Online]. [16] S. Z. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. E. Weston, Chain-of-verification reduces hallucination in large language models, 2024. [Online]. Available: [17] T. Chen, F. Kuo, H. Liu, P. Poon, D. Towey, T. Tse, and Z. Zhou, Metamorphic testing: A review of challenges and opportunities, ACM Computing Surveys , vol. 51, no. 1, pp. 1 27, 2018. [18] C. Wang and R. Sennrich, On exposure bias, hallucination and domain shift in neural machine translation, 07 2020. [19] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang, Exposing attention glitches with flip-flop language modeling, in Thirty-seventh Conference on Neural Information Processing Systems , 2023. [Online]. [20] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, Deduplicating training data makes language models better, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 8424 8445. [Online]. [21] N. Dziri, A. Madotto, O. Za ane, and A. J. Bose, Neural path hunter: Reducing hallucination in dialogue systems via path grounding, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 2197 2214. [Online]. [22] H.-S. Chang and A. McCallum, Softmax bottleneck makes language models unable to represent multi-mode word distributions, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 8048 8073. [Online]. [23] B. Dhingra, M. Faruqui, A. P. Parikh, M.-W. Chang, D. Das, and W. W. Cohen, Handling divergent reference texts when evaluating table-to-text generation, ArXiv , vol. abs/1906.01081, 2019. [Online]. [24] S. Lin, J. Hilton, and O. Evans, TruthfulQA: Measuring how models mimic human falsehoods, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 3214 3252. [25] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee, A. Roberts, T. B. Brown, D. X. Song, U. Erlingsson, A. Oprea, and C. Raffel, Extracting training data from large language models, in USENIX Security Symposium , 2020. [Online]. Available: [26] Z. Li, S. Zhang, H. Zhao, Y . Yang, and D. Yang, Batgpt: A bidirectional autoregressive talker from generative pre-trained transformer, ArXiv preprint , vol. abs/2307.00360, 2023. [27] Y . Liu, Y . Yao, J.-F. Ton, X. Zhang, R. Guo, H. Cheng, Y . Klochkov, M. F. Taufiq, and H. Li, Trustworthy llms: A survey and guideline for evaluating large language models alignment, ArXiv preprint , vol. [28] N. Lee, W. Ping, P. Xu, M. Patwary, M. Shoeybi, and B. Catanzaro, Factuality enhanced language models for open-ended text generation, ArXiv , vol. abs/2206.04624, 2022. [Online]. Available: https://api. [29] W. Shi, X. Han, M. Lewis, Y . Tsvetkov, L. Zettlemoyer, and S. W.-t. Yih, Trusting your evidence: Hallucinate less with context-aware decoding, ArXiv preprint , vol. abs/2305.14739, 2023. [30] A. Radford et al. , Language models are unsupervised multitask learn- ers, OpenAI blog , vol. 1, no. 8, p. 9, 2019. [31] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al. , The pile: An 800gb dataset of diverse text for language modeling, arXiv preprint arXiv:2101.00027 , 2021.[32] H. Touvron et al. , Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2306.00186 , 2023. [33] A. Sinitsin, V . Plokhotnyuk, D. Pyrkin, S. Popov, and A. Babenko, Editable neural networks, in 8th International Conference on Learning Representations, ICLR 2020 . Addis Ababa, Ethiopia: OpenReview.net, [34] Y . Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, Editing large language models: Problems, methods, and opportunities, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 10 222 10 240. [Online]. Available: [35] Z. Jiang et al. , Active retrieval augmented generation, arXiv preprint [36] D. Sobania, M. Briesch, C. Hanna, and J. Petke, An analysis of the automatic bug fixing performance of chatgpt, in 2023 IEEE/ACM International Workshop on Automated Program Repair (APR) . Los Alamitos, CA, USA: IEEE Computer Society, may 2023, pp. 23 30. [Online]. Available: https://doi.ieeecomputersociety.org/10. [37] A. Radford and K. Narasimhan, Improving language understanding by generative pre-training, 2018. [Online]. Available: https://api. [38] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. GONG, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. LIU, CodeXGLUE: A machine learning benchmark dataset for code understanding and generation, inThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1) , 2021. [Online]. Available: [39] J. Lanchantin et al. , Learning to reason and memorize with self-notes, arXiv preprint arXiv:2305.00833 , 2023. [40] B. Peng et al. , Check your facts and try again: Improving large language models with external knowledge and automated feedback, arXiv preprint arXiv:2302.12813 , 2023. [41] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, Retrieval augmentation reduces hallucination in conversation, in Findings of the Association for Computational Linguistics: EMNLP 2021 , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3784 3803. [Online]. Available: https: [42] A. Madaan et al. , Self-refine: Iterative refinement with self-feedback, arXiv preprint arXiv:2303.17651 , 2023. [43] C. Malaviya et al. , Quest: A retrieval dataset of entity-seeking queries with implicit set operations, arXiv preprint arXiv:2305.11694 , 2023. [44] P. Manakul et al. , Selfcheckgpt: Zero-resource black-box hallucina- tion detection for generative large language models, arXiv preprint [45] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in North American Chapter of the Association for Computational Linguistics , 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID: [46] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, CodeBERT: A pre-trained model for programming and natural languages, in Findings of the Association for Computational Linguistics: EMNLP 2020 , T. Cohn, Y . He, and Y . Liu, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 1536 1547. [Online]. Available: https://aclanthology.org/2020.findings-emnlp.139[47] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, J. Yin, D. Jiang, and M. Zhou, Graphcodebert: Pre-training code representations with data flow, ArXiv , vol. abs/2009.08366, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID: [48] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, Unified pre-training for program understanding and generation, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani- Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y . Zhou, Eds. Online: Association for Computational Linguistics, Jun. 2021, pp. 2655 2668. [Online]. Available: https://aclanthology.org/2021. [49] C. Raffel, N. M. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, J. Mach. Learn. Res. , vol. 21, pp. 140:1 140:67, 2019. [Online]. Available: [50] L. A. Goodman, Snowball sampling, Annals of Mathematical Statis- tics, vol. 32, no. 1, pp. 148 170, 1961. [51] G. Rodriguez-Perez, G. Robles, and J. M. Gonzalez-Barahona, How much time did it take to notify a bug? two case studies: Elasticsearch and nova, in 2017 IEEE/ACM 8th Workshop on Emerging Trends in Software Metrics (WETSoM) , 2017, pp. 29 35. [52] G. Rodr guez-P erez, Gregorio, Robles, A. Serebrenik, Andy, Zaidman, D. M. Germ an, Jesus, and J. M. Gonzalez-Barahona, How bugs are born: a model to identify how bugs are introduced in software components, Empirical Software Engineering , vol. 25, pp. 1294 1340, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID: [53] T. Zhang, D. Han, V . Vinayakarao, I. C. Irsan, B. Xu, F. Thung, D. Lo, and L. Jiang, Duplicate bug report detection: How far are we? ACM Trans. Softw. Eng. Methodol. , vol. 32, no. 4, may 2023. [Online]. [54] Z. Rasool, S. Barnett, S. Kurniawan, S. Balugo, R. Vasa, C. Chesser, and A. Bahar-Fuchs, Evaluating llms on document-based qa: Exact answer selection and numerical extraction using cogtale dataset, ArXiv , vol. abs/2311.07878, 2023. [Online]. Available: [55] C. Wang, X. Liu, Y . Yue, X. Tang, T. Zhang, C. Jiayang, Y . Yao, W. Gao, X. Hu, Z. Qi, Y . Wang, L. Yang, J. Wang, X. Xie, Z. Zhang, and Y . Zhang, Survey on factuality in large language models: Knowledge, retrieval and domain-specificity, ArXiv , vol. abs/2310.07521, 2023. [Online]. Available: https://api.semanticscholar. [56] M. P. Polak and D. Morgan, Extracting accurate materials data from research papers with conversational language models and prompt engineering, Nature Communications , vol. 15, 2023. [Online]. [57] L. Basyal and M. Sanghvi, Text summarization using large language models: A comparative study of mpt-7b-instruct, falcon-7b-instruct, and openai chat-gpt models, arXiv preprint arXiv:2310.10449 , October 2023. [Online]. Available: https://arxiv.org/abs/2310.10449 [58] LangChain, SQLDatabaseChain, Webpage. [Online]. [59] Hugging Face, all-MiniLM-L6-v2, Webpage. [Online]. Available: [60] F. Rahutomo, T. Kitasuka, and M. Aritsugi, Semantic cosine similarity, 2012. [Online]. Available: https://api.semanticscholar.org/CorpusID:'},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'text': 'Tool-Integrated Reasoning for Bengali Mathematical Department of Computer Science Bangladesh University of Engineering and Technology Department of Computer Science Bangladesh University of Engineering and Technology Abstract In this research paper, we present an innovative approach tailored to solving mathematical problems in Bengali, developed for the DL Sprint 3.0 - BUET CSE Fest 2024 Competition. Our methodology harnesses the power of advanced deep learning models, notably the Qwen 2.5 series, with iter- ative improvements made through prompt engineering, model quantization, and Tool Integrated Reasoning (TIR) to handle complex calculations. Initially, we explored various model archi- tectures, such as fine-tuned Mistral and quantized Qwen models, progressively refining them through translation techniques, RAG (Retrieval-Augmented Generation), and custom dataset curation. Through manual hyperparameter tuning, we optimized param- eters like temperature and top-p to improve model adaptability and response accuracy. Additionally, the removal of RAG and careful parameter adjustments further contributed to our final models robustness. Our approach demonstrates the potential of advanced NLP techniques in effectively interpreting and solving Index Terms Keywords: Bengali Mathematical Problem Solv- ing, Qwen 2.5, Mathematical Reasoning in Bengali, Tool Integrated Reasoning (TIR), Retrieval-Augmented Generation (RAG), Model Quantization, Prompt Engineering, Self-Consistent The ability to understand and solve mathematical problems is a foundational skill for AI, essential for advancements across fields like science, engineering, and finance. However, while AI models have made strides in various languages, they still face significant challenges when tackling mathematical reasoning in low-resource languages, such as Bengali. This gap becomes particularly evident in tasks involving complex problem-solving and precise calculations. To address this, the DL Sprint 3.0 - BUET CSE Fest 2024 Competition [1] intro- duced the unique challenge of building an AI model capable of solving mathematical problems in Bengali, targeting issues akin to those in the Bengali Math Olympiad. This competition not only tests participants technical skills but also aims to push the boundaries of AIs adaptability and performance in Bengali. Our work contributes to this pioneering effort, focusing on enhancing AIs mathematical reasoning in Bengali through advanced NLP and deep learning techniques. Weexplore and iteratively refine state-of-the-art models, such as the Qwen series, alongside strategies like prompt engineering, Tool Integrated Reasoning (TIR), and manual hyperparameter tuning to achieve robust problem-solving capabilities. By contributing to this research, we aim to advance AIs reach into Bengali language processing, ultimately creating models that can assist students, educators, and researchers in tackling complex problems with precision and reliability. For this task, we aimed to select models capable of effi- ciently solving mathematical problems in Bengali, considering both performance and computational efficiency. Our initial approach involved using a fine-tuned Mistral 7B model [2], which, despite being a strong general-purpose model, did not deliver the desired accuracy for mathematical reasoning tasks. This led us to explore other models better suited for handling mathematical challenges, particularly in a low-resource lan- guage like Bengali. The Qwen series emerged as the most promising option due to its strong performance on mathemat- ical reasoning benchmarks. The Qwen-32B-Instruct model [3], with impressive scores on the MATH benchmark (83.1) and GSM8K benchmark (95.9) [4], was especially appealing for its capability in solving high-level mathematical problems. Given its robust performance, we chose to focus on the Qwen-2.5 series, including the 7B, 14B, and 32B models, each offering different trade-offs between accuracy and computational de- mands. We initially fine-tuned the Qwen-14B-Instruct model for a single epoch, which, though promising, did not provide sufficient improvements in performance. To enhance model ef- ficiency, we implemented VLLM (Variable-Length Language Model) for faster inference, which allowed us to speed up the testing process while maintaining model accuracy. Addition- ally, we employed model quantization techniques to reduce memory requirements, making the models more practical for large-scale inference tasks. To further refine performance, we incorporated Tool Integrated Reasoning (TIR), which enabled the model to perform complex calculations using Python. This 8 Nov 2024method improved the models ability to handle mathematical operations effectively. Moreover, manual hyperparameter tun- ing of parameters like temperature and top-p helped optimize the models response accuracy and adaptability. In summary, after exploring various models, we selected the Qwen series, particularly the Qwen-32B-Instruct model, for its exceptional performance in mathematical reasoning. Combined with tech- niques like VLLM, TIR, and hyperparameter optimization, we were able to enhance the models ability to solve mathematical problems in Bengali effectively. In this project, preprocessing was crucial to handling Ben- gali mathematical problems and enhancing the models ability to solve them accurately. Given the complexity of understand- ing Bengali text directly in the initial stage, we leveraged the Qwen-32B-Instruct model to translate Bengali mathematical questions into English for improved processing. This approach enhanced the performance of our model significantly. Prompt tuning played a key role in optimizing the per- formance of our model as we experimented with various reasoning techniques, including Chain of Thought (COT), Tool Integrated Reasoning (TIR), RAG, self-consistent TIR, and self-consistent COT. We initially designed prompts to guide the model through step-by-step reasoning for COT. For TIR, we adapted the prompts to instruct the model to perform calculations using Python tools for more complex problems. With Self-COT and Self-TIR, the prompts were modified to encourage the model to generate multiple reasoning paths and select the most consistent solution. we experimented with Retrieval-Augmented Generation (RAG). The RAG approach was implemented to provide context and improve the quality of answers in both Bengali and translated English questions. However , after experimenting with Retrieval-Augmented Generation (RAG), we decided to discontinue its use due to its poor performance, as it did not significantly improve the models accuracy compared to other approaches.Additionally, we fine-tuned hyperparameters such as temperature and topp to control the diversity and confidence of the models responses. Lower temperatures were used for more deterministic answers, while higher values promoted creativity.Adjusting topp helped the model select the most plausible solutions.These prompt tuning and hyperparameter adjustments, particularly with TIR and Self-TIR, significantly improved the models ability to solve complex Bengali math problems. In this section, we present the performance of our deep learning model using various approaches. The model was evaluated on the public leaderboard of the DL Sprint 3.0, with the baseline score being 28 out of 100. We tested different configurations, including using various versions of the Qwen model, translation, retrieval-augmented generation (RAG), and Tool Integrated Reasoning (TIR).Translation RAG TIR Self-Consistency Score No No No No 49 Yes(Qwen2.5-14B-Instruct) No No No 61 No Yes No Yes 48 Yes(Qwen2.5-14B-Instruct) Yes No Yes 65 Yes(Qwen2.5-32B-Instruct) Yes No Yes 70 No No No Yes 66 Yes(Qwen2.5-32B-Instruct) No No Yes 73 TABLE I: Performance of Qwen2.5-32B-Instruct with Differ- Temperature Top p Number of Candidates Inference Time(s) Score 0.2 0.9 4 7844.8 68 0.35 0.775 4 7327.0 72 0.4 0.8 4 7391.5 77 0.4 0.8 10 19753.3 77 0.3 0.75 4 7579.1 76 0.3 0.75 5 9721.5 74 0.3 0.75 3 5436.7 71 0.7 0.8 4 7567.7 70 TABLE II: Hyperparameters and performance of the model The experiment began with a baseline score of 28, and the fine-tuned Mistral 7B model scored only 15, indicating the need for a more powerful model architecture and translation capabilities. Scaling to larger Qwen 2.5 models, such as the 7B and 14B-Instruct, improved performance to 40 points but still showed limitations with certain problem types. The quantized Qwen 2.5-32B-Instruct model performed better, reaching a score of 49 without translation, benefiting from its larger Introducing translation using the 14B-Instruct model boosted the score to 61, allowing the model to leverage pre- existing English knowledge. Further improvement occurred when translation was done using the 32B model. Incorporating Retrieval-Augmented Generation (RAG) with translated En- glish questions raised the score to 70, showing that additional context from external datasets enhanced model performance. However, removing RAG led to a score of 73, suggesting that it may have introduced noise, or the larger model could handle The introduction of Tool Integrated Reasoning (TIR), where Python was used for complex calculations, brought the score to 76. This step significantly enhanced the models efficiency and accuracy, especially with Bengali questions, which saw a notable improvement. The final model, optimized with prompt engineering and hyperparameter tuning, achieved a score of 77, marking the culmination of the models refinement in handling diverse problem types.Model Translation TIR Self-Consistency Score Deepseek-math-7b-Instruct(Baseline) No No Yes 28 mCot(Fine-tuned Mistral 7B) No No No 15 Qwen2.5-14B-Instruct No No No 40 Qwen2.5-Math7B-Instruct No Yes No 39 Yes Yes No 48 NuminaMath-7b-TIR(Finetuned Deepseek-math-7b) Yes(Qwen2.5-14B-Instruct) Yes Yes 66 Yes(Qwen2.5-32B-Instruct) Yes Yes 68 TABLE III: Performance of other tried models with Different Configurations Overall pipeline of our solution Fig. 1: Iterative pipeline Throughout the development of our AI model for solving Bengali mathematical problems, we encountered several key Bengali questions presented without translation consistently scored lower than their translated counterparts. Translating questions into English improved model understanding andproblem-solving, leveraging the models richer pre-trained B. Model Size for Translation Using the Qwen 2.5-32B-Instruct model for translation yielded better results compared to the Qwen 2.5-14B-Instruct model. The larger model demonstrated superior language comprehension, which improved translation quality and con- tributed to overall score improvements.C. RAG Limitations Initial experiments with Retrieval-Augmented Generation (RAG) aimed to enhance the models responses by adding contextual information. However, RAG often introduced noise, leading to a decrease in performance. Removing RAG sub- sequently improved scores, indicating that the inherent ca- pabilities of the larger Qwen models were sufficient without additional retrieved context for this task. D. Scarcity of Bengali Datasets The limited availability of high-quality Bengali mathemati- cal datasets restricted the models exposure to diverse problem E. Inference Optimization with VLLM Integrating Variable-Length Language Modeling (VLLM) enabled faster inference, which was crucial for handling the computational demands of larger models. This optimization helped streamline the testing process within our resource F . Prompt Sensitivity The model exhibited a high sensitivity to prompt wording, with minor adjustments in phrasing significantly affecting output quality and accuracy. Effective prompt engineering became essential to guide the model toward optimal solutions, underscoring the importance of precision in prompt construc- G. Quantization for Resource Constraints Due to Kaggles limited GPU memory, we had to quantize the larger Qwen models to make them feasible for testing. This process reduced memory requirements, allowing us to utilize the larger 32B model in a resource-limited environment without compromising performance excessively. H. Fine-Tuning Limitations for Larger Models While fine-tuning smaller Qwen models was manageable, memory constraints in Kaggle prevented us from fine-tuning the Qwen 2.5-32B model. This limitation hindered further customization of the 32B model, restricting our ability to fine- tune it for specific Bengali problem-solving tasks. V. C ONCLUSION AND FUTURE WORK This paper presented a novel approach to solving Bengali mathematical problems by leveraging the Qwen 2.5 series models and optimizing through prompt engineering, trans- lation, and Tool Integrated Reasoning (TIR). Our methods demonstrated the efficacy of advanced NLP techniques in mathematical problem-solving, particularly for low-resource languages like Bengali. The Qwen 2.5-32B model, combined with techniques such as self-consistency and TIR, achieved significant improvements over baseline models, highlighting the models potential in handling complex reasoning tasks with minimal Bengali-specific data. Despite these achievements, challenges remain, especially regarding translation dependen- cies, limited Bengali datasets, and memory constraints forlarge models. These factors impacted both model performance and adaptability to a wider range of problem types. Our results also revealed the sensitivity of model outputs to prompt phrasing, underscoring the need for refined prompt engineering To build on this work, we propose the following areas for Enhanced Bengali Data Collection: Increasing the availability and diversity of Bengali mathematical datasets will enable better model training and adaptability to complex problem types. Domain-Specific Fine-Tuning: With sufficient resources, fine-tuning the Qwen 2.5-32B model specifically on Ben- gali math problems could further improve accuracy and Optimized Prompt Engineering: Researching prompt optimization strategies for mathematical reasoning tasks, especially in low-resource languages, could make the model responses more consistent and accurate. Exploration of Lightweight Models: Investigating smaller, efficient models tailored for Bengali could bal- ance memory constraints and inference speed without By addressing these areas, we aim to enhance the utility of deep learning models in Bengali problem-solving, making ad- vanced educational tools more accessible to Bengali-speaking learners and educators. This research lays a foundation for further progress in low-resource language applications of AI in mathematical education and reasoning. [1] S. Sushmit, A. Rahman, A. Azad, A. Rahman, M. S. Hossain, N. Tahmid, S. Islam, F. A. Akash, M. Islam, A. Muhtasim, T. Tahsin, and T. T. Dipto, DL Sprint 3.0 Bengali AI Math Olympiad, Kaggle, 2024. [Online]. [2] H. Lai and M. Nissim, mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics , Bangkok, Thailand, Aug. 2024. Association for Computational Linguis- [3] Qwen Team, Qwen2.5: A Party of Foundation Models, Sept. 2024. [4] Qwen Team, Qwen2.5 Overview, Qwen2 Official Website. Available:'},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'text': 'ENHANCING CLUSTER RESILIENCE : LLM- AGENT BASED AUTONOMOUS INTELLIGENT CLUSTER DIAGNOSIS SYSTEM AND EVALUATION FRAMEWORK Honghao Shi1Longkai Cheng1Wenli Wu1Yuhang Wang1Xuan Liu1Shaokai Nie1Weixv Wang1 Xuebin Min1Chunlei Men1Yonghua Lin1 Recent advancements in Large Language Models (LLMs) and related technologies such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have enabled the creation of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have developed an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovations include a knowledge base tailored for cluster diagnostics, enhanced LLM algorithms, practical deployment strategies for agents, and a benchmark specifically designed for evaluating LLM capabilities in this domain. Through extensive experimentation across multiple dimensions, we have demonstrated the superiority of our system in addressing the challenges faced in cluster diagnostics, particularly in detecting and rectifying performance issues more efficiently and accurately than traditional methods. Recent advancements in Large Language Models (LLMs) and complementary technologies such as Retrieval- Augmented Generation (RAG) and Diagram of Thought (DoT) have paved the way for the development of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have created an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovative approach includes the establishment of a specialized knowledge base for cluster diagnostics, the enhancement of LLM algorithms to better suit the demands of the domain, practical deployment strategies for agents within real-world environments, and the development of a benchmark specifically tailored to evaluate LLM capabilities in the context of cluster diagnostics. These components collectively contribute to a robust framework that addresses the complexities inherent in managing AI clusters, particularly in scenarios involving performance degradation or other operational anomalies. Through rigorous experimentation, we have validated the effectiveness of our LLM-agent system across multiple dimensions. Our benchmark, which consists of 150 manually crafted advanced questions, serves *Equal contribution1Beijing Academy of Artificial Intelli- gence, Beijing, China. Correspondence to: Yonghua Lin <yh- >.as a comprehensive evaluation tool that highlights the performance differences between our enhanced LLM-agent and baseline open-source models. In practical applications, the LLM-agent demonstrates its superior capability to identify and resolve performance issues more efficiently than traditional methods, reducing the troubleshooting time significantly. For instance, in a simulated scenario where one GPU was throttled to a much lower frequency, our system identified and resolved the issue within a matter of minutes, whereas conventional approaches would have taken a senior operations engineer nearly an hour to diagnose and rectify using pre-written automated detection Moreover, the LLM-agents ability to detect and initiate corrective actions even before the performance degrada- tion is noticed by human operators marks a significant advancement in proactive system maintenance. This capability not only mitigates immediate issues but also enhances the overall availability and reliability of the cluster by preemptively addressing potential faults. By leveraging the strengths of RAG and DoT, the LLM-agent can autonomously execute remediation measures, thereby freeing up engineering resources to focus on more complex and value-driven tasks. Our research underscores the transformative potential of combining AI-driven diagnostics with practical deployment strategies, setting the stage for a new era of intelligent cluster management solutions. 8 Nov 2024Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework 2.1 LLMs Alignment and Enhancement In recent years, generative artificial intelligence centered around large language models(LLMs) has seen rapid development, with powerful natural language generating capabilities demonstrated by proprietary models such as the GPT series(Achiam et al., 2023) and Gemini series(Team et al., 2023), as well as open-source models like Llama(Dubey et al., 2024) and Qwen(Yang et al., There are multiple approaches to enhancing the capabil- ities of LLMs across different stages such as training, inference, and deployment, as well as in areas like data, algorithms, and computational resources. In light of the achievements of autoregressive models like GPT- 2(decoder-only transformers)(Radford et al., 2019) and LLaMA(transformer++)(Touvron et al., 2023), enhancing the quality of the data has become a critical method for improving the efficacy of models during the pre-training process(Adler et al., 2024; Liu et al., 2024). For modern LLMs, there exists several training or fine- tuning works between pre-training and the deployment. ChatGPT(Ouyang et al., 2022) describes this process as Su- pervised Fine-Tuning (SFT), Reward Modeling (RM), and Reinforcement Learning with Human Feedback (RLHF), while LLaMA3.1(Dubey et al., 2024) integrates these into a continuous process known as Continue Training. Besides training, LLMs can leverage Retrieval-Augmented Generation (RAG)(Lewis et al., 2020) to utilize knowledge from data distributions that were not part of the training set. We can refer to the above content as the alignment and 2.2 AI-agent based Applications After the model parameters have been frozen, it is possible to enhance the inherent capabilities of the model through mechanisms such as chain-of-thought(CoT) reasoning(Wei et al., 2022), scaling test time(Snell et al., 2024), and com- bining CoT LLM and AI agents(Castelfranchi, 1998) as LLM-agent(Park et al., 2023). CoT is a prompting technique used to guide LLMs to generate intermediate reasoning steps before arriving at a final conclusion. There are extensions to classic CoT, such as Tree of Thought (ToT)(Yao et al., 2024) for tree- like backtracking, Graph of Thought (GoT)(Besta et al., 2024) for graph-based reasoning, and Diagram of Thought (DoT)(Zhang et al., 2024) for a propose-critique-summarize approach based on topos theory. The development of CoT and the scaling of test-time are unified, with CoT applications always aiming to maintain optimal results with limited test-time or scaling test-time toachieve extraordinaire results(Snell et al., 2024). The CoT series technics are also one of the foundations for build- ing LLM-agents. LLM-agents can leverage LLMs as the processing core while integrating traditional AI-agent capa- bilities such as memory, planning, and execution, creating semi-autonomous software entities that are highly adaptive and capable(Xi et al., 2023). 2.3 Diagnosis and Repair for AI Clusters Constructing and utilizing LLM applications typically re- quire hardware infrastructure on a scale costing millions of or more dollars. Meta constructed the LLM application core LLaMA 3.1 within 54 days, leveraging a cluster that included 16,000 GPUs(Dubey et al., 2024), with just the GPU costs amounting to over billion dollars. However, such complex and expensive systems face significant chal- lenges in terms of reliability and availability. During the 54-day training, the Meta cluster experienced 419 unex- pected interruptions, averaging one disruption every three hours. At such a frequency of interruptions, the cluster, from the operating system to the AI framework and distributed scheduling software, requires the ability to capture, identify, attribute, and repair exceptions to ensure successful and ef- ficient model training. Microsofts Superbench(Xiong et al., 2024) has systematically built a suite of standard test cases to comprehensively assess the availability of clusters. In terms of capture and repair, the Torch(Paszke et al., 2019) Elastic solution aims to enable automatic restarts of model training, while works such as FlashCheckpoint- ing in DLRover(Wang et al., 2023) focus on reducing the cost of checkpoint saving and loading during the automatic restart process. Building upon automatic restart capabili- ties, many works at the AI framework level have conducted research and practical implementations to enhance reliabil- ity and availability, particularly those featuring highly cus- tomized solutions based on Megatron(Shoeybi et al., 2019). ByteDances Megascale(Jiang et al., 2024) and Alibabas Pai-Megatron(Qian et al., 2024) both provide toolkits for cluster diagnostics, which are used to check the health of servers and networks, as well as to perform manual or auto- mated error identification and repair. With the advancement of AI technologies, researchers are beginning to explore the use of AI techniques to address cluster diagnostic issues. Using big data techniques to ana- lyze log files was an typical approach to automating cluster diagnostics(Jung & Chung, 2021). However, such meth- ods primarily involve static or real-time analysis of files produced by the training process, which limits their attribu- tion capabilities and means they lack intelligent autonomy, relying instead on pre-written execution and planning pro- cedure.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework AI computing tasks: refers to programs or processes designed to achieve intelligence, such as training large language models, inference with large language models, world model inference, and LLM-agent inference. AI chips: processors suitable for or dedicated to performing AI computing tasks, such as NVIDIA GPUs, Intel Gaudi AI accelerators, and Google TPUs(Jouppi et al., 2017). AI servers: computers equipped with AI chips that are suitable for or specifically designed to perform AI computing tasks, such as the NVIDIA DGX H100. AI servers often have requirements beyond those of classic servers in terms of stability, availability, cooling, and power AI cluster: a distributed server cluster composed of two or more AI servers set up to accomplish a single target task, such as Metas cluster containing 16 thousand GPUs. Additionally, AI servers typically require RDMA or higher bandwidth interconnect protocals, such as InfiniBand RDMA(Shanley, 2003) and RDMA over Converged Ethernet(RoCE)(Guo et al., 2016), and do not usually adopt Cluster diagnosis: ensuring that AI computing tasks can run with normal performance on the AI cluster, promptly detecting task failures, identifying the points of failure, clarifying the reasons for failure, repairing the corresponding faults, and ensuring the overall availability of the AI cluster. We incorporate advanced techniques from the field of LLM alignment and enhancement to creatively develop a solution for building a cluster intelligent maintenance system based on LLM-agents. Figure 1 illustrates the overall process of The upper part of the figure represents the core component of solution: the LLM-agent. The LLM-agent consists of an agent program and an LLM. The LLM interprets the input information provided by the agent as external stimuli and task instructions, and responds appropriately. The agent then directly writes code or calls specific software interfaces based on the feedback from the LLM, thereby operating the cluster. For LLM itself, there are two main challenges. First, how does the LLM acquire domain-specific knowledge of cluster diagnostics, and furthermore, where does this knowledge come from. Second, how can the LLM reason and plan? For the entire LLM-agent, ensuring that the LLMs inputs and outputs match with the actual operations performed by the agent controlling the cluster is another crucial aspect that needs to be addressed. Figure 1. Overview of the Intelligent Maintenance System Based In order to solve the above problems, we have introduced three innovations. First, we use 250 cluster failure records collected from GitHub as a starting point, and treat the cluster operation failure logs actually managed by the LLM-agent as a continuous source of data. We utilize RAG(Lewis et al., 2020) to enable the LLM to capture detailed knowledge corresponding to specific terms within the context. Figure 1 describes the alert, compute cluster, and storage sections, along with their communication with the LLM-agent, which outlines this process. Second, we use DoT(Zhang et al., 2024) enables the model to effectively handle non-natural language information such as symbols, formulas, and code. Similar to vision-text multimodal models, we effectively leverage textual elements that go beyond the inherent meaning of natural language based on DoT. The planning algorithm section at the top of Figure 1 illustrates this innovation. Third, we use self-play technology(Snell et al., 2024) to enable the LLM to autonomously, also intelligently, devides long tasks or challenging reasoning objectives into multiple steps, self-assess the output of each step, and ultimately The lower part of Figure 1 forms the basis of our work. It includes a mature operations alarm troubleshooting and repair process, as well as several mature or advanced software tools. Based on related works, we have developed a unified, multi-level, multi-dimensional cluster diagnostic toolkit as Figure 2. This tool diagnoses the health status of the cluster fromEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Figure 2. Tools for LLM-agent to Diagnose AI Cluster both the supply side and the demand side simultaneously. The bottom part of Figure 2 lists the various components required to build an AI cluster, including the computing component, storage component, network component, and others. AI clusters following different technical routes provide similar capabilities, as shown in the middle part of Figure 2. We inspect all resource supply items affecting AI computing tasks to determine if their content is correct, if their performance is appropriate, and if they are stable. For example, for the feature of RDMA read/write between two GPUs across servers, our tool checks whether the read/write content is correct, whether the IOPS, bandwidth, latency, and other performance metrics are appropriate, and the stability under complex scenarios such as long-duration or multi-process read/writes. Most of these tools are improved versions of packages provided by chip, server, or operating system vendors. The top part of Figure 2 takes the demand side into consideration, evaluating the metric of concern for AI computing tasks with various characteristics. In summary, we have built an LLM-agent capable of retriev- ing and utilizing vast amounts of external information, with autonomous planning, learning, reasoning, and execution capabilities. This LLM-agent works alongside either custom-written tools or existing mature tools to perform early warning, troubleshooting, and repair tasks for the 4.2 Cluster Diagnosis Domain-specific Knowledge Our knowledge base consists of two sources. One part is logs, monitoring information, or program output content, come from pre-collected, cleaned, and organized GitHub data, carefully selected to address pain points in the cluster diagnostics and troubleshooting domain, incorporating knowledge from issues in the GitHub community, also come from operational data acquired after the initial deployment and operation of the LLM-agent. We call it Diagnosis Dataset. The second part is composed of symbolicreasoning. These reasoning structures use AI computation tasks and hardware specification information as input, and through a bottom-up modeling approach, predict the theoretical performance of the given AI computation tasks, thereby determining the correctness of the performance. We drew on effective practices from Alibabas experience in managing cluster startup operations(Xu et al., 2024) to build a database. We cleaned, organized, and structured the unstructured data obtained from GitHub, ultimately forming an effective dataset. We collected over a thousand questions and feedback items from the GitHub issue section. Through automated processes and manual review, we filtered out over 200 entries with substantive knowledge content and well-structured Q&A formats. Each piece of organized data contains four fields: problemkey, rawtext, function, and The problemkey is a domain keyword identified either manually or based on openai o1. Rawtext refers to the original content of a website after simple formatting, stored as a long string containing the questions asked on the web page and the developers responses. The function is based on our cluster diagnosis toolkits and is manually correlated by cluster troubleshooting personnel. This part is used as annotation in the portion of the dataset that the model can perceive, it is not perceived by the model for the answers used in the benchmark evaluation part, and it serves as the starting point for knowledge acquisition after the LLM-agent is deployed. The final results are the causes of the faults extracted from the rawtext based on the developers answers. For an LLM capable of driving an agent to perform cluster diagnostics, we expect it to be able to determine the causes of faults based on real-time operational information from the cluster and to call existing tools or write tool code on-the-fly for cluster repairs, without relying on rawtext containing developer replies. We will demonstrate this capability in subsequent experiments. We use a series of progressive methods to model the correct performance of given AI computation tasks, and through the DoT, we convert this special modal data into tokens to feed into the model. In addition to cluster health check, we have included modules in the toolkits to determine whether different AI computing tasks exhibit correct performance. These modules can, on one hand, be invoked by the agent to provide results to the LLM for analysis, and on the other hand, they can be called by the LLM to have the agent check We start modeling with the simplest task types. ConsideringEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework that existing AI clusters are composed of computing devices with the von Neumann architecture, AI computing tasks require the use of computing cores, memory, and I/O ports. It is worth noting that what AI computing tasks occupy are not narrowly defined CPU computing cores, main memory, or input/output ports, but rather in a broader sense, such as computing cores dedicated to matrix multiplication, HBM memory composed of multi-level caches, and high-speed I/O ports formed by PCIe or RDMA protocols. To build a unified model, we use the concepts of equivalent computing power, equivalent memory bandwidth, and equivalent I/O We refer to computational tasks that occupy or primarily oc- cupy one type of resource as single-resource computational tasks. We construct a single-variable computational task per- formance model and use experiments based on Khinchins law of large numbers to get the results. We assume that for a certain computational task T, the total amount of resource Rirequired is Mi. The hardware running this task can pro- videNiunits of resource Riper second. Assume that the single-variable task Txdepends only on resource R0. We determine M0based on the mathematical formula used for the tasks computation. For N0, we consider it a random variable. Through a large number of repeated experiments after warm-up, we ensure that the difference between the measured results and the expected value of the random vari- able approaches zero. We define performance as the number of times a specific task can be executed per unit time. For the aforementioned task Tx, we predict its performance to For non-single-variable tasks, we focus on modeling whether the different resources they depend on can oper- ate in parallel. A widely used method in multivariate task modeling is the roofline model(Ofenbeck et al., 2014). The roofline model introduces a new variable: task characteristic CT. The Roofline model introduces a new variable: the task characteristic CT. Consider a task Txdepends on two resources R0andR1, the effective utilization of resource R0is plotted on the Y-axis, and the ratio of effective utiliza- tion of resource R0to resource R1is plotted on the X-axis. By changing CT, a scatter plot can be drawn, forming a shape like a roofline. The Roofline model is equivalent to modeling the performance of multivariable tasks under fully parallel scenarios, which does not align with real-world conditions. Additionally, in the context of existing LLM performance modeling, changes in CTare not about varia- tions in the input size of a single task but about the changing proportions of two different primary resource-consuming tasks within the total task. Therefore, we use the proportion of different subtasks as variables to model multivariable tasks for the three main resources provided by AI clusters: equivalent floating- point computing power for matrix multiplication, memory read/write bandwidth, and I/O port bandwidth. The results 0% computeA B CFigure 3. Multi-variable Task Performance Modeling. A shows compute-memory, B shows interconnect-memory, C shows at figure 3 show that computing and memory are in domains that are completely non-parallelizable, whereas computing, memory, and I/O ports can approach full parallelization. This conclusion and related figures have been compiled and placed in the RAG documentation. 4.3 Create LLM-agent with RAG-DoT-Selfplay 4.3.1 Using RAG to Build an LLM That Can Utilize RAG integrates two core components: retrieval and genera- tion. The retrieval module is responsible for finding context- relevant information from an external knowledge base, a process that typically involves indexing large volumes of documents to quickly locate the most pertinent segments. The retrieved information is then passed to the generation module as additional input. The generation module builds upon a pre-trained language model, leveraging the retrieved context to enhance its generation capabilities, thereby pro- ducing responses that are more accurate and better aligned Considering other similar technologies, SFT requires sub- stantial computing resources and may diminish the models inherent generalization capabilities. In-context learning consumes context length and inference time, making it unsuitable for importing datasets with millions of entries. RAG can acquire relevant knowledge during inference with minimal resources and inference time, without altering the weights of the model itself. 4.3.2 Using DoT to Build an Agent That Can Reason and DoT(Diagram of Thoughts)(Zhang et al., 2024) models iter- ative reasoning in LLMs as constructing a Directed Acyclic Graph (DAG) within a single model. The DAG consists of nodes representing propositions, critiques, refinements, and verifications, with edges indicating the logical relationships or dependencies between them. We use XML to handle mul- timodal special symbol data and perform reasoning based Based on the principles of DoT, we use XML tags to sep-Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework arate different types of text, including plain text, special symbols, code, formulas, and inference rules. Thanks to the rope positional encoding adopted by LLama3.1, the model can accurately capture the content within XML pairs. Based on the reasoning graph, our experiments confirmed that this application allows the LLM to correctly reason according to specific rules, achieving the capability to support the agent in completing cluster fault attribution and repair tasks. This significantly exceeds the capabilities of pre-trained or 4.3.3 Using Selfplay Techniques to Construct a With the help of RAG and DoT, the LLM can utilize information from outside the training set as well as abstract symbolic reasoning information. However, this still has limitations for an agent designed for intelligent cluster diagnostics. We permit the LLM to generate content over a longer duration. The quality of solutions to challenging problems can be enhanced through multiple rounds of planned selfplay or spontaneous self-questioning and answering by the agent. Spontaneous self-questioning and answering is applied in DoT reasoning. On the planned selfplay process, we transform the complex problem of cluster fault attribution into a three-round process. In the first round, the agent, based on error logs passed from the cluster, prompts the LLM to identify potential keywords from the error items and corresponding solutions from the knowledge base, performing information extraction and RAG. In the second round, the LLM evaluates its own answers, making corrections or accepting them directly, then proceeds to write or call appropriate tools for the Agent to execute. In the final round, the LLM makes an accurate attribution judgment based on the results of the agents interaction with the actual cluster. Compared to existing selfplay work focused on the text side, we integrate it with the agent, granting it the permissions to operate machines and interact with the environment, fully simulating the capabilities of a human engineer to solve problems. We conducted a three-phase experiment to demonstrate the advanced nature of the proposed LLM-agent in the field of cluster intelligent diagnostics. The first phase involves creating a dataset and benchmark for the field of cluster intelligent diagnostics. First, we define the statistical characteristics of the external data knowledge base and introduce the process of generating an evaluation benchmark from this knowledge base. Next, we describe the features of this benchmark and explain its advanced naturein the field of cluster intelligent diagnostics. Throughout this process, we emphasize fairness and impartiality, strictly distinguishing between the parts of the model that can be perceived and the scoring portions of the evaluation. We further elaborate on the benchmark using the results of the mainstream open-source model LLaMA3.1-70B. The second phase involves evaluating the innovative aspects of the three models we proposed RAG, DoT, and selfplay using the aforementioned benchmark for comparative assessment. The experiments in the second phase are aimed at demonstrating the advanced nature of our proposed models in the field of cluster intelligent In the third phase, we expose the LLM-agent to both the training and testing sets in the benchmark, allowing it to operate in its most complete form to address real-world problems encountered in production environments. We demonstrate the accuracy, efficiency, and autonomous intelligence of this solution through two typical cases. Specifically, we found that this solution can provide early warnings for AI clusters, further enhancing the availability Finally, we will conduct a qualitative analysis and discus- sion on the topics of correctness, safety, and reliability, which are at the forefront of the LLM and LLM-agent fields and have yet to be conclusively resolved, to demonstrate the series of work we have undertaken in these areas. 5.1 Statistics and Evaluation for Dataset and The materials provided to the LLM come from three sources. The first source is automatically collected Q&A data from relevant GitHub communities involved in AI cluster troubleshooting, such as the issue sections of repositories like Megatron, PAI, Deepspeed, and NCCL. This serves as our initial dataset. The data has undergone two rounds of filtering, both automatic and manual, retaining parts with clear solutions and logical dialogues. The second source is the program output obtained by the LLM-agent using RAG+DoT technology on several AI clusters running tasks. These tasks are executed on clusters ranging from 4 to 100 A800 AI servers. The third part consists of special modal data such as symbolic representations and formulas processed using XML according to DoT logic, all of which are unified into the text modality. The total amount of pure text material is 200+ items compared with 1.2GB origin files. This also confirms that if more than 200 items consist of pure text content is fully pre-tokenized to serve as the context for LLM inference, it not only poses a significant challenge to the LLMs capability to handle long texts but also increases theEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework consumption of inference resources, thereby slowing down the execution speed of the LLM-agent. 5.1.2 Benchmarks Source and Statistics for Benchmark We divided the original dataset into two parts, approximately in a 20%-80% ratio. From the 80%, we manually compiled 150 questions to assess the LLMs capabilities in the field of cluster diagnostics. During comparative experiments, unless otherwise specified, we provide only 20% of the original data to all models. During case studies and practical applica- tions, we provide the entire original dataset to the deployed We designed three evaluation metrics. Metric A evaluates the large models information extraction capabilities, in- cluding extracting the cluster IP addresses and SSH port numbers from conversations, as well as the ability to deter- mine whether further execution is needed, evaluated through string matching. The challenge here is to assess the models ability to follow instructions and extract information, since logs are derived from user conversations and may contain unnecessary commands that need to be ignored during the determination process. Metric B evaluates the large models code generation capabilities in the diagnostic domain, in- cluding the ability to generate prescribed code based on descriptions given in conversations, control the input and output of the code, and create unseen test cases, imple- mented in a manner similar to human-eval(Chen et al., 2021) but transferred to a real distributed cluster. Metric C eval- uates the large models information attribution capabilities in the diagnostic domain, including the ability to provide attribution based on users error logs and information. This is currently implemented through multiple-choice questions. 5.1.3 Evaluation of Benchmark on Standard We applied this benchmark to several of the most widely used open-source LLMs, namely LLaMA3.1-70B, nemotron-70B(Adler et al., 2024), mistral-120B(Jiang et al., 2023), and llama3.2 3B. The results is in table 1. Due to the lack of relevant data and information, as well as reasoning logic such as DoT, all models were only able to complete the first task, scoring zero on the second and third tasks. Since the results of llama3.2 3B did not meet the minimum requirements for building the LLM-agent, and the 120B model is difficult to infer on a single AI server, we opted for the better-performing and more widely used LLama3.1-70B out of the two 70B models as the basis for subsequent SFT (Supervised Fine-Tuning) and the application of RAG, DoT, and selfplay.Table 1. Benchmarks Results on Open-source LLMs 70Bno yes 0.8658 0.0 0.0 70Bno yes 0.7315 0.0 0.0 120Bno no 0.7383 0.0 0.0 3Byes yes 0.047 0.0 0.0 Table 2. MMLU Benchmarks Results on LLama3.1 and Nemotron Model SFT or notMMLU We conduct two parts of experiments to comprehen- sively evaluate and compare the innovative effects of our work. In the first part, we use the mature and universal MMLU(Hendrycks et al., 2020) benchmark to evaluate the comprehensive ability of the model in basic text understand- ing after it has been enhanced by RAG, DoT, and self-play. In the second part, through ablation and comparison exper- iments, combined with the focus areas of the sub-items in our proposed benchmark, we quantitatively demonstrate the advantages of our three innovations. 5.2.2 General Capability Evaluation Based on MMLU Firstly, we aim to substantiate why SFT is not advisable in this domain. Although the LLM that supports the agent needs to possess extensive knowledge in cluster diagnostics, performance modeling, and code writing, we discovered that when the LLM reaches a level where this knowledge can be effectively applied, it often lacks the fundamental interaction capabilities required to engage with the agent. We illustrate this point using the MMLU benchmark. We converted the knowledge repository into tokens compatible with the model and constructed an instruction dataset. We iterated through multiple training rounds until the model could respond correctly to instructions. We thenEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Table 3. Multi Comprehensive Benchmarks Results on LLMs 70Bno 0.6246 0.8691 0.8786 0.3720 0.8230 70Byes 0.6032 0.8649 0.8862 0.3680 0.8007 70Bno 0.6280 0.8620 0.8780 0.3680 0.8234 70Byes 0.6126 0.8653 0.8859 0.3580 0.7917 120Bno 0.6544 0.8788 0.9012 0.3980 0.8229 3Bno 0.4352 0.7428 0.7835 0.2800 0.6040 evaluated the SFT model that reached this state against the original open-source model using the Multi-Machine Learning Understanding (MMLU) benchmark. The results are presented in Table 2. From the above results, it can be seen that Supervised Fine-Tuning (SFT) leads to a decline in performance when evaluated using general assessment methods such as MMLU. Subsequently, in our proposed cluster diagnostics benchmark, we further observed adverse consequences of this performance decline in metric C. As a result, we ultimately decided not to use the SFT approach to construct To avoid the potential risks associated with relying solely on MMLU, we further selected three additional LLM benchmarks that are closely related to the problems we aim to solve in our domain or are entirely generalizable: Abstraction and Reasoning Challenge(ARC)(Peter, 2022), BoolQ(Clark et al., 2019), and OpenbookQA(Mihaylov et al., 2018). The results are presented in the table 3. The results of this set of experiments support the conclusions we drew from the MMLU benchmark. 5.2.3 Results of Our Benchmark Table 4 presents all of our experimental results. The second column of the table indicates whether there was cheating. We define experiments that do not participate fairly in the benchmark as cheating. While this is unfair for the benchmark portion, it is clearly meaningful for our core research objective: to build an LLM-agent system that can autonomously and intelligently perform cluster diagnostics and troubleshooting. When evaluating the benchmark section, the cheating items can be considered as ground These experimental results can illustrate several conclusions.Table 4. Benchmarks Results on Open-source LLMs(baselines) 70BNone None 0.8658 0.0 0.0 plan)None 0.8658 0.4615 0.6470 70BNone SFT 0.0 0.0 0.0 planSFT 0.0 0.9230 0.0 70BNone RAG 0.8658 0.0 0.0 planRAG 0.8658 0.4615 0.7059 70BNone None 0.7315 0.0 0.0 planNone 0.7315 0.4615 0.7059 120BNone None 0.7383 0.0 0.0 planNone 0.7383 0.7692 0.8235 3BNone None 0.047 0.0 0.0 planNone 0.047 0.2307 0.1176 First, we found that a pre-defined plan can help a naive LLM control the agent. However, this plan was specifically written based on the benchmark questions and cannot beEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework used in a production environment. Correspondingly, all experiments utilizing DoT technology and not cheating scored well on metrics B and C for evaluating the agent, although the scores were slightly lower than those achieved with preplanning. This indicates that our proposed knowl- edge processing approach based on DoT and self-play can be used to control cluster troubleshooting agents. Second, we found that SFT significantly improved the scores on metric B, which focuses on evaluating code writing or the invocation of diagnostic tools. However, as a trade-off, all models that underwent SFT, even with preplanning, were unable to control the agent properly, resulting in poor performance on metric C. Third, we found that the results based on LLama3.1-70B were not significantly different from those of Mistral-120B, which has nearly twice the number of parameters. Twice the number of parameters implies double or more inference costs (considering multi-GPU linearity), making it impractical. On the other hand, the 3B smaller model, even with preplanning in a cheating scenario, is still unable to handle the task of We proceeded with subsequent experiments and actual deployment using the LLM-agent enhanced with the whole dataset and all of our innovative methods. 5.3 Intelligent Early Warning and Troubleshooting: A To demonstrate the superiority of the LLM-agent system we have built in the context of intelligent cluster diagnostics, we can present a concrete example to illustrate how the system operates and how it is more efficient and accurate compared to traditional methods. In the production environment of AI clusters, abnormal events or interruptions are not the most challenging problems to resolve. Clear information about anomalies or interruptions can effectively guide senior engineers in diagnosing the causes of issues. Current research is also progressively integrating technologies such as automatic restarts and automatic scheduling into the procedures for handling anomalies or interruptions in AI computing tasks. However, once an AI computing task exhibits slow performance, it becomes difficult to quickly identify the problem, and it is even harder to pinpoint the cause of the slowdown. Assume there is an AI training cluster composed of dozens of servers, where one of the servers suddenly experiences a performance drop. This could be due to various reasons, such as increased network latency, memory leaks, high CPU load, or insufficient storage space. Traditionally, administrators or engineers would check the log files of the cluster to manually identify possible issues. This would involve reviewing logs from different nodes, monitoring system metrics, attempting to reproduce the problem, andso on. This method is time-consuming and labor-intensive and may require multiple attempts to pinpoint the root cause. In our system, the LLM-agent automatically gathers relevant log information, performance metrics, and other necessary data from the nodes of the cluster. Leveraging the LLM-agents capabilities assessed through the benchmark, the system extracts useful information from the collected data, such as cluster IP addresses, SSH ports, and other crit- ical diagnostic details. Using its diagnostic capabilities in code generation and information attribution, the LLM-agent identifies the root cause of the issue based on the collected data and information. This may include generating new test cases to validate hypotheses. Once the problem is identified, the LLM-agent generates corresponding remediation scripts and requests human review. After approval, the LLM-agent executes the remediation measures in the cluster. Following the execution of remediation measures, the system collects data again to assess the outcome, forming a closed loop of data, algorithm, and hardware to optimize future diagnostic We manually constructed a scenario. This scenario would lead to slow performance in AI model training tasks and has repeatedly occurred in the development environment. We simulated an extreme heat situation with HV AC failure, throttling the frequency of one of the dozens of GPUs to approximately 200 MHz, rather than the 1410 MHz that the A800 GPUs should operate at. Observing the actual logs shows that the speed of this AI computing task decreased to approximately one-third of its normal performance. Our LLM-system initially flagged the slow AI task through power consumption monitoring and performance modeling results, triggering an automatic alert. Following this, through three rounds of self-play, it recommended checking the GPU core frequencies, a suggestion that the agent then dispatched for execution across all GPUs. Based on the execution results, the LLM accurately pinpointed the GPU with the low core frequency that we had specifically altered. The entire troubleshooting process took less than 10 minutes. In contrast, a senior operations engineer would typically need about one hour to correctly identify the problem and then use a pre-written automated detection software tool created by engineers to determine the specific GPU with the low-frequency fault. More importantly, our LLM-agent can identify the fault before algorithm engineers or operations engineers detect the slow-down phenomenon and automatically complete the repair. This achieves resolving the issue before the fault occurs, thereby enhancing the overall availability of the cluster.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework 5.4 Qualitative Analysis of Correctness, Safety, and Based on the existing research that is not yet fully mature, and in the context of this specific field of study, we provide reasonable definitions for correctness, safety, and reliability. In this study, we define correctness as whether the process and results of the LLM-agent executing tasks are correct. Compared to evaluating the output of the LLM, assessing the correctness of the LLM-agents actions is more challenging. An apparently incorrect operation process may produce the correct result, whereas seemingly perfect output at the textual level might lead to an erroneous result when executed. Since we focus on the field of cluster diagnostics with the actual output being the execution of procedures by the agent, we do not investigate the potential harmfulness or bias in the textual content generated by the LLM. Instead, we examine the ability of our LLM-agent to avoid performing harmful operations on the cluster when the information fed back to the agent changes, or even when malicious content is inserted by an attacker, such as deleting files, shutting down, overclocking, or modifying critical system configurations. Regarding reliability, we define it as the overall quality of fault handling by the LLM-agent compared to human engineers or expert human engineers. In addition to whether the attribution is correct, we also consider factors such as the time taken to complete typical fault handling, the resources consumed, and the ability to We incorporate the assessment of correctness into the benchmark evaluation. For the potential risks associated with the LLM-agent, we implement a whitelist plus human review approach. Initially, we ensure the safety of the existing toolkit, followed by creating a whitelist for the program interfaces included in the toolkit and conducting human reviews for the LLM-agents requests to execute self-authored code. Finally, we observed that the LLM-agent can attribute faults with an average of fewer than three test cases across multiple rounds of self-play, which is more efficient than the twelve cases typically required by human experts. However, regarding communication abilities, the LLM-agent currently does not possess such capabilities. The qualitative analysis described above is mainly aimed at reducing the probability of harmful incidents. Quantitative analysis or a comprehensive model still necessitates further advancements in the field of 6 CONCLUSION AND DISCUSSION 6.1 Work Summary and Further Plan Based on our experience and research in the fields of cluster diagnostics, LLM enhancement, and LLM-agent construc-tion, we innovatively proposed a system solution utilizing LLM-agents to autonomously and intelligently perform clus- ter troubleshooting. In terms of LLM algorithms, we intro- duced a benchmark consisting of 150 advanced problems manually crafted, demonstrating the performance differ- ences between our constructed LLM-agent and the original open-source LLMs under fair data conditions. In the realm of LLM-agent construction, we innovatively proposed inte- grating DoT reasoning mathematics and the ability to handle special symbols and formulas into the agent, enabling the LLM to operate machines at the software level and receive feedback. Ultimately, we applied our innovative achieve- ments to cluster diagnostics, exploring the potential in this field, and were pleasantly surprised to find that the LLM- agent systems, despite being in their extremely early stages, are already capable of handling repetitive and low-end tasks, thus freeing industry practitioners to tackle more challeng- ing and valuable problems. In the future, we will continue our work in four aspects. In terms of LLM algorithms, we will expand and upgrade the existing benchmark and build a more comprehensive and valuable metrics system. In the Agent field, we will further unlock the potential of DoT and make self-written code by the LLM gradually become the main execution body, re- ducing reliance on preset tools. At the system application level, we will form a closed loop of data, algorithm, and hardware, enriching the database with results from actual deployments. Finally, in terms of safety and reliability, we will continue to work with researchers in related fields to ensure and evaluate the safety and reliability of the agents. 6.2 Shortcomings and Limitations Our research still has shortcomings and limitations. In terms of shortcomings, our agent currently relies on a mechanism of human review to ensure safety, depends on pre-written tools for code, and relies on data sourced from GitHub as a starting point. An ideal LLM-agent system should form a self-sustained relationship with the AI cluster, maintaining In terms of limitations, our work depends on the LLM within the LLM-agent, but smaller models like llama3.2-3B cur- rently cannot support the capabilities of the agent. There- fore, our work can only be applied to data centers or large- scale distributed clusters and cannot be deployed in edge computing or personal computer scenarios. We need to continuously monitor the development of smaller models and explore the possibility of teaching the capabilities of the LLM-agent to smaller models in the form of DoT when appropriate.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Co- hen, J., et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704 , 2024. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Pod- stawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. InProceedings of the AAAI Conference on Artificial In- telligence , volume 38, pp. 17682 17690, 2024. Castelfranchi, C. Modelling social action for ai agents. Artificial intelligence , 103(1-2):157 182, 1998. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint Guo, C., Wu, H., Deng, Z., Soni, G., Ye, J., Padhye, J., and Lipshteyn, M. Rdma over commodity ethernet at scale. InProceedings of the 2016 ACM SIGCOMM Conference , Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring mas- sive multitask language understanding. arXiv preprint Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint Jiang, Z., Lin, H., Zhong, Y ., Huang, Q., Chen, Y ., Zhang, Z., Peng, Y ., Li, X., Xie, C., Nong, S., et al. {MegaScale }: Scaling large language model training to more than 10,000 {GPUs }. In 21st USENIX Symposium on Net- worked Systems Design and Implementation (NSDI 24) , pp. 745 760, 2024.Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual inter- national symposium on computer architecture , pp. 1 12, Jung, H. and Chung, K. Social mining-based clustering process for big-data integration. Journal of Ambient In- telligence and Humanized Computing , 12(1):589 600, Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K uttler, H., Lewis, M., Yih, W.-t., Rockt aschel, T., et al. Retrieval-augmented generation for knowledge- intensive nlp tasks. Advances in Neural Information Pro- cessing Systems , 33:9459 9474, 2020. Liu, Y ., Tao, S., Zhao, X., Zhu, M., Ma, W., Zhu, J., Su, C., Hou, Y ., Zhang, M., Zhang, M., et al. Coachlm: Automatic instruction revisions improve the data quality in llm instruction tuning. In 2024 IEEE 40th International Conference on Data Engineering (ICDE) , pp. 5184 5197. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint Ofenbeck, G., Steinmann, R., Caparros, V ., Spampinato, D. G., and P uschel, M. Applying the roofline model. In2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS) , pp. 76 85. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems , 35:27730 27744, 2022. Park, J. S., OBrien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology , pp. 1 22, 2023. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems , 32, 2019. Peter, E. Abstraction and reasoning challenge. 2022.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Qian, K., Xi, Y ., Cao, J., Gao, J., Xu, Y ., Guan, Y ., Fu, B., Shi, X., Zhu, F., Miao, R., et al. Alibaba hpn: a data center network for large language model training. In Proceedings of the ACM SIGCOMM 2024 Conference , Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. Shanley, T. InfiniBand network architecture . Addison- Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multi- billion parameter language models using model paral- lelism. arXiv preprint arXiv:1909.08053 , 2019. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test- time compute optimally can be more effective than scal- ing model parameters. arXiv preprint arXiv:2408.03314 , Team, G., Anil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971 , 2023. Wang, Q., Sang, B., Zhang, H., Tang, M., and Zhang, K. Dlrover: An elastic deep training extension with auto job resource recommendation. arXiv preprint Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824 24837, Xi, Z., Chen, W., Guo, X., He, W., Ding, Y ., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864 , 2023. Xiong, Y ., Jiang, Y ., Yang, Z., Qu, L., Zhao, G., Liu, S., Zhong, D., Pinzur, B., Zhang, J., Wang, Y ., et al. {SuperBench }: Improving cloud {AI}infrastructure reli- ability with proactive validation. In 2024 USENIX Annual Technical Conference (USENIX ATC 24) , pp. 835 850, Xu, Y ., Chen, Y ., Zhang, X., Lin, X., Hu, P., Ma, Y ., Lu, S., Du, W., Mao, Z., Zhai, E., et al. Cloudeval-yaml: Apractical benchmark for cloud configuration generation. Proceedings of Machine Learning and Systems , 6:173 Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y ., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems , 36, 2024. Zhang, Y ., Yuan, Y ., and Yao, A. C.-C. On the diagram of thought. arXiv preprint arXiv:2409.10038 , 2024. APLEASE ADD SUPPLEMENTAL MATERIAL Put anything that you might normally include after the refer- ences as an appendix here, not in a separate supplementary file. Upload your final camera-ready as a single pdf, includ-'},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'text': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification Yohei Seki1, Hakusen Shu2, Ana s Lhuissier3, Hanwool Lee4, Juyeon Kang3,Min-Yuh Day5,Chung-Chi Chen6 1Institute of Library, Information, and Media Science, University of Tsukuba, Japan, 2College of Knowledge and Library Sciences, School of Informatics, University of Tsukuba, Japan, 33DS Outscale, France,4Shinhan Securities Co., Korea, 5Graduate Institute of Information Management, National Taipei University, Taiwan6AIST, Japan Promises made by politicians, corporate lead- ers, and public figures have a significant impact on public perception, trust, and institutional rep- utation. However, the complexity and volume of such commitments, coupled with difficul- ties in verifying their fulfillment, necessitate innovative methods for assessing their credi- bility. This paper introduces the concept of Promise Verification, a systematic approach in- volving steps such as promise identification, evidence assessment, and the evaluation of tim- ing for verification. We propose the first mul- tilingual dataset, ML-Promise, which includes English, French, Chinese, Japanese, and Ko- rean, aimed at facilitating in-depth verification of promises, particularly in the context of En- vironmental, Social, and Governance (ESG) reports. Given the growing emphasis on corpo- rate environmental contributions, this dataset addresses the challenge of evaluating corpo- rate promises, especially in light of practices like greenwashing. Our findings also explore textual and image-based baselines, with promis- ing results from retrieval-augmented generation (RAG) approaches. This work aims to foster further discourse on the accountability of pub- lic commitments across multiple languages and In a world where promises shape perceptions and drive decisions, the integrity of commitments made by politicians, corporate leaders, and public fig- ures must be scrutinized. These promises, ranging from environmental sustainability to social respon- sibility and governance ethics, significantly influ- ence the general publics and stakeholders trust, as well as government and corporate reputations. Yet, the complexity and abundance of such commit- ments, coupled with the challenge of verifying their fulfillment, create a pressing need for innovative approaches to assess their strength and verifiabil- ity. Recognizing the critical role of transparencyand accountability in todays society, we propose a groundbreaking task: Promise Verification. To perform promise verification, several steps are required, including (1) identifying the promise, (2) linking the promise with supporting evidence, (3) assessing the clarity of the promise-evidence pair, and (4) inferring the timing for verifying the promise. For example, after identifying a promise, the availability of evidence to support that the speaker is taking action to fulfill the promise could serve as a coarse-grained evaluation of the promises quality. However, the clarity of the ev- idence may also affect the quality of the promise, which would be a fine-grained evaluation. Addi- tionally, whether the speaker provides a clear time- line for verifying the promise is an important crite- rion. For instance, we will achieve net zero carbon emissions within five years is a stronger promise than we will achieve net zero carbon emissions. Following this line of thought, this paper proposes the first multilingual dataset for in-depth promise verification, including Chinese, English, French, In recent years, increasing emphasis has been placed on companies environmental contributions, especially in addressing climate change, deforesta- tion, and compliance with labor conditions and gov- ernance, when evaluating their investment value. In the evolving landscape of ESG (environmental, social, and governance) criteria, the ability to accu- rately assess a companys promises and adherence to its ESG promises has become paramount. How- ever, unlike traditional financial statements, ESG reports still lack clear standards regarding corporate promises. This allows some companies to use mis- leading information to project an overly positive environmental image, a practice known as green- washing. As Gorovaia and Makrominas (2024) points out, companies involved in environmental misconduct tend to produce longer, more positive, and more frequent reports. We hypothesize that 7 Nov 2024Task Label English French Chinese Japanese Korean Promise IdentificationYes 84.5 80.5 40.2 74.9 77.5 No 15.5 19.5 59.8 25.1 22.5 Supporting EvidenceYes 20.1 71.6 20.1 66.4 75.6 No 79.9 28.4 79.9 33.6 24.4 Clarity of Promise-Evidence PairClear 53.3 56.6 64.7 61.2 94.8 Not Clear 42.9 41.9 35.3 34.7 5.2 Misleading 3.8 1.5 0.0 4.1 0.0 Timing for VerificationWithin 2 years 1.9 12.4 37.5 7.3 45.5 2-5 years 14.1 15.0 10.0 9.3 8.4 Longer than 5 years 9.0 21.6 15.0 18.7 17.5 Other 75.0 51.0 37.5 64.7 28.7 Table 1: Label distribution in each language. (%) such reports may lack substantive evidence, or the information presented may be irrelevant or ambigu- ous, leading to misinterpretation. To this end, the proposed dataset, ML-Promise, focuses on ESG reports released by corporations in five countries: the U.K., France, Taiwan, Japan, and Korea. In addition to exploring text-based baselines, we also provide pilot results on image-based experi- ments, as most reports are released in PDF format. Our experiment further shows that the retrieval- augmented generation (RAG) approach (Lewis et al., 2020) can help in some language cases. Al- though we do not find a silver bullet for all lan- guages and tasks, we hope the proposed dataset can open a new chapter in discussions on the respon- sibility of promises, especially those from public Recent studies have sought to improve the anal- ysis of ESG or sustainability reports for estimat- ing company values using contextual embedding approaches. For example, Gutierrez-Bustamante and Espinosa-Leal (2022) evaluated sustainability reports from publicly listed companies in Nordic countries using latent semantic analysis (LSA) and the global vectors for word representation (GloVe) model, enhancing document retrieval performance based on similarity. Garigliotti (2024) explored the integration of sustainable development goals (SDGs) into environmental impact assessments (EIAs) using a RAG framework powered by large language models (LLMs). Their work focused on two tasks: detecting SDG targets within EIA reports and identifying relevant textual evidence, specifically in European contexts. Hillebrand et al. (2023) introduced sustain.AI, a context-aware rec- ommender system designed to analyze sustainabil- ity reports in response to increasing corporate so-cial responsibility (CSR) regulations. The system, based on a BERT architecture, identified relevant sections of lengthy reports using global reporting initiative (GRI) indicators and demonstrated strong performance on datasets from German companies. Previous studies have a few shortcomings. First, most of them focus solely on reports from one country. Second, none of them attempt to analyze corporate promises, despite the abundance of sus- tainability reports. To address these problems, our study extends these works by focusing on multi- lingual companies from both European and Asian regions, including Taiwan, the UK, France, Japan, and Korea. With the proposed new task, we aim to highlight the importance of anti-greenwashing by evaluating corporate promises in ESG reports. We collect ESG reports from five countries: the UK, France, Taiwan, Japan, and Korea. The annotators are native speakers of the target language or are familiar with the language at the work level. The task designs are as follows when given a paragraph in the ESG reports. 1.Promise Identification : This is a boolean label (Yes/No) based on whether a promise exists. 2.Supporting Evidence : This is a boolean label (Yes/No) based on whether supporting evidence exists. 3.Clarity of the Promise-Evidence Pair : We designed three labels (Clear/Not Clear/Misleading) for this task, which should depend on the clarity of the given evidence in relation to the promise. 4.Timing for Verification : Following the MSCI guide- lines and previous work (Tseng et al., 2023), we set timing labels (within 2 years/2-5 years/longer than 5 years/other) to indicate when readers/investors should return to verify the promise. Here, other denotes the promise has already been verified or doesnt have a specific timing to verify it.Approach Task English French Chinese Japanese Korean w/o RAGPromise Identification (PI) 0.842 0.816 0.521 0.670 0.849 Supporting Evidence (SE) 0.680 0.746 0.163 0.720 0.792 Clarity of Promise-Evidence Pair (CPEP) 0.411 0.443 0.569 0.450 0.897 Timing for Verification (TV) 0.636 0.523 0.317 0.632 0.406 w/ RAGPromise Identification 0.866 0.798 0.540 0.659 0.807 Supporting Evidence 0.757 0.732 0.503 0.850 0.774 Clarity of Promise-Evidence Pair 0.467 0.487 0.628 0.465 0.939 Timing for Verification 0.693 0.601 0.469 0.684 0.571 Table 2: Experimental Results (F1-Score). The best performance in each language is denoted in bold . Finally, we obtained 3,010 instances, i.e., 600 for each language and 10 additional instances in the Chinese dataset. The Cohens agreement (Co- hen, 1960; McHugh, 2012) for these tasks is ap- proximately 0.65-0.96, 0.71-0.88, 0.62-0.80, and 0.60-0.89, respectively. Table 1 presents the distri- bution of the proposed ML-Promise dataset. First, we observe that around 35-40% of the evidence is not clear in supporting the associated promises in four out of five languages. This highlights the necessity of the proposed task for evaluating the quality of the promise-evidence pairs from corpora- tions. Furthermore, about 4% of instances contain (potentially) misleading evidence in the English and Japanese datasets. It is crucial for corporations to re-examine this evidence, and it is also essen- tial for supervisory authorities to monitor these instances. Second, we noted that corporations in Taiwan and Korea tend to make more short-term promises (within 2 years), whereas corporations in the remaining countries tend to make longer-term promises. This finding shows the need for a multi- lingual comparison of ESG reports across different countries, as the narrative styles vary among them. RAG (Lewis et al., 2020) was introduced as a method to enhance LLMs by integrating external knowledge sources. This approach combines re- trieval mechanisms with generative models, pro- ducing more accurate and contextually relevant outputs. Yu et al. (2024) highlights the advantages of RAG systems, particularly their ability to extract domain-specific information. By incorporating ex- ternal retrieval processes, RAG enables generative models to access a broader, field-specific knowl- edge base, improving the accuracy and relevance of responses. This capability is especially impor- tant for handling domain-specific queries, an areaRAG TaskChinese Korean Image-Based Text-Based Image-Based Text-Based w/oPI 0.530 0.521 0.837 0.849 SE 0.124 0.163 0.812 0.792 CPEP 0.510 0.569 0.922 0.897 TV 0.202 0.317 0.201 0.406 w/PI 0.580 0.540 0.843 0.807 SE 0.512 0.503 0.845 0.774 CPEP 0.618 0.628 0.893 0.939 TV 0.297 0.469 0.330 0.571 Table 3: Image-based experimental results. Bolded denotes the best performance in each language. Underlined denotes performance with RAG better than where existing LLMs often encounter difficulties. Fan et al. (2024) discusses training strategies for RAG, including independent, sequential, and joint methods, which can be tailored to optimize retrieval and generation for specific domains. For Chinese language applications, Wang et al. (2024b) empha- sizes the importance of domain-specific corpora over general knowledge sources. Ardic et al. (2024) applied RAG to analyze sustainability reports from ten Turkish companies, focusing on ESG factors. The study found BM25 outperformed BERTurk in retrieving relevant sections, highlighting the effec- tiveness of advanced retrieval techniques. Following the findings of previous studies, we also explore and design the RAG approach for the proposed tasks. Specifically, when given a para- graph, we first retrieve the six most similar samples in the training set. We leveraged Multilingual E5 Text Embeddings (Wang et al., 2024a) to calcu- late the cosine similarity between target paragraphs and instances from the training set. Then, we pro- vide the top-six examples for the LLM to perform in-context learning (Dong et al., 2022). In our ex- periment, we use GPT-4o as the base LLM. In the experiment, we randomly select 200 in- stances from each language as the test set, and the remaining instances are used for training. We use the F1 score to evaluate the performance of eachtask. Table 2 shows the performance of each task in each language. First, the performance of most tasks improves when adopting RAG. Specifically, for En- glish and Chinese, all tasks perform better when using RAG. Second, RAG enhances performance in estimating the clarity of the promise-evidence pair and inferring the timing for verification, regard- less of the language used. These results suggest the usefulness of RAG in these two novel tasks. Addi- tionally, the findings demonstrate the value of the proposed annotations. With the proposed dataset, the performance of fine-grained promise evaluation can be improved. Third, although the performance in promise identification and supporting evidence identification tasks may slightly decrease in French, Japanese, and Korean, the declines are minimal (less than 2% in most cases). These results suggest that the method for retrieving and suggesting sam- ples similar to the paragraph requires refinement for imbalanced boolean datasets. In future work, we will focus on improving the RAG approach by extracting balanced samples, particularly for minor We noticed a significant difference between Tai- wan/Korea reports and the reports from other coun- tries.1The reports from these two countries uti- lize a large number of graphs instead of textual descriptions. This observation raises the question of whether we could use multimodal LLMs to read PDF files directly instead of relying on extracted text. To explore this, we align the annotations with a PDF page and employ GPT-4o to reassess the tasks using an image as input. For RAG, we lever- aged E5-V Universal Embeddings (Jiang et al., 2024) to calculate the cosine similarity between target pages and instances from the training set. Table 3 presents the performance. First, using GPT-4o with image input reduces performance in three out of four tasks in the Chinese dataset and in two out of four tasks in the Korean dataset. Sec- ond, RAG improves the performance of most tasks when using image input. Third, with RAG, the per- formance in promise identification and supporting evidence identification tasks improves with Chi- nese image input, and the performance of support- ing evidence identification improves with Korean image input. However, for estimating the clarity of 1We provide some examples in Appendix A.Input RAG Task ROUGE-L Table 4: Results of promise and evidence extraction. the promise-evidence pair and inferring the timing for verification, using text input with RAG remains superior. In summary, our experimental results sug- gest that image input should be used for PI and SE tasks, while text input is preferable for CPEP and TV tasks. Additionally, RAG performs well regardless of input type. 5.2 Promise and Evidence Extraction In the previous section, we explored the promise and evidence identification tasks. However, the task can also be formulated in an extractive man- ner. Instead of only outputting a yes or no, we can also ask models to extract the promise and evidence from the report. We provide additional annotations in the Chinese dataset and experiment in multimodal settings with and without RAG. The ROUGE-L (Lin, 2004) score is used to evaluate ex- traction performance. Table 4 presents the results. These results indicate that the best performance is achieved in the image-based setting with RAG for both promise and evidence extraction. This empha- sizes the importance of exploring multimodal input for ESG report understanding. In this paper, we introduce the concept of Promise Verification, a novel task aimed at evaluating the credibility and fulfillment of promises made by corporations, particularly in the context of ESG reports. We propose the first multilingual dataset, ML-Promise, to emphasize the importance of assessing corporate environmental and social promises. Our results demonstrate that RAG im- proves performance, while also showing the po- tential of multimodal approaches in promise ver- ification. Our annotations will be released under the CCBY-NC-SA 4.0 license. We hope this work serves as a foundation for the robustness of promise verification systems and contributes to greater ac- countability in corporate and public disclosures.Limitation Several limitations warrant discussion. First, al- though the ML-Promise dataset includes five lan- guages?Chinese, English, French, Japanese, and Korean?its scope is still limited to a few countries and may not fully capture the diversity of corpo- rate promise communication styles globally. The dataset focuses on ESG reports from specific re- gions, which may limit the generalizability of the findings to other languages and cultural contexts. Future studies can follow our design to expand the dataset to include more regions and languages, which could enhance the robustness and applicabil- ity of the proposed methods. Second, although the study uses RAG to improve performance, the re- sults show that this approach does not consistently outperform baseline models across all languages and tasks. These inconsistencies suggest that RAG may require further optimization or task-specific adjustments, particularly in handling the nuances of each language and dataset structure. These limitations and our findings highlight ar- eas for future research, including expanding the dataset, refining the RAG approach, enhancing mul- timodal learning, and addressing the inherent am- biguities in corporate ESG reporting. Ozgur Ardic, Mahiye Uluyagmur Ozturk, Irem Demir- tas, and Secil Arslan. 2024. Information Extraction from Sustainability Reports in Turkish through RAG Approach. In 2024 32nd Signal Processing and Com- munications Applications Conference (SIU) , pages Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhi- fang Sui. 2022. A Survey on In-context Learning. arXiv preprint arXiv:2301.00234 . Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs: To- wards Retrieval-Augmented Large Language Models. Dario Garigliotti. 2024. SDG target detection in envi- ronmental reports using Retrieval-augmented Gener- ation with LLMs. In Proceedings of the 1st Work- shop on Natural Language Processing Meets Cli- mate Change (ClimateNLP 2024) , pages 241 250, Bangkok, Thailand. Association for Computational Linguistics.Nina Gorovaia and Michalis Makrominas. 2024. Identi- fying greenwashing in corporate-social responsibility reports using natural-language processing. European Marcelo Gutierrez-Bustamante and Leonardo Espinosa- Leal. 2022. Natural Language Processing Meth- ods for Scoring Sustainability Reports? A Study of Nordic Listed Companies. Sustainability , 14(15). Lars Hillebrand, Maren Pielka, David Leonhard, To- bias Deu er, Tim Dilmaghani, Bernd Kliem, R diger Loitz, Milad Morad, Christian Temath, Thiago Bell, Robin Stenzel, and Rafet Sifa. 2023. sustain.AI: a Recommender System to analyze Sustainability Re- ports. In Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law , ICAIL 23, page 412?416, New York, NY , USA. Association Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. 2024. E5-V: Universal Embeddings with Multimodal Large Language Mod- Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich K ttler, Mike Lewis, Wen-tau Yih, Tim Rock- t schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge- intensive NLP tasks. In Proceedings of the 34th International Conference on Neural Information Pro- cessing Systems , Red Hook, NY , USA. Curran Asso- Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out , pages 74 81, Barcelona, Spain. Association for Computational Linguistics. Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica , 22(3):276 282. Yu-Min Tseng, Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen. 2023. DynamicESG: A Dataset for Dynamically Unearthing ESG Ratings from News Articles. In Proceedings of the 32nd ACM Interna- tional Conference on Information and Knowledge Management , pages 5412 5416. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024a. Multi- lingual E5 Text Embeddings: A Technical Report. arXiv preprint arXiv:2402.05672 . Shuting Wang, Jiongnan Liu, Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yu- tao Zhu, and Zhicheng Dou. 2024b. Domain- RAG: A Chinese Benchmark for Evaluating Domain- specific Retrieval-Augmented Generation. Preprint , Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of Retrieval- Augmented Generation: A Survey. Preprint , We provide five ESG report exam- ples in this section, and please refer to our training set for more instances:'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_rag_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OPENAI API KEY\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OPENAI API KEY\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Chunking and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into chunks for embeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = []\n",
    "\n",
    "for paper in top_5_rag_papers:\n",
    "    chunks = text_splitter.split_text(paper[\"text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append({\n",
    "            \"paper_id\": paper[\"paper_id\"],\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"content\": chunk,\n",
    "            \"chunk_id\": i\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1: 47 chunks\n",
      "Paper 66: 78 chunks\n",
      "Paper 77: 17 chunks\n",
      "Paper 80: 61 chunks\n",
      "Paper 87: 25 chunks\n"
     ]
    }
   ],
   "source": [
    "# Counts the number of chunks for each paper\n",
    "\n",
    "chunk_counts = Counter(doc[\"paper_id\"] for doc in documents)\n",
    "\n",
    "# Print results\n",
    "for paper_id, count in chunk_counts.items():\n",
    "    print(f\"Paper {paper_id}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation Robin D. Pesl1, Jerin G. Mathew2, Massimo Mecella2, and Marco Aiello1 1University of Stuttgart, Stuttgart, Germany 2Sapienza Universit di Roma, Rome, Italy Abstract Integrating multiple (sub-)systems is essential to create ad- vanced Information Systems (ISs). Difficulties mainly arise when integrat- ing dynamic environments across the IS lifecycle, e.g., services not yet existent at design time. A traditional approach is a registry that provides the API documentation of the systems endpoints. Large Language Mod- els (LLMs) have shown to be capable of automatically creating system integrations (e.g., as service composition) based on this documentation but require concise input due to input token limitations, especially regard- ing comprehensive API descriptions. Currently, it is unknown how best to preprocess these API descriptions. Within this work, we (i) analyze the usage of Retrieval',\n",
       "  'chunk_id': 0},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'best to preprocess these API descriptions. Within this work, we (i) analyze the usage of Retrieval Augmented Generation (RAG) for endpoint discovery and the chunking, i.e., preprocessing, of state-of-practice OpenAPIs to reduce the input token length while preserving the most relevant informa- tion. To further reduce the input token length for the composition prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that only receives a summary of the most relevant endpoints and retrieves specification details on demand. We evaluate RAG for endpoint discovery using the RestBench benchmark, first, for the different chunking possibil- ities and parameters measuring the endpoint retrieval recall, precision, and F1 score. Then, we assess the Discovery Agent using the same test set. With our prototype, we demonstrate how to successfully employ RAG for endpoint discovery to reduce the token count. While revealing high values for recall, precision, and F1, further research is',\n",
       "  'chunk_id': 1},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'the token count. While revealing high values for recall, precision, and F1, further research is necessary to retrieve all requisite endpoints. Our experiments show that for preprocessing, LLM-based and format-specific approaches outperform na ve chunking methods. Relying on an agent further enhances these results as the agent splits the tasks into multiple fine granular subtasks, improving the overall RAG performance in the token count, precision, and F1 score. Keywords: Retrieval augmented generation Large language models OpenAPI Endpoint discovery RestBench. OpenAPI is the state-of-practice for describing interfaces for integrating Informa- tion Systems (ISs). It contains formal elements like paths and natural language 29 Nov 20242 Pesl et al. constituents like descriptions. For integrating these systems automatically, au- tomated service composition using Large Language Models (LLMs) has been proposed [ 27,28,29]. These approaches exploit the capabilities of LLMs to process formal',\n",
       "  'chunk_id': 2},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'has been proposed [ 27,28,29]. These approaches exploit the capabilities of LLMs to process formal and natural language input, combining them with the inherent nature of automated service composition of decoupling and independent lifecycle manage- ment. While prohibiting any manual modeling effort by relying on already broadly available OpenAPIs, the approaches face the challenge of limited input token length [29]. This bounds the quantity and extent of the input service description. Even for proprietary models with a large input token context, e.g., OpenAIs GPT4 with a context size of 128,000 tokens [ 23], an economic constraint emerges as these models are paid by input and output token count. Therefore, a smaller prompt length is beneficial to (i) insert further service documentation and (ii) reduce costs for proprietary models. To address these challenges, Retrieval Augmented Generation (RAG) [ 16] has emerged as a promising resort. In this approach, the external information is',\n",
       "  'chunk_id': 3},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': '(RAG) [ 16] has emerged as a promising resort. In this approach, the external information is collected in a database, typically structured as a set of documents or document chunks. The primary goal is retrieving only a small subset of the most relevant documents or document chunks, which is then inserted into the prompt [ 16]. How to optimally apply RAG for endpoint discovery in IS is open to investigation, leading to the following research questions: RQ1.How best to preprocess, i.e., chunk, OpenAPIs for RAG endpoint discovery? RQ2.Can LLM agents be employed to reduce token count further and improve To answer RQ1, we develop an OpenAPI RAG system that takes as input service descriptions. We apply different token-based and LLM-based chunking strategies to split the documentation and evaluate them based on retrieval quality. The token-based strategies process the document using a classical parser and then split the parts, e.g., endpoints, into equal-sized chunks. The LLM-based',\n",
       "  'chunk_id': 4},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'classical parser and then split the parts, e.g., endpoints, into equal-sized chunks. The LLM-based strategies let an LLM create a description, i.e., a summary or a question, for each endpoint and then use these descriptions for similarity matching. We employ mainstream open-source and proprietary embedding models for similarity matching, which can create an embedding vector for an input. The similarity between two inputs can then be determined by comparing their embedding vectors using, e.g., the cosine similarity. We evaluate the OpenAPI RAG and the different chunking strategies by relying on the already available RestBench benchmark [ 35] for LLMs agents, measuring recall, precision, and F1 score for each chunking strategy. The benchmark consists of the OpenAPI descriptions of Spotify and TMDB and corresponding queries, each with a set of endpoints as the sample solution. To address RQ2, we propose an LLM agent called Discovery Agent . As LLM agents allow the usage of external',\n",
       "  'chunk_id': 5},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'RQ2, we propose an LLM agent called Discovery Agent . As LLM agents allow the usage of external tools, we first investigate using one tool that simply inputs the results of the RAG to the prompt. Then, we experiment with using two tools: the first tool filters and enters the LLM endpoint summaries to the prompt using RAG, while the second tool allows the retrieval of the endpoint details on demand. We resort to the same RestBench benchmark for evaluation and measure recall, precision, F1 score, and additional token count.Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 3 As the chunking strategy, we rely on the LLM-based summary strategy with OpenAIs text-embedding-3-large embedding model [24]. The remainder of the paper is structured as follows. First, we provide an overview of related works regarding service discovery and LLMs in Section 2. Then, we present how to use RAG for endpoint discovery and the OpenAPI chunking strategies in Section 3. We evaluate and discuss',\n",
       "  'chunk_id': 6},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'for endpoint discovery and the OpenAPI chunking strategies in Section 3. We evaluate and discuss the RAG and the different chunking strategies in Section 4. Finally, we conclude with Section 5. Regarding endpointdiscovery,we provide abrief overview ofthe essential concepts of the various service discovery approaches. Additionally, we provide relevant insights into LLMs and the novel approach of integrating LLMs with tools, known as LLM agents, and how they relate to our approach. The most common service discovery implementation is a service registry, which collects information about available services and offers search facilities. This service registry is usually backed by a component residing at the middleware or application level [ 15]. It is characterized by the syntax used to describe the services and their invocation and the expressive power of the available query language. The typical integration model is a pull model where service consumers search for the required services.',\n",
       "  'chunk_id': 7},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'typical integration model is a pull model where service consumers search for the required services. Less is a push model as used in the UPnP protocol, where service providers regularly advertise their services [32]. In the early days of XML-based services, the infrastructure for service discov- ery was the Universal Description, Discovery, and Integration (UDDI) specifi- cation [7]. UDDI had a global incarnation called the UDDI Business Registry (UBR), intended to offer an Internet-wide repository of available web services and promoted by IBM, Microsoft, and SAP. Unfortunately, UBR never gained widespread adoption and was short-lived (2000-2006). Significant research in the early days focused on enhancing service discovery on UDDI, improving search capabilities, and creating federated registries, e.g., [ 3,4,11]. Alternatively, WS-Discovery is a multicast protocol that finds web services on a local network. Nowadays, OpenAPI is the de facto standard for describing services. While not',\n",
       "  'chunk_id': 8},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'on a local network. Nowadays, OpenAPI is the de facto standard for describing services. While not offering a discovery protocol and mechanism, given its popularity, OpenAPI would also benefit from discovery [ 34]. So, additional infrastructure for discovery has been proposed, such as centralized repositories (SwaggerHub or Apiary), service registry integration (Consul, Eureka), API Gateways (Kong, Apigee), or Populating registries of services requires effort from service providers, which often hinders the success of such approaches, especially if the service provider is expected to provide extensive additional information beyond the service endpoints. This additional effort has often been the reason for the failure of some of these technologies, most notably UBR. Approaches confined to specific applications,4 Pesl et al. domains, or enterprises have been more successful, e.g., Eureka. Developed by Netflix as part of its microservices architecture [ 36], Eureka helps clients find',\n",
       "  'chunk_id': 9},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'Developed by Netflix as part of its microservices architecture [ 36], Eureka helps clients find service instances described by host IP, port, health indicator URL, and home page. Developers can add additional data to the registry for extra use cases. extensive modeling, e.g., as semantic annotations. Hence, our approach relies on already broadly available state-of-practice OpenAPI specification and their 2.2 Large Language Models (NLP) and machine learning field [ 1,2,13]. Often containing billions of parameters, these models are trained on extensive text corpora to generate and manipulate human-like text [ 30]. They are primarily based on an encoder-decoder architecture tasks using decoder-only models such as GPT [ 31]. Usually, the input is a natural language task called prompt, which first needs to be translated to a sequence of input tokens. The model processes this prompt and returns an output token sequence, which can then be translated back to a natural language answer. As',\n",
       "  'chunk_id': 10},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'an output token sequence, which can then be translated back to a natural language answer. As these models can, in general, capture intricate linguistic nuances and semantic contexts, they can be applied to a wide range of tasks, e.g., in software engineering [ 10]. LLMs can be used to create integration based on endpoint documentation automatically [ 27,28,29]. Yet, these face strict input token limitations, e.g., 128,000 tokens for current OpenAI models [23,29]. With this paper, we analyze how RAG can be used to preprocess API documentation to mitigate this issue. Another approach is encoder-only models such as BERT [ 8], often referred to as embedding models. They allow condensing the contextual meaning of a text into a dense vector, termed embedding. Using similarity metrics such as dot product, cosine similarity, or Euclidean distance allows for assessing the similarity of two input texts. Embedding models are usually used for the similarity search in RAG systems [6], which we',\n",
       "  'chunk_id': 11},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'texts. Embedding models are usually used for the similarity search in RAG systems [6], which we also do in our implementation. LLMs have shown remarkable capabilities in solving complex tasks by decompos- ing them in a step-by-step fashion [ 38] or by exploring multiple solution paths simultaneously [ 41]. Typically, these plans are generated iteratively by using the history of the previously generated steps to guide the generation of the next step. Additionally, recent studies have shown the potential of providing LLMs access to external tools to boost their reasoning capabilities and add further knowledge. This approach consists of prompting the LLM to interact with external tools to solve tasks, thus offloading computations from the LLM to specialized functions. Notable examples of such tools include web browsers [ 20], calculators [ 5], andAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 5 Python interpreters [ 12]. In practice, a tool is usually a Python function,',\n",
       "  'chunk_id': 12},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'Generation 5 Python interpreters [ 12]. In practice, a tool is usually a Python function, which can be called during the interaction with the LLM. The LLM agent paradigm [ 19,22,40] combines the concepts of (i) external tool usage, (ii) the planning capabilities of LLMs, and adds a shared (iii) memory, to solve complex tasks. Given an input task, an LLM agent uses its reasoning capabilities to decompose the task into a set of simpler subtasks. For each subtask, the LLM finds and interacts with the set of tools to solve the subtask. Then, based on the outcome of the current task and the history of previously executed subtasks, the LLM agent generates a new subtask and repeats the steps mentioned above or terminates if the original task is solved. To instruct the processing, the outcome of the tool invocations and the history of the subtasks are stored in the memory, typically consisting in the LLM agents own context. Within this work, we apply the LLM agent paradigm to create the',\n",
       "  'chunk_id': 13},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'in the LLM agents own context. Within this work, we apply the LLM agent paradigm to create the Discovery Agent as an LLM agent for endpoint discovery. A critical challenge for LLM agents is the accessibility to a set of common APIs and tasks for their evaluation, e.g., tested using benchmarks like API Bank [17] or RestBench [ 35]. API Bank is a benchmark to evaluate the tool use of an LLM consisting of a set of APIs exposed through a search engine. Unfortunately, the available code of the benchmark is incomplete. The RestBench benchmark contains a collection of tasks and endpoints expressed using the OpenAPI specification of Spotify and TMDB [ 35]. As the currently most extensive available benchmark, we employ RestBench to validate our results. OpenAPIs within LLM agents have been used in RestGPT [ 35] and Chain of Tools [ 33]. The former combines multiple LLM agents to solve complex tasks by interacting with a set of tools exposed using the OpenAPI specification. The latter solves an',\n",
       "  'chunk_id': 14},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'by interacting with a set of tools exposed using the OpenAPI specification. The latter solves an input query by framing the problem as a code generation task and interacts with the set of tools to generate Python code to solve the query. In contrast, our Discovery Agent does not directly interact with the endpoints stated in the OpenAPIs. Instead, it filters and returns matching endpoints that can be used for subsequent processing. Even when considering the similarity to the tool selection within LLM agents, the task of selecting a set of tools from a larger pool to solve a specific problem remains relatively underexplored [ 42]. Existing research primarily focuses on the a priori selection of human-curated tools [ 25], heuristic-based methods for tool selection [ 18], choosing the relevant tool by scoring each query against every tool based semantic retrieval using a combination of different vector databases [ 42]. With our work, we contribute the analysis of preprocessing OpenAPIs',\n",
       "  'chunk_id': 15},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'vector databases [ 42]. With our work, we contribute the analysis of preprocessing OpenAPIs into this We first introduce the general architecture to employ RAG for endpoint discovery. As state-of-practice for service documentation, we then investigate how to chunk OpenAPIs as preprocessing for RAG.6 Pesl et al. 3.1 RAG for Endpoint Discovery RAG comprises a preprocessing step ahead of the answer generation of an LLM to a semantic search based on some knowledge sources. Usually, the semantic search is done by embedding similarity, and the data from the knowledge sources is reduced to small chunks to allow fine-grained information retrieval [16]. Figure1. RAG for Endpoint Discovery Figure 1 depicts the application of RAG for endpoint discovery, i.e., the OpenAPI RAG. Initially, the chunking strategy determines how the chunks are created from the OpenAPIs, i.e., how many chunks are created and what they contain. Each chunk has an embedding as metadata for similarity search in addition to',\n",
       "  'chunk_id': 16},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'and what they contain. Each chunk has an embedding as metadata for similarity search in addition to its content. The chunking strategy specifies which data is used as input to the embedding model to create the embedding. This input does not have to match the chunk content, e.g., it can be a summary instead of the entire content. The chunks are finally stored in the chunk database. For retrieval, the user submits in 1a natural language query qto the chunk retriever, which converts qinto the embedding eusing the same embedding model as for the chunk creation. In 2, the chunk retriever queries the chunk database using e. The chunk database compares eusing a similarity metric with the embeddings of the service chunks contained in the database. The results are the top kmost similar chunks according to the metric, which are then returned to the chunk retriever in 3. Finally, in 4, the chunk retriever forwards the retrieved results to the user, who can add them to their prompt either',\n",
       "  'chunk_id': 17},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'chunk retriever forwards the retrieved results to the user, who can add them to their prompt either manually or automatically through integration into their tooling. The benefit of employing RAG is the insertion of only the gist of the available information, which allows picking more and only the most relevant information for the fix LLM context size. A drawback is that, based on the retrieval algorithm, not all relevant information may be retrieved. Further, fixing kreveals the advantage of controlling the result size. An alternative would be to return all chunks about a certain similarity threshold, introducing the question about the optimal cutoff. Figure 2 shows how the Discovery Agent extends on the RAG from Figure 1 shown in yellow hued. Instead of passing qto the RAG, the user submits it in1to the Discovery Agent, which then iteratively decomposes qinto a set of fine-grained tasks in 2. Breaking down the query into smaller, more manageable tasks can potentially fill the gap',\n",
       "  'chunk_id': 18},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'in 2. Breaking down the query into smaller, more manageable tasks can potentially fill the gap between the coarse semantics of the query and the specificities in the services documentation. In 3, the Discovery AgentAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 7 Figure2. Overview of the Discovery Agent Approach for Endpoint Discovery submits each task to the RAG to retrieve the set of relevant chunks to solve the current task specifically. Finally, in 4, the Discovery Agent collects the retrieval results of each individual task, filters them, and repeats 2ifqneeds further processing or returns the results to the user in 5. 3.2 OpenAPI Chunking Strategies A critical step in the RAG workflow is creating the chunks for the chunk database. registries can contain tens of thousands of services, each containing multiple potentially lengthy endpoints due to detailed descriptions or extensive input and output schemas. So, a single service might not fit into the context size of',\n",
       "  'chunk_id': 19},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'or extensive input and output schemas. So, a single service might not fit into the context size of the embedding model or even exceed the limit of the LLM that further processes the output of the RAG system. In addition, service documentation can also feature additional metadata that, while valuable for understanding service details, is not necessarily relevant for composing services to solve a query. Table 1. Implemented Chunking Strategies Category Splitting Refinement Meta-Parameters Token-based No split Token chunking m(model), s(chunk size), l(overlap) Endpoint split Token chunking m(model), s(chunk size), l(overlap) Endpoint split Remove examples m(model) Endpoint split Relevant fields m(model) JSON split Token chunking m(model), s(chunk size), l(overlap) LLM-based Endpoint split Query m(model) Endpoint split Summary m(model) To determine advantageous chunking strategies, we employ the seven different and output is a list of chunks. The chunking strategies can be categorized',\n",
       "  'chunk_id': 20},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'the seven different and output is a list of chunks. The chunking strategies can be categorized into token-based andLLM-based strategies. Each strategy consists of a splitting method, which dissects the OpenAPI specification into a list of intermediate chunks, and another refinement step, which converts the intermediate chunks to the final list of chunks. In addition, there is the meta-parameter for the used embedding model m. For the token chunking refinement step, there is also the8 Pesl et al. chunk size sin tokens and their overlap l, i.e., how many tokens two consecutive chunks share, in tokens. For the token-based approaches, we consider three main splitting methods. Theno split method returns a single intermediate chunk for each OpenAPI containing the whole specification. The endpoint split divides the OpenAPI into one chunk per endpoint. The JSON split is a built-in LlamaIndex3splitting strategy tailored to files in the JSON format. This strategy parses the JSON file and',\n",
       "  'chunk_id': 21},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'strategy tailored to files in the JSON format. This strategy parses the JSON file and traverses it using depth-first search, collecting leaf nodes, i.e., key-value pair where the value is a primitive type, e.g., strings, numbers, etc.. During this traversal, the parser concatenates keys and values into single lines of text to create a comprehensive textual representation of each leaf node. For the refinement, we implemented token chunking ,remove example , and relevant field . Thetoken chunking splits each intermediate chunk into a list of fixed-size chunks of stokens respecting an overlap of ltokens with the previous node. The remove example removes the requestBody and recursively all examples fields for each endpoint as these are typically lengthy but contribute little information. The relevant field extracts representative fields, i.e., service title, service description, endpoint verb, endpoint path, and endpoint description, which contribute much information but few tokens. For',\n",
       "  'chunk_id': 22},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'endpoint path, and endpoint description, which contribute much information but few tokens. For the LLM-based processing strategies, we apply the endpoint split and a summary (similar to [ 21]) andqueryapproach for refinement. In the summary approach, we prompt an LLM to generate a summary for each OpenAPI endpoint. For thequeryapproach, we instruct the LLM to generate a possible query matching the OpenAPI endpoint, as this might be closer to a possible input query than the summary. For both approaches, we only consider the LLM output for the embedding creation. The chunk content remains the original OpenAPI endpoint information. The no split and JSON split splitting methods can only be used with token chunking since all other refinement strategies rely on exactly one endpoint as an intermediate chunk. To evaluate the OpenAPI RAG and the Discovery Agent, we implement it as a fully operational prototype. Then, we employ the RestBench [ 35] benchmark to validate it in a real-world',\n",
       "  'chunk_id': 23},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'operational prototype. Then, we employ the RestBench [ 35] benchmark to validate it in a real-world setting. We implement the OpenAPI RAG and Discovery Agent approaches as open- source prototypes based on the LlamaIndex library.3For the prototypes, we rely solely on OpenAPIs as the state-of-practice for service descriptions. All sources and results are available online.4 4https://doi.org/10.18419/darus-4605Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 9 For the OpenAPI RAG, we focus on the components presented in Figure 1. At the first start, the system loads the OpenAPIs and applies a chunking strategy to create chunks and their embeddings for their later retrieval. The chunks contain thereby the information from the OpenAPIs, e.g., a whole endpoint or a part of it. A chunk embedding does not necessarily have to match the chunks content; for example, the content can be the endpoint, and the embedding is created using a natural language summary of the endpoint. Thus,',\n",
       "  'chunk_id': 24},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'the endpoint, and the embedding is created using a natural language summary of the endpoint. Thus, the matching is performed based on the embedding, and the result returned is the chunks content, which can include additional information not required for the matching process. As the service database, we use FAISS, which allows the storage and the similarity search of chunks [ 9]. We use a so-called QueryEngine from LlamaIndex for the chunk retriever, which allows us to query a chunk database based on textual We realize the Discovery Agent from Figure 2 using a LlamaIndex Ope- nAIAgent, which implements the LLM agent pattern for OpenAIs LLMs. An OpenAIAgent takes a list of tools, i.e., Python functions with a name and a description as parameters, and interacts with these using the OpenAI API. We implement two strategies for the tools. In the first strategy ( Query), we use the OpenAPI RAG as input for a LlamaIndex QueryEngineTool, which allows the agent to interact with the RAG on',\n",
       "  'chunk_id': 25},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'RAG as input for a LlamaIndex QueryEngineTool, which allows the agent to interact with the RAG on demand. This has the advantage of being a simple, straightforward implementation but may increase the token count as the results of the RAG are fed into the chat history, which is transferred to the LLM for the reasoning on this data. The second strategy ( Summary ) uses a RAG with chunks of the endpoints verb, path, and summary as contents and for their embeddings. We create the summary by instructing an LLM to create it based on the endpoint information, i.e., as in the summary chunking strategy. This should reduce the token count, as the chunks are much smaller, as not all endpoint details are returned and processed. To account for the same function as the first approach with the OpenAPI RAG and provide all information, we introduce a second tool, which takes the endpoint verb and path as input parameters and returns the whole endpoint information. The complete data is only inserted',\n",
       "  'chunk_id': 26},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'as input parameters and returns the whole endpoint information. The complete data is only inserted into the history for indispensable endpoints. To enable measuring the retrieved endpoints, we attach the endpoint infor- mation, i.e., verb and path, to each chunk as metadata. For the endpoint split splitting strategies, we take the information from the endpoint. For the other strategies, we first attach a list of all endpoints to the nodes before splitting and then filter on the endpoint paths in the final chunks after splitting. So, for each chunk, we know to which endpoint or endpoints it relates to. 4.2 Dataset and Metrics We evaluate our approach using the RestBench benchmark, covering the Spotify and TMDB OpenAPI specifications [ 35]. With 40 endpoints for Spotify and 54 for TMDB, this benchmark is much more complex than usual Service-Oriented Com- puting (SOC) case studies containing usually just three to seven endpoints [28].10 Pesl et al. Nevertheless, a holistic benchmark',\n",
       "  'chunk_id': 27},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'usually just three to seven endpoints [28].10 Pesl et al. Nevertheless, a holistic benchmark covering various domains is still missing (see RestBench contains 57 queries for Spotify and 100 for TMDB. For each of these queries, a solution set of endpoints is given, i.e., one to four endpoints that must be called to fulfill the query. For example, one query is Who directed the top-1 rated movie? The solution contains the endpoints GET /movie/top_rated As embedding models, we employ OpenAIs text-embedding-3-large [24] as one of the currently leading proprietary models. As open-source models, we utilize BAAI/ bge-small-en-v1.5 [39], which is relatively small while still producing reasonable results, allowing the model to be executed on commonly available hardware like laptops, and Nvidias NV-Embed-v1 [14] as one of the leading open-source models. For the LLM, we use OpenAIs gpt-4o-2024-05-13 . We evaluate the quality of the retrieved information in terms of accuracy and the token count of',\n",
       "  'chunk_id': 28},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': '. We evaluate the quality of the retrieved information in terms of accuracy and the token count of the returned result. We measure the accuracy using standard information retrieval metrics, namely recall =TP To correctly solve the query q, a service discovery approachs recall should ideally be maximal. However, this could result in retrieving several irrelevant services, turning into a drop in precision. The F1 score represents a balance between recall and precision. Table 2 shows the RestBench results for the OpenAPI RAG on the Spotify API. In recall, the JSON split method performs exceptionally well, especially with a high chunk size s, as this approach densely packs the information from the JSON into the chunks by removing all formatting. For precision and F1, the endpoint splitting approaches perform best because each chunk corresponds to precisely one endpoint. Differences between the models are minor, except that thebge-small-en-v1.5 performs worse for the no split approach. We',\n",
       "  'chunk_id': 29},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'the models are minor, except that thebge-small-en-v1.5 performs worse for the no split approach. We also tested s= 2048and s= 4096, which are not reported here for space reasons. We show s= 1024because it is the default chunk size of LlamaIndex and s= 8191 because it is the maximum input token count for the OpenAI model. It is worth mentioning that with an increasing chunk size, the token size of the returned result also increases. Generally, a higher recall seems to correlate with a higher token count, e.g., no splitting with s= 1024, and l= 0has 4717tokens output on average. In contrast, the JSON split has 10056with the same parameters, but this needs further analysis. Due to length limitations, we cannot show the token count comparison and other values for top khere. We also tested top k= 5and topk= 20. Recall increases with a higher top k, but precision drops. Additional data is available in the complementary material.4 Table 3 presents the OpenAPI RAG RestBench results for the',\n",
       "  'chunk_id': 30},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'in the complementary material.4 Table 3 presents the OpenAPI RAG RestBench results for the TMDB API. The TMDB OpenAPI is more complex in length and extent than the Spotify OpenAPI. In this case, the endpoint split-based approaches performs best inAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 11 Table 2. Results for the OpenAPI RAG for top k= 10with the Spotify API. The bge-small-en-v1.5 and NV-Embed-v1 , respectively. TC is for token chunking, RE for remove examples, and RF for relevant fields. Recall, precision, and F1 are in percent. The highest values per column are marked in bold. Category Splitting Refinement s lOSNOSNOSN Character No TC 1024 0674062161219261829 No TC 8191 0887191777121313 Endpoint TC 1024 0707576191920293131 Endpoint TC 8191 0737576191920293031 JSON TC 1024 08184859810171519 JSON TC 8191 09795100555101010 LLM Endpoint Query N/AN/A715758181515292324 Table 3. Results for the RAG for top k= 10with the TMDB API. Schema as in Category Splitting',\n",
       "  'chunk_id': 31},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'Table 3. Results for the RAG for top k= 10with the TMDB API. Schema as in Category Splitting Refinement s lOSNOSNOSN Character No TC 1024 0114733173821511 No TC 8191 0301715195923811 Endpoint TC 1024 0404046201518272126 Endpoint TC 8191 0664759191214291923 JSON TC 1024 0444446181216261924 JSON TC 8191 0616050886141411 LLM Endpoint Query N/AN/A566546131510202417 Endpoint Summary N/AN/A69596516131529222412 Pesl et al. Table 4. Results of the Discovery Agent experiments. We set topk= 10and use OpenAIs text-embedding-3-large as the embedding model. Spotify and TMDB are the two test sets from the RestGPT benchmark. RAG are the results for the summary chunking strategy from the Tables 2 and 3. Query is the standard LlamaIndex QueryEngineTool to retrieve data from a RAG system. The summary is our approach with a QueryEngineTool for summaries and a details-on-demand fetcher. Accuracy values are in percent. #Token is the number of tokens per query averaged over all queries in the test set. The',\n",
       "  'chunk_id': 32},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'in percent. #Token is the number of tokens per query averaged over all queries in the test set. The best value per row is marked in bold. Accuracy Recall71.92 63.70 66.4469.33 43.11 46.67 Precision 18.4267.3970.29 15.60 45.9750.97 #Token Prompt4233.658606.873125.21 41001.46 65699.75 4544.57 Completion 0.00262.30 256.26 0.00242.65 231.73 Total4233.658869.183411.47 41001.46 65942.40 4776.30 precision and F1. The no split approaches achieve high values in precision due to their low value of true positives. Overall, the endpoint split tends to outperform no splitting. The JSON splitting benefits Spotify as the endpoints are already very dense, i.e., the endpoints do not contain examples, and schemas are only referenced. Therefore, many endpoints can be condensed into one chunk. This approach performs much worse for the lengthier endpoints in the TMDB API. The summary refinement outperforms the query refinement, leading to the Discovery Agent. We present the RestBench results of the',\n",
       "  'chunk_id': 33},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'the query refinement, leading to the Discovery Agent. We present the RestBench results of the Discovery Agent in Table 4. For accuracy, we measure recall, precision, and F1 equally to the OpenAPI RAG experiments. For the token count, we measure the actual tokens sent from the agent to the LLM from the agent as prompt, the tokens received as completion , and their sum astotal. For the RAG approach, we accumulate the tokens of the retrieved The results show that both agent approaches improve precision and F1 but reduce recall. The Query approach increases the tokens in the prompt. Contrarily, in the total token count. The completion token count is by a magnitude smaller than the prompt token count for the agent approaches, which is relevant as completion tokens are usually more expensive than prompt tokens. No LLM is invoked in the RAG approach, so the completion tokens are zero.Analyzing OpenAPI Chunking for Retrieval-Augmented Generation 13 We demonstrated the effectiveness of the',\n",
       "  'chunk_id': 34},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'OpenAPI Chunking for Retrieval-Augmented Generation 13 We demonstrated the effectiveness of the OpenAPI RAG and the Discovery Agent using our implementation. They are able to retrieve large portions of relevant data while not revealing all relevant information in all cases. To address RQ1, we implemented the OpenAPI RAG to apply RAG for endpoint discovery with seven chunking strategies and numerous parameter combinations. We showed its effectiveness using the RestBench benchmark. Overall, the ability to adequately reduce the token size to fit into the LLM context size while maintaining most of the relevant information is exhibited by the prototype. Regarding the chunking strategies, endpoint split-based chunking strategies achieve favorable accuracies. Limitations are primarily that the RAG results may not contain all relevant information, and the precision is low due to the retrieval of exactly kchunks. Additional research is needed to improve the retrieval performance further and',\n",
       "  'chunk_id': 35},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'of exactly kchunks. Additional research is needed to improve the retrieval performance further and prove the results in a generalized setting across For RQ2, we introduced the Discovery Agent, which transfers the LLM agent pattern to endpoint discovery. Especially using Summary approach, the Discovery Agent showed strong improvement over the OpenAPI RAG in terms of precision, F1, and token count. Further research is needed to improve the decline in recall due to the processing through the LLM. While we rely on the research benchmark RestBench for our results, which covers two extensive OpenAPIs, queries, and ground truth, it is still limited to these two services. OpenAPI RAG systems in practice may operate on much larger datasets. For the data processing, we rely on standard RAG implementations like LlamaIndex, which are already designed to operate on large amounts of data. The performance evaluation, especially in larger real-world scenarios, remains open for future research. The',\n",
       "  'chunk_id': 36},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'evaluation, especially in larger real-world scenarios, remains open for future research. The applicability of the OpenAPI RAG depends on the availability of service documentation. We try to mitigate this issue by relying on widely adopted OpenAPI specifications, but this might not be valid for all domains. A solution to consider is automatically generating service documentation using an LLM. Another factor influencing the discovery is the quality of the OpenAPIs. The discovery may fail if no descriptions, meaningful naming, or erroneous information is given. This is not an issue of the approach, as a human developer would face the same problem, but it highlights the importance of high-quality documentation. strategies, e.g., CRAFT [ 42], could be added to the OpenAPI RAG. These could improve retrieval performance by combining multiple strategies or by creating a custom chunking strategy for a specific kind of service documentation. Another advancement could also be creating a custom',\n",
       "  'chunk_id': 37},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'for a specific kind of service documentation. Another advancement could also be creating a custom embedding model tailored explicitly to service descriptions and service description chunks. This model may also be trained for one specific chunking strategy or intended use case. Additionally, the RAG output may be trimmed to boost precision. This could be done by, e.g., employing a similarity threshold.14 Pesl et al. The presented Discovery Agent could be further improved to handle whole service compositions. In this case, the agent would be extended by an additional component for the service composition, and the user would only submit their ser- vice composition task to the agent to retrieve the executable service composition Besides capabilities of the RAG system, resource consumption is a major issue in LLM-based systems. The OpenAPI RAG only uses embedding models. These are much more efficient than LLMs, resulting in costs in fractions of a cent per query. In contrast, the Discovery',\n",
       "  'chunk_id': 38},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'than LLMs, resulting in costs in fractions of a cent per query. In contrast, the Discovery Agent requires significantly more resources, i.e., running RestBench in our experiments resulted in about $50 of API fees. Further work is needed to reduce this resource footprint. different ISs. With the application of automated LLM-based service composition approaches, the LLM input context limitations have become prominent, as the the preselection of relevant information. To address this issue, we proposed an OpenAPI RAG, which facilitates semantic search based on state-of-the-practice OpenAPIs and reduces the input token size. Further, we show an advanced integration through a Discovery Agent, which can retrieve service details on demand to reduce the input token count further. Our evaluation based on the RestBench benchmark shows that our approach is viable and performing. Limitations are especially in the restriction of RestBench to two services of the entertainment domain. We will address',\n",
       "  'chunk_id': 39},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'in the restriction of RestBench to two services of the entertainment domain. We will address this in an extended version of this work. Further improvements are in optimizing the implementation and extending the agent for additional tasks, e.g., whole service compositions. We leave this for Acknowledgments. This work was partially funded by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) project Software-Defined Car (SofD- Car) (19S21002). The authors acknowledge support by the state of Baden-W rttemberg Disclosure of Interests. The authors Pesl and Aiello are listed as inventors of a patent [27], which covers automated service composition using LLMs for the automotive 1.Achiam, J., et al.: GPT-4 technical report (2023), https://arxiv.org/abs/2303. 2.AI@Meta: Llama 3 model card (2024), https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.mdAnalyzing OpenAPI Chunking for Retrieval-Augmented Generation 15 3.Baresi, L., Miraz, M.: A distributed approach',\n",
       "  'chunk_id': 40},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'Chunking for Retrieval-Augmented Generation 15 3.Baresi, L., Miraz, M.: A distributed approach for the federation of heterogeneous registries. In: ICSOC 2006. pp. 240 251. Springer (2006). https://doi.org/10. 4.Bohn, H., Golatowski, F., Timmermann, D.: Dynamic device and service discovery extensions for WS-BPEL. In: ICSSSM 2008. pp. 1 6. IEEE (2008). https://doi. 5.Cobbe, K., et al.: Training verifiers to solve math word problems (2021), https: 6.Cuconasu, F., et al.: The power of noise: Redefining retrieval for RAG systems. In: SIGIR. vol. 47, pp. 719 729 (2024). https://doi.org/10.1145/3626772.3657834 7.Curbera, F., et al.: Unraveling the web services web: an introduction to SOAP, WSDL, and UDDI. IEEE Internet Computing 6(2), 86 93 (2002). https://doi. 8.Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: NAACL-HLT 2019. pp. 9. Douze, M., et al.: The Faiss library (2024),',\n",
       "  'chunk_id': 41},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'for language understanding. In: NAACL-HLT 2019. pp. 9. Douze, M., et al.: The Faiss library (2024), https://arxiv.org/abs/2401.08281 10.Fan, A., et al.: Large language models for software engineering: Survey and open 11.Fikouras, I., Freiter, E.: Service discovery and orchestration for distributed service repositories. In: ICSOC 2003. pp. 59 74. Springer (2003). https://doi.org/10. 12. Gao, L., et al.: Pal: Program-aided language models. In: International Conference on Machine Learning. pp. 10764 10799. PMLR (2023) 13.Kim, M., Stennett, T., Shah, D., Sinha, S., Orso, A.: Leveraging large language models to improve REST API testing. In: ICSE. vol. 44, pp. 37 41 (2024). https: 14.Lee, C., et al.: Nv-embed: Improved techniques for training llms as generalist embedding models (2024), https://arxiv.org/abs/2405.17428 15.Lemos, A.L., Daniel, F., Benatallah, B.: Web service composition: A survey of techniques and tools. ACM Comput. Surv. 48(3) (dec 2015). https://doi.org/10. 16.Lewis, P., et',\n",
       "  'chunk_id': 42},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'of techniques and tools. ACM Comput. Surv. 48(3) (dec 2015). https://doi.org/10. 16.Lewis, P., et al.: Retrieval-augmented generation for knowledge-intensive NLP tasks. In: NeurIPS. vol. 33, pp. 9459 9474. Curran Associates (2020) 17.Li, M., et al.: API-Bank: A comprehensive benchmark for tool-augmented LLMs. In: EMNLP. Association for Computational Linguistics (2023). https://doi.org/ 18.Liang, Y., et al.: Taskmatrix.AI: Completing tasks by connecting foundation models with millions of APIs. Intelligent Computing 3, 0063 (2024). https://doi.org/ 19.Mialon, G., et al.: Augmented language models: a survey (2023), https://arxiv. 20.Nakano, R., et al.: WebGPT: Browser-assisted question-answering with human 21.Nogueira, R., Yang, W., Lin, J., Cho, K.: Document expansion by query prediction 22.OpenAI: Function calling and other API updates (Jun 2024), https://openai.com/ index/function-calling-and-other-api-updates/ , last accessed 2024-07-18 23.OpenAI: GPT-4 Turbo in the OpenAI API.',\n",
       "  'chunk_id': 43},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': ', last accessed 2024-07-18 23.OpenAI: GPT-4 Turbo in the OpenAI API. https://help.openai.com/en/ articles/8555510-gpt-4-turbo-in-the-openai-api (2024), last accessed 2024- 11-1916 Pesl et al. 24.OpenAI: New embedding models and API updates (Jan 2024), https://openai. com/blog/new-embedding-models-and-api-updates , last accessed 2024-07-18 25.Parisi, A., Zhao, Y., Fiedel, N.: Talm: Tool augmented language models (2022), 26.Patil, S.G., Zhang, T., Wang, X., Gonzalez, J.E.: Gorilla: Large language model connected with massive APIs (2023), https://arxiv.org/abs/2305.15334 27.Pesl, R.D., Klein, K., Aiello, M.: Verfahren zur Nutzung von unbekannten neuen Systemdiensten in einer Fahrzeuganwendung (2024), Patent DE102024108126A1 28.Pesl, R.D., St tzner, M., Georgievski, I., Aiello, M.: Uncovering LLMs for service- composition: Challenges and opportunities. In: ICSOC 2023 WS. Springer (2024). 29.Pesl, R.D., et al.: Compositio Prompto: An architecture to employ large language models in',\n",
       "  'chunk_id': 44},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': '29.Pesl, R.D., et al.: Compositio Prompto: An architecture to employ large language models in automated service computing. In: ICSOC 2024. Springer (2024) 30.Radford, A., Wu, J., Amodei, D., Amodei, D., Clark, J., Brundage, M., Sutskever, I.: Better language models and their implications. OpenAI blog 1(2) (2019), https: //openai.com/index/better-language-models/ , last accessed 2024-11-28 31.Radford, A., et al.: Improving language understanding by generative pre-training 32.Santana, J.M.S., Petrova, M., Mahonen, P.: UPnP service discovery for heteroge- neous networks. In: IEEE PIMRC. vol. 17, pp. 1 5. IEEE (2006) 33.Shi, Z., et al.: Chain of tools: Large language model is an automatic multi-tool 34.Soki, A.T., Siqueira, F.: Discovery of RESTful Web services based on the OpenAPI 3.0 standard with semantic annotations. In: AINA. pp. 22 34. Springer (2024). 35.Song, Y., et al.: RestGPT: Connecting large language models with real-world applications via restful APIs (2023),',\n",
       "  'chunk_id': 45},\n",
       " {'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'content': 'RestGPT: Connecting large language models with real-world applications via restful APIs (2023), https://arxiv.org/abs/2306.06624 36.Th nes, J.: Microservices. IEEE software 32(1), 116 116 (2015). https://doi.org/ 37. Vaswani, A., et al.: Attention is all you need. NeurIPS 30(2017) 38.Wei, J., et al.: Chain-of-thought prompting elicits reasoning in large language models. NeurIPS 35, 24824 24837 (2022) 39.Xiao, S., Liu, Z., Zhang, P., Muennighoff, N.: C-pack: Packaged resources to advance general chinese embedding (2023), https://arxiv.org/abs/2309.07597 40.Yao, S., et al.: React: Synergizing reasoning and acting in language models (2023), 41.Yao, S., et al.: Tree of thoughts: Deliberate problem solving with large language 42.Yuan, L., et al.: CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets (2024), https://arxiv.org/abs/2309.17428',\n",
       "  'chunk_id': 46},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet? Abstract Hallucinations, the tendency to produce irrelevan- t/incorrect responses, are prevalent concerns in generative AI- based tools like ChatGPT. Although hallucinations in ChatGPT are studied for textual responses, it is unknown how ChatGPT hallucinates for technical texts that contain both textual and technical terms. We surveyed 47 software engineers and produced a benchmark of 412 Q&A pairs from the bug reports of two OSS projects. We find that a RAG-based ChatGPT (i.e., ChatGPT tuned with the benchmark issue reports) is 36.4% correct when producing answers to the questions, due to two reasons 1) limita- tions to understand complex technical contents in code snippets like stack traces, and 2) limitations to integrate contexts denoted in the technical terms and texts. We present CHIME ( ChatGPT Inaccuracy Mitigation Engine) whose underlying principle is that if we can preprocess the technical',\n",
       "  'chunk_id': 0},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Inaccuracy Mitigation Engine) whose underlying principle is that if we can preprocess the technical reports better and guide the query validation process in ChatGPT, we can address the observed limitations. CHIME uses context-free grammar (CFG) to parse stack traces in technical reports. CHIME then verifies and fixes ChatGPT responses by applying metamorphic testing and query transformation. In our benchmark, CHIME shows 30.3% more correction over ChatGPT responses. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT Index Terms ChatGPT, Hallucination, Software Issue Reports The reliability of LLMs is often questioned due to their tendency to produce nonsensical or incorrect outputs, a phe- nomenon commonly referred to as hallucination [1], [2], [3]. Like any LLM, ChatGPT can also suffer from hallucination issues like inconsistency in responses [4], [5] or factual inaccuracies. These problems can arise even when',\n",
       "  'chunk_id': 1},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'inconsistency in responses [4], [5] or factual inaccuracies. These problems can arise even when the model is provided with the context as a document/paragraph. While progress is made to assess hallucinations in textual data [6], [7], we are not aware of how hallucinations can be detected and mitigated for software technical reports that contain both textual and technical terms (e.g., crash dumps, code snippets, This paper studies the detection and mitigation of ChatGPT inaccuracies in technical reports. We pick software bug reports for our study, because bug reports often contain a blend of descriptive text, technical terminology, code references, and snippets of crash/system dumps [8]. These documents are crucial for tracking and resolving software issues but can be overwhelming due to their volume and complexity [9]. An AIchatbot, trained to understand these reports, may streamline the process by extracting information. But for that, first we need to ensure that the responses from',\n",
       "  'chunk_id': 2},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'process by extracting information. But for that, first we need to ensure that the responses from the chatbot are correct. In the first phase of our study , we conducted a survey of 47 software engineers to understand the types of questions they ask while exploring bug reports and for which they wish for an automated Q&A tool like a chatbot. We found that developers ask diverse questions during bug exploration, which we could group into five types: 1) issue analytics, 2) issue trends, 3) issue summary, 4) issue labeling, and 5) issue backlogs. Based on the survey findings, we produced a benchmark of 412 Q&A pairs by consulting our industry partner (with whom we conducted regular bi-weekly sessions) and the literature. The Q&A pairs are collected by analyzing the issue reports of two popular open-source software (OSS). In the second phase of our study , we tuned ChatGPT with issue reports from the two studied OSS based on the Retrieval Augmented Generation (RAG) techniques [10], [11],',\n",
       "  'chunk_id': 3},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'from the two studied OSS based on the Retrieval Augmented Generation (RAG) techniques [10], [11], [12], [13]. We then evaluated the correctness of ChatGPT re- sponses against our benchmark. Each question was asked and compared automatically to its expected answer. Correctness was assessed as the ratio of questions whose answers were found as correct. We found that our RAG-based ChatGPT was correct in only 36.4% cases. For the rest of the questions, it hallucinated by producing incorrect or irrelevant answers. We manually examined each hallucination case and identified two limitations in ChatGPT to process technical documents like bug reports: 1) limitations to understand complex technical contents in code snippets like stack traces (e.g., when a partial code snippets/crash dump is provided and the question is about determining the cause of the crash by assessing both the crash dump and the textual contents), and 2) limitations to integrate contexts denoted in the technical terms and',\n",
       "  'chunk_id': 4},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'the textual contents), and 2) limitations to integrate contexts denoted in the technical terms and texts (e.g., when ChatGPT was required to assess the relationships among multiple metadata and the technical terms). In the third phase of our study , we designed CHIME (ChatGPT Inaccuracy Mitigation Engine) to address the above two limitations. The underlying principle in CHIME is that (1) if we can preprocess the technical reports better and store information relevant to an issue report as a combination of metadata and actual contents and (2) then guide the query validation process in ChatGPT with guided iterative prompting approaches, we can address the observed limitations. The 11 Nov 2024TABLE I: Demography of Survey Participants Current Role 0-3 4-5 6-10 11-15 16-20 Total Developer 13 20 4 2 - 39 QA Engineer 1 1 - - - 2 Project Manager - 1 - - - 1 Other 2 - 2 - 1 5 Total 16 22 6 2 1 47 (a)Resolution Triaging Reporting Management89.36% Fig. 1: (a) Frequency & (b) Reasons of Bug',\n",
       "  'chunk_id': 5},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '2 1 47 (a)Resolution Triaging Reporting Management89.36% Fig. 1: (a) Frequency & (b) Reasons of Bug Report Exploration usage of metadata is found to improve LLM search capabilities [14]. For us, such metadata could be generated by organizing the mix of textual and technical contents into a structured form. A challenge was how to separate the textual and technical contents and process the code terms within a crash dump and then organize those within a structure. We introduce a novel context-free grammar (CFG) in CHIME to efficiently parse stack traces in technical reports. As for the second principle (i.e., guided prompting for verification), we extended recent similar work on textual content. CHIME verifies and fixes ChatGPT responses by using query transformation [15] and by extending CoVe [16] with metamorphic testing (MT) [17]. CoVe is a zero-shot iterative prompting-based query verifi- cation technique. We evaluated CoVes response by further mutating the question using MT, because',\n",
       "  'chunk_id': 6},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'cation technique. We evaluated CoVes response by further mutating the question using MT, because CoVe may discard correct responses or promote incorrect responses. We evaluated CHIME using our benchmark. CHIME shows on average 30.3% improvement over ChatGPT responses by offering more correct answers. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT without CHIME. Our replication package (https://bit.ly/4fyaMIP) contains all the data and code developed in the study. II. R ELATED WORK A. Hallucinations in Large Language Models Extensive studies in the literature identified the causes of hallucinations as sub optimal training, inference [18], [19], [20], [21], [22], and insufficient/low-quality data [23], [24], [25]. Techniques such as bidirectional auto-regressive models [26] and attention-sharpening mechanisms [27] have been developed to address training-related hallucinations. Inference issues, primarily due',\n",
       "  'chunk_id': 7},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'have been developed to address training-related hallucinations. Inference issues, primarily due to decoding strategies, often result in in- accurate outputs. Strategies like factual-nucleus sampling [28]and in-context pretraining [29] are implemented to mitigate these inaccuracies. Challenges posed by flawed data sources introduce biases and inaccuracies into models, stemming from misinformation, duplication biases, and social biases in the training datasets. Mitigating data biases involves manual dataset creation [30], integrating high-quality sources such as the Pile [31], and up-sampling factual data [32]. Furthermore, knowledge editing [33], [34] and Retrieval-Augmented Gen- eration (RAG) [35], [11], [12], [13] are employed to bridge knowledge gaps, utilizing external sources for more accurate Our study utilizes RAG-based ChatGPT for technical bug report understanding. We enhance ChatGPTs knowledge base by integrating it with a database of bug reports through RAG methods. We then',\n",
       "  'chunk_id': 8},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'knowledge base by integrating it with a database of bug reports through RAG methods. We then develop CHIME, which refines both the preprocessing of input data and the validation of RAG-based B. LLMs for Software Engineering In recent years, the application of LLMs has been widely utilized in Software Engineering (SE) tasks, ranging from code analysis to bug detection [36], [37], [38], [39], [40], [41], [42], [43], [44]. Encoder-only models like BERT [45] and its derivatives, including CodeBERT [46] and GraphCodeBERT [47], excel in processing code. While encoder-decoder models, such as T5 and PLBART excel in understanding semantics of code for tasks like code summarization [48], [49]. Decoder- only models like the GPT series and specialized versions like CodeGPT and Codex generate direct responses from prompts. However, the specific challenge of mitigating inaccuracies in software technical reports remains unexplored. Addressing this gap, we introduce CHIME to reduce inaccuracies in',\n",
       "  'chunk_id': 9},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'reports remains unexplored. Addressing this gap, we introduce CHIME to reduce inaccuracies in ChatGPT- generated responses during bug report exploration. III. AI C HATBOT NEEDS FOR BUGREPORT EXPLORATION To evaluate a chatbot on software technical documents, we needed a benchmark, which at the time of our study was not available. We thus adopted a systematic approach to create such a benchmark. First, we conducted a survey of software developers to produce a catalog of questions that they ask during bug reports. Second, we used the catalog to produce our benchmark (see Section IV). This section discusses the survey, which answers the following research question (RQ): RQ1. What types of questions would software practitioners like to ask a chatbot during bug report exploration? We employed the snowball sampling approach [50] to recruit participants, resulting in 47 software practitioners. The majority of respondents (83%) held developer roles. The largest proportion of participants (55%)',\n",
       "  'chunk_id': 10},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'majority of respondents (83%) held developer roles. The largest proportion of participants (55%) reported having 4- 5 years of experience in the field. Table I illustrates the distribution of participants across their roles in the software industry and years of experience. We ensured that all se- lected participants regularly use Issue Tracking Systems (ITS), such as Jira, GitHub, or in-house systems for several keyTABLE III: Key Questions (KQ) of the Survey. 1 How would you like to utilize a chatbot during bug 2 Would you want to use the chatbot for T#? C 3 Rate the usefulness of T# for your work C 4 Rate the potential usefulness of the following bench- mark questions of T#C TABLE II: Identified Task Types for Bug Report Exploration T# Task Title Task Description T1 Issue Analytics Explores complex details within/across bug reports, including technical jargon, error codes, and contextual nuances, etc. T2 Issue Trends Patterns and trends in bug occurrences. T3 Issue Summaries',\n",
       "  'chunk_id': 11},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'contextual nuances, etc. T2 Issue Trends Patterns and trends in bug occurrences. T3 Issue Summaries Summaries of key topics discussed in an issue or across multiple issues. T4 Issue Labeling Inquires labels for bug reports to help orga- nize and categorize them effectively. T5 Issue Backlog Explores whether the issue remains open for purposes. Around 89.36% of respondents used ITS for issue resolution addressing and for solving bugs; followed by issue triaging which involves prioritizing and assigning issues. Project Management and documenting issues for records or stakeholder communication were also noted. A vast majority of respondents (70.21%) reported engaging with issue reports daily. Figure 1a presents the distribution of issue report inter- action frequency and Figure 1b illustrates the survey responses for the primary reason for issue report exploration. Before the main survey, we consulted two software profes- sionals from a reputed software company to get insights on the',\n",
       "  'chunk_id': 12},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'we consulted two software profes- sionals from a reputed software company to get insights on the potential tasks that an AI chatbot can support during bug report understanding. Each had 16 and 35 years of experience respectively. We finally settled on five distinct tasks as shown in Table II). For our survey, we formulated questions to address During the survey, participants were prompted to envision an AI chatbot with comprehensive access to bug reports. Subsequently, they were asked whether they would like to utilize the chatbot for the specified tasks (T#) and to rate the perceived usefulness of each task for their work. We condensed the key survey questions into Table III for clarity. All survey questions can be found in our online appendix. C. Understanding Preferences of Software Practitioners on Identified Task Types (RQ1) In Table III, with Key Question 1 (KQ1), we tried to identify the desired chatbot roles during bug report explorations. With KQ2-4, we examined how',\n",
       "  'chunk_id': 13},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'to identify the desired chatbot roles during bug report explorations. With KQ2-4, we examined how participants perceived the useful- ness of each task type. The questions are summarized in TableIV. For each question, we show quantitative evidence from the survey about what the participants thought of the usefulness 1) KQ1. Desired Chatbot Roles: We asked KQ1 as an open- ended question to check whether the participants considered the same five task types that we identified in Table II. As such, we did not show them KQ2-4 until they answered KQ1. Participants desired a tool whose capabilities closely aligned with our predefined tasks (T1 T5). They emphasized the chat- bots potential to analyze issues (T1), such as extracting pivotal details like exceptions or log entries from similar bug reports. According to R43: Queries for potential duplicate issues could be really helpful. The capability to analyze trends (T2) within reported issues to uncover recurring problems was seen as crucial.',\n",
       "  'chunk_id': 14},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'to analyze trends (T2) within reported issues to uncover recurring problems was seen as crucial. Participants also noted the importance of the chatbots ability to summarize (T3) and categorize (T4) issues. The survey responses indicated a significant interest in features that would allow users to query backlogged items (T5), as R43 stated: Queries for issues with very little recent activity and no clear resolution would be super helpful. Apart from these tasks, participants also expressed desires for additional capabilities such as sorting issues based on priority, severity, or difficulty and forecasting resolution times using historical data. We leave support for those tasks as our future work. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 2: (a) Interest for T1 (b) Usefulness Perception of T1 2) KQ2-4. Issue Analytics (T1): The survey results reveal a strong preference for the chatbots analytical capabilities, particularly in the context of',\n",
       "  'chunk_id': 15},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'reveal a strong preference for the chatbots analytical capabilities, particularly in the context of analyzing multiple issues, with more than 80% of participants expressing interest in utilizing the chatbot for detecting similar or duplicate issues and finding it useful; while 72.34% participants value the chatbots utility in analyzing individual issues as illustrated in Figure 2. In Table IV, we show 10 questions under T1 that each par- ticipant assessed. On average, 73.62% of participants marked these useful. The capability to identify and summarize stack traces (Q1.1) within the single issue analysis domain was highly valued, evidenced by a utility score of 78.72% and remark from respondent R11, Summary of stack trace is a good idea . In comparison, the importance of determining the environment linked to an exception (Q1.2) was rated lower, at 57.45%. For the analysis of multiple issues, the ability to find similar issues (Q1.3) was highly valued at 82.98%, as quoted by R09, They',\n",
       "  'chunk_id': 16},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'the ability to find similar issues (Q1.3) was highly valued at 82.98%, as quoted by R09, They are all extremely time-consuming when done manually. A chatbot will definitely help with this. 0 20% 40% 60% 0No Maybe Yes 0 20% 40% 60% 80% 0Not Useful Neutral Useful Fig. 3: (a) Interest for T2 (b) Usefulness Perception of T2TABLE IV: Perceived Usefulness of Benchmark Questions Presented in the Survey. Not Useful Neutral Useful Q# Question Perceived Usefulness of T# (KQ4) T1 - Issue Analytics Extracting Information from Issue Details or Find Similarities Among Issues Q1.1 Is there a stack trace provided in issue 123, and can you summarize it? Q1.2 Where in the code does the exception in issue 123 occur? Q1.3 What is the exception reported in issue 123? Q1.4 How many tests failed as reported in issue 123? Q1.5 Which environment is associated with the exception reported in issue 123? Q1.6 Are there any issues similar to issue 123? Q1.7 Find duplicate reports of the X error (or other) in A',\n",
       "  'chunk_id': 17},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'there any issues similar to issue 123? Q1.7 Find duplicate reports of the X error (or other) in A module Q1.8 Find all similar issues related to X failures] Q1.9 Identify any performance degradation issues reported on last month Q1.10 Has there been a report of a crash on a X machine running the A Test recently? T2 - Issue Trend Detect and Analyze the Trends and Patterns among Issues Q2.1 What are the frequently encountered errors in the nightly builds? Q2.2 What are the recurring themes in bug reports post the latest OS update? T3 - Issue Summary Obtain a Comprehensive Overview of Reports Selected by Different Criterion Q3.1 List all issues related to an X feature and their current status Q3.2 Generate a report detailing the distribution of issues across different project modules Q3.3 Compile a summary of unresolved issues not older than 60 days Q3.4 Can you generate a summary of all issues tagged as bug in the last 30 days? Q3.5 Create a summary of user-reported issues versus',\n",
       "  'chunk_id': 18},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'all issues tagged as bug in the last 30 days? Q3.5 Create a summary of user-reported issues versus internally identified issues T4 - Issue Label Provide Suggestions for Categorizing and Tagging Issues with Appropriate Labels Q4.1 Suggest existing labels to tag issue 123 Q4.2 Can you recommend labels for performance-related issues? T5 - Issue Backlog Analyze Unresolved Issues Reported but not yet Addressed Q5.1 Are there any long-standing issues that have been consistently postponed? Q5.2 Find issues that have not been assigned to any milestone but are older than 60 days Q5.3 List issues that have missed two or more release cycles Q5.4 Identify issues with no activity in the last 30 days. 3) KQ2-4. Issue Trend (T2): 87.2% of participants ex- pressed interest in using this feature and 85.1% found the corresponding questions on identifying and analyzing trends within bug reports useful (see Figure 3). In Table IV, we show two questions that we asked under this task. Both received a',\n",
       "  'chunk_id': 19},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '(see Figure 3). In Table IV, we show two questions that we asked under this task. Both received a favorable response. Respondent R31 noted, By focusing on recurring errors and themes, these questions provide valuable insights that can guide decision-making, resource allocation, and issue resolution efforts. When participants were asked about the utility of chatbots in identifying frequently encoun- tered errors in the nightly builds of their development envi- ronment (Q2.1), e.g., in CI/CD pipelines, 76.60% perceived this functionality as useful. 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 4: (a) Interest for T3 (b) Usefulness Perception of T3 4) KQ2-4. Issue Summary (T3): 80.6% of participants were keen on a chatbot to produce summaries of issues and 70.2% considered the asked questions useful for efficiently understanding and resolving software issues. In Table IV, regarding the chatbots ability to report on how issues are distributed across',\n",
       "  'chunk_id': 20},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'issues. In Table IV, regarding the chatbots ability to report on how issues are distributed across different project modules (Q3.1), 76.6% found this function useful. On the other hand, the feature for distinguishing between user-reported and internallyidentified issues (Q3.2) was seen as useful by 51.06% of participants, indicating a notable but more moderate interest in differentiating the sources of issues. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 5: (a) Interest for T4 (b) Usefulness Perception of T4 5) KQ2-4. Issue Labeling (T4): 70.2% of participants are interested in leveraging chatbots for the task of issue labeling (see Figure 5). However, its worth noting that this task re- ceived the lowest percentage of perceived usefulness (61.70%) compared to others. Regarding the chatbots ability to suggest appropriate labels for an issue (Q4.1) and to recommend labels for performance-related issues (Q4.2), about 68% of respondents considered',\n",
       "  'chunk_id': 21},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'and to recommend labels for performance-related issues (Q4.2), about 68% of respondents considered these features to be useful. 20% 0 20% 40% 60% 0No Maybe Yes 20% 0 20% 40% 60% 0Not Useful Neutral Useful Fig. 6: (a) Interest for T5 (b) Usefulness Perception of T5 6) KQ2-4. Issue Backlog (T5): The management of Issue Backlogs is an essential aspect of software development. R16 highlighted the challenge: Sometimes change of prioritiespushes issue out of find and stay unresolved for days. So it is good to find out long-running or inactive issues. Despite its importance, this task garnered the least interest (63.8%) among all tasks for potential chatbot utilization. Notably, T5 records the highest percentage of No responses (17.02%) regarding interest and Not Useful perceptions (19.15%). 70.2% of respondents see value in identifying long-standing, postponed issues (Q5.1) but interest slightly drops to 61.70% for detecting issues with no recent activity over the last 30 Summary of RQ1.',\n",
       "  'chunk_id': 22},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'drops to 61.70% for detecting issues with no recent activity over the last 30 Summary of RQ1. When examining software practition- ers preferences for AI chatbot capabilities in exploring bug reports, the identification of similar issues and the analysis of recurring error trends were highly favored. In contrast, capabilities related to categorizing issues and handling pending bugs were deemed less critical. IV. A B ENCHMARK OF Q&A P AIRS TO EVALUATE AI CHATBOTS FOR BUGREPORT EXPLORATION In Table IV, we showed a catalog of 23 questions that we validated with our survey participants and for which they wished for chatbot support. Each question is a template, which can be used to produce multiple similar questions.Based on the question templates in Table IV, we produced a total of 412 questions from the issue reports of two popular OSS repos, OpenJ9 and ElasticSearch. OpenJ9 was chosen due to its alignment with our industrial partner. ElasticSearch [51], [52], [53] is frequently',\n",
       "  'chunk_id': 23},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'due to its alignment with our industrial partner. ElasticSearch [51], [52], [53] is frequently referenced in academic studies. We then produced an answer to each question by assessing the two OSS repos and by consulting among the authors. Four authors (the first two and last two) engaged in many hours of discussions that spanned over six months (both in- person and over formal presentations). The last two authors are also among the maintainers of OpenJ9. Given the benchmark was created via mutual discussion, we did not compute any standard agreement analysis metrics. We created the benchmark by selecting 80 complex issues (40 from each repository). Following Deeksha et al. [9], we define an issue as complex if it is excessively long and/or it has stack traces. We picked issues within the last year of our analysis because those are likely to be explored more by developers. We sorted issues by length and selected 40 issues with stack traces (per repo). Following the standard chatbot',\n",
       "  'chunk_id': 24},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'by length and selected 40 issues with stack traces (per repo). Following the standard chatbot evaluation process, we contained three types of answers: binary [54], factual [55], [56], and summary [57]. The binary (i.e., Query Type = Y/N) queries have ques- tions with answers as Y/N. These are designed to assess the chatbots accuracy in identifying clear-cut, definitive binary decisions based on information available in bug reports; such as the presence of a particular error code or the applicability of a specific scenario. To verify a chatbot response for these queries, we simply need to check for Y/N in their responses and match those against the benchmark answer.Q1. Type: Y/N. Source: ElasticSearch Question: Is there any issue similar to issue 100071? The factual (i.e., Query Type = Factual) queries assessed the chatbots ability to extract concrete information from bug reports, such as identifying, retrieving, and presenting specific details from the dataset, such as error messages,',\n",
       "  'chunk_id': 25},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'identifying, retrieving, and presenting specific details from the dataset, such as error messages, stack traces, configuration settings, etc. Like binary queries, this method also allows for a straightforward assessment of the chatbots accuracy, and thus direct matching can be used for verification. Q2. Type: Factual. Source: ElasticSearch Question: What existing label is recommended for issues that need immediate triaging? The summary-based (i.e., Query Type = Summary) queries challenge the chatbot to engage in deeper analysis and syn- thesis of data. These queries require the chatbot to identify patterns and even to propose potential solutions based on the analysis of multiple data points. Since these queries demand a synthesis of information and provide insights or summaries, we need a similarity analysis between a response and the expected answer for verification. Q3. Type: Summary. Source: ElasticSearch Question: Summarize similarities between issues 103072 Expected Answer:',\n",
       "  'chunk_id': 26},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Source: ElasticSearch Question: Summarize similarities between issues 103072 Expected Answer: Issues 103072 and 103344 both involve test failures within the LearningToRankRescorerIT class. The root cause of these failures stems from a named object notfound exception and xcontent parse exception, resulting in ... TABLE V: Distribution of Benchmark Questions over Survey- identified Tasks from Table IV. T# Y/N Fact Summarization T1 - Issue Anlys(S)48 140 24 212 T1 - Issue Anlys(M)12 20 8 40 T2 - Issue Trend16 16 8 40 T3 - Issue Summary8 32 40 T4 - Issue Labeling12 20 8 40 T5 - Issue Backlog12 24 440 Table V shows the distribution of question types Yes/No, Fact, and Summarization across OpenJ9 and ElasticSearch, totaling 206 questions per project. OpenJ9 and ElasticSearch have a similar overall structure, with a strong emphasis on factual questions (114 for OpenJ9, 114 for ElasticSearch), followed by binary (yes/no) and summarization questions. Our online appendix contains details about',\n",
       "  'chunk_id': 27},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'followed by binary (yes/no) and summarization questions. Our online appendix contains details about each of the 412 questions and how each question maps to our catalog of 23 survey questions.V. E FFECTIVENESS OF CHATGPT ON THE BENCHMARK In this section, we answer the following research question: RQ2. How effective is a RAG-enhanced ChatGPT to answer to the benchmark questions while exploring the corre- The RAG architecture combines ChatGPT with an external knowledge retriever to provide responses to queries. This framework utilizes external database sources, primarily issue reports with structural data and metadata fetched by the GitHub API. Figure 7 illustrates the pipeline for this. It functions by first retrieving pertinent information from the database based on the input query. This step is crucial as it aligns the models focus with the most relevant data. Then, the augmented data from the retrieval step are combined with the inherent generative capabilities of ChatGPT to help',\n",
       "  'chunk_id': 28},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'from the retrieval step are combined with the inherent generative capabilities of ChatGPT to help ChatGPT provide high-quality responses. We used ChatGPT 3.5-turbo within LangChain framework [58] to implement this pipeline. We used a temperature setting of 0. A temperature value above 0 produces slightly different answers to a prompt across multiple runs, which is unnecessary when we expect consistent answers from ChatGPT. We ran it multiple times on our benchmark dataset to ensure that the answers were indeed consistent across multiple runs. Fig. 7: Pipeline of the RAG-based ChatGPT We measure the effectiveness of the above RAG-based ChatGPT on our benchmark by calculating correctness (C): C=Number of Correct Responses Total Number of Queries 100% (1) Correctness analysis involved comparing the generated re- sponses against the predefined correct answer for a query in the benchmark. We adopted the following automated approaches to measure the correctness of the responses for the',\n",
       "  'chunk_id': 29},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'We adopted the following automated approaches to measure the correctness of the responses for the three types of queries in our benchmark (i.e., Y/N, Fact, and Summaries). For evaluating yes/no responses, we used a zero-shot clas- sification approach, enabling us to automatically determine if detailed answers from the chatbot align with a simple Yes or No expectation. For evaluating factual query responses, we combine two approaches: direct comparison of extracted information (such as issue numbers) and semantic similarity assessment for non-listed facts. First, we extract and compare key factual elements. If the response and expectation directly match or share common elements, the correctness is assessed accordingly. For responses without explicit factual elements, we utilize the SentenceTransformer library, employing the all- MiniLM-L6-v2 model [59] to encode the actual and expected answers into embeddings. Subsequently, we gauge the seman- tic similarity between these embeddings by',\n",
       "  'chunk_id': 30},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'into embeddings. Subsequently, we gauge the seman- tic similarity between these embeddings by computing thecosine similarity [60] and assessing how closely the actual answer aligns with the expected fact, considering nuances in phrasing and context. For evaluating Summarization queries, we compute the semantic similarity like before between the actual summary provided by ChatGPT and the expected sum- mary. Based on empirical observations (see Section VIII-B), we used a similarity threshold of 0.7. The RAG-based pipeline achieved 36.4% accuracy in our benchmark (see Table VI). We manually assessed each of the 262 incorrect answers to determine the causes of its incorrectness. Given that ChatGPT is a black-box model, our assessment is based on the nature of the questions asked and the provided answers. We observed two limitations in L1. Limitations in Understanding Complex Technical Content L2. Contextual Understanding Challenges (57.3% cases). TABLE VI: Correctness of RAG Based ChatGPT',\n",
       "  'chunk_id': 31},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'L2. Contextual Understanding Challenges (57.3% cases). TABLE VI: Correctness of RAG Based ChatGPT T# Y/N Fact Sum Total Total 49.0% 30.7% 36.9% 36.4% T1 - Issue Anlys(S) 41.7% 32.1% 33.3% 34.4% T1 - Issue Anlys(M) 50.0% 15.0% 0.0% 22.5% T2 - Issue Trend 50.0% 12.5% 37.5% 32.5% T3 - Issue Summary 0.0% 75.0% 43.8% 50.0% T4 - Issue Labeling 58.3% 40.0% 62.5% 50.0% T5 - Issue Backlog 66.7% 25.0% 25.0% 37.5% The challenge in understanding technical contents (L1) occurred when ChatGPT faced challenges in deeply under- standing and accurately processing highly technical content, particularly when it involved intricate programming/technical concepts. For example, when asked to provide the root location of the exception encountered in issue 18151; it gives a non- useful answer; even though the issue provides a stack trace. Q4. Asking ChatGPT - OpenJ9 Question: Where in code exception of issue 18151 occur? Incorrect Answer: The exception of issue 18151 occurs in the body of the issue. Again,',\n",
       "  'chunk_id': 32},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '18151 occur? Incorrect Answer: The exception of issue 18151 occurs in the body of the issue. Again, for asking to find the line number where the error occurs, ChatGPT just gives an irrelevant answer by showing Q5. Asking ChatGPT - OpenJ9 Question: Which line in CharacterDataLatin1 class trig- gered ArrayIndexOutOfBoundsException in issue 17063? Irrelevant Answer: The line of code that is The Contextual Understanding Challenges category (i.e., L2) shows a fundamental difficulty in ChatGPT to processand respond to queries about technical issues. This category primarily involves the models struggles with: Integrating and interpreting the context in which queries are made. Some context is explicitly stated within the query or the referenced issue, such as a specific error message or stack trace. Other times, the context is implicit, requiring the model to infer based on its broader knowledge or related data points. Handling ambiguous or insufficiently detailed queries necessitates the',\n",
       "  'chunk_id': 33},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'or related data points. Handling ambiguous or insufficiently detailed queries necessitates the chatbot to fill in the gaps with assumptions or inferred knowledge. Not all contextual in- formation holds equal relevance to a given query, requiring the chatbot to prioritize the most pertinent context based on the nuances of the query. Adapting to the technical conventions of specific domains. Technical domains often have their own conventions for documentation, communication, and issue tracking. For instance, understanding that a particular label in an issue tracking system denotes the responsible team, requires domain-specific knowledge that the AI must possess. On asking to find a responsible team for an issue, ChatGPT searches on the assignee list, but the team details are on the issue labels. Due to this lack of contextual information, it fails to answer the question. Q6. Asking ChatGPT - ElasticSearch Question: Which team is responsible for issue 104160? Incorrect Answer: The team',\n",
       "  'chunk_id': 34},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '- ElasticSearch Question: Which team is responsible for issue 104160? Incorrect Answer: The team responsible for issue 104160 is not specified in the database. Even when the relevant context is identified, retrieving and applying it accurately to generate a response is challenging. This includes understanding the specific ways in which information is structured or presented within data sources and how it relates to the users query. Effectively bridging this gap is crucial for generating accurate and contextually Summary of RQ2. A RAG-based ChatGPT showed an average correctness of 36.4% on our benchmark. The pipeline encountered challenges in comprehending com- plex technical content and grasping contextual nuances, leading to inaccuracies in its responses. VI. CHIME: C HATGPT I NACCURACY MITIGATION Our observations in Section V of ChatGPT limitations contributed to the design of CHIME, as a suite of techniques to detect and fix incorrectness in ChatGPT responses. The underlying',\n",
       "  'chunk_id': 35},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'as a suite of techniques to detect and fix incorrectness in ChatGPT responses. The underlying principle of CHIME is that by offering ChatGPT with a more structured representation of bug reports and by applying a systematic approach to assess ChatGPT responses, we can address the two limitations we observed in Section V-B. A more structured representation of bug reports can be achieved if we can process the different technical and textual terms properly and store those in a structured way, e.g., in a database with metadata offering more information about thoseterms. A systematic approach to verify the responses can be achieved by applying/adapting the techniques of guided itera- tive prompting of LLM responses that are used in the literature for textual content. As such, we designed to preprocess the inputs (both the bug report and the query) and to verify the CHIME treats ChatGPT as an API, where the inputs (ques- tions) and outputs (answers) are processed for inaccuracy detection and',\n",
       "  'chunk_id': 36},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'where the inputs (ques- tions) and outputs (answers) are processed for inaccuracy detection and mitigation. We can use another LLM as an API in CHIME and apply all the techniques we developed. Doing so would simply require changing the API endpoints to point to the other LLM within the LangChain toolkit. CHIME will need to be updated significantly while using multi-modal LLMs, e.g., to process/validate modalities other than texts, etc. Fig. 8: The Architecture of CHIME In Figure 8, we show the architectural diagram of CHIME. We apply an Issue Preprocessor component to parse crash dumps and other technical terms. Issue Preprocessor employs Context-Free Grammars (CFGs) to interpret complex technical data, addressing ChatGPTs limitations in understanding com- plex technical content. To address the second limitation (i.e., Contextual understanding challenges), we introduce two more components in CHIME. First, we apply Query Preprocessor component to decipher users intents. Second, we',\n",
       "  'chunk_id': 37},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'in CHIME. First, we apply Query Preprocessor component to decipher users intents. Second, we designed the Response Validator component, which evaluates the accuracy in a response using a combination of two techniques: Chain of Verification (CoVe) [16] and Metamorphic Testing (MT) [17]. A query goes through each of these components. We discuss We developed a CFG as shown in Listing 1 which sup- ports parsing stack traces to the level of individual code elements. Stack traces can also contain auxiliary information like timestamps and memory addresses (e.g., crash dumps). We preprocessed these auxiliary information and applied our CFG to parse the code elements in a Java stack trace. Finally, our issue preprocessor stored the code blocks in the database by separating the code blocks from the textual contents. The issue preprocessor saves the following information by parsing an issue report: title, body, stack trace, and other metadata(creation date, status, etc). When ChatGPT, enhanced',\n",
       "  'chunk_id': 38},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'title, body, stack trace, and other metadata(creation date, status, etc). When ChatGPT, enhanced with Issue Preprocessor, is presented with the query regarding the type of exception in issue 18151 (Q4) or 17063 (Q5), it correctly identifies the location of the exception detailed in Listing 1: CFG Grammar for Stack Traces 1 Root ::= StackTraceElems 2 StackTraceElems ::= StackTraceElem StackTraceElems | 3 StackTraceElem ::= ExceptionElems |CodeDetails 4 ExceptionElems ::= ExceptionElem ExceptionElems | 5 ExceptionElem ::= ExceptionType |ExceptionMessage 6 CodeDetails ::= ClassElem |MethodElem |FileElem | Asking CHIME with Q4 from Section V. OpenJ9 Question: Where in the code exception of issue 18151? Correct Answer: The exception in issue 18151 can be triggered at line 98 in the JvmErgonomics.java file. Asking CHIME with Q5 from Section V. OpenJ9 Question: Which code in CharacterDataLatin1 triggered the ArrayIndexOutOfBoundsException in issue 17063? Correct Answer: Line 72 in the',\n",
       "  'chunk_id': 39},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'triggered the ArrayIndexOutOfBoundsException in issue 17063? Correct Answer: Line 72 in the CharacterDataLatin1 A user query may not provide enough details, leading ChatGPT to generate responses based on faulty assumptions or context. We leveraged the query transformation algorithm from [15] that strategically rephrases and modifies a query. Each query goes through this component for error correction as follows. We prompt ChatGPT with instructions on how to transform a query based on the question type. For example, for a Yes/No query type, we instruct ChatGPT as follows Change is/are/have there issues to check if there are any issues with the provided condition. For a summarization type question, the instruction is Summarize the contents from issue title, exceptions, body, and labels. We then provide some example transformations (in a few-shot setting) to clarify the instruction, like the following. Original Query: Are issue 18102 and 18669 similar? Transformed Query: Compare the',\n",
       "  'chunk_id': 40},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'the following. Original Query: Are issue 18102 and 18669 similar? Transformed Query: Compare the exceptions, stack traces, and descriptions of issues 18102 and 18669 to Finally, GPT answers based on the transformed query. For example, when presented with the query, Q6 from Section V, the Query Preprocessor directs the question with additional in- structions: Summarize the team responsible for issue 104160 based on the assignee and labels. This instruction results in generating a more precise SQL query to our database (we used LangChain SQL engine + RAG ChatGPT pipeline).Asking CHIME with Q6 from Section V. ElasticSearch Question: Which team is responsible for issue 104160? Correct Answer: The team responsible for issue 104160 is the ML team, with no specific assignee and labels indicating it is related to ML. The third component of CHIME is added to validate ini- tial responses from RAG-based ChatGPT. We employed two strategies sequentially, CoVe [16] and Metamorphic Testing (MT)',\n",
       "  'chunk_id': 41},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'RAG-based ChatGPT. We employed two strategies sequentially, CoVe [16] and Metamorphic Testing (MT) [17]. Drawing inspiration from investigative interroga- tion techniques, CoVes methodology is designed to test the consistency and authenticity of ChatGPTs responses through a series of challenge questions. CoVes process can be unfolded into three distinct stages 1) Upon receipt of a query, ChatGPT formulates an initial response based on its interpretation of the query within the context of available data. 2) CoVe then autonomously generates a set of follow-up questions related to the initial response. CoVe dynamically determines the number of follow-up questions. These questions aim to clarify its preceding statements. 3) The subsequent responses are ana- lyzed for consistency with the initial response. Any observed contradictions signal the need for a review and adjustment of the initial response, leading to the refinement or generation of a new, more accurate answer. Listing 2: How',\n",
       "  'chunk_id': 42},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'response, leading to the refinement or generation of a new, more accurate answer. Listing 2: How CoVe validates in CHIME Initial Question: Is issue 18102 and 18669 similar? Response: No, issue 18102 and 18669 are not similar. Follow-up Questions Generated by CoVe 1. Can issue 18102 and 18669 be considered similar based on their descriptions and reported problems? 2. Have there been any interactions or dependencies between the teams working on issue 18102 and 18669 that could 3. Are the issues 18102 and 18669 related to the same error? ChatGPT Responses to Follow-up Questions 1. Yes, based on the descriptions and reported problems, they can be considered similar as both involve 2. Yes, there have been interactions between the teams working on issue 18102 and 18669. 3. No, issues 18102 and 18669 are not related to same error. Final Response: Yes, issue 18102 and 18669 are similar. However, CoVe can discard correct responses (if not found in responses to challenge questions) or promote',\n",
       "  'chunk_id': 43},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'CoVe can discard correct responses (if not found in responses to challenge questions) or promote incorrect responses (if responses are found both in base and challenge questions). We thus refine CoVe with MT as follows. Using CoVe to verify ChatGPTs initial responses, we get an intermediate response which is then passed for consistency verification to MT. Central to MT is the concept of metamor- phic relations (MRs), which involve applying transformationsto input queries to anticipate changes in output. In CHIME, this approach is leveraged to introduce nuanced variations to the queries for testing the consistency of its responses under diverse conditions. A specific implementation of MRs in CHIME involves Sentence-Level Mutation, where equivalent questions are generated to test. This process includes 1) using predefined MRs to subtly alter the phrasing or focus of the original queries. In our implementation, MT generates three mutated questions each time in CHIME. For instance, the',\n",
       "  'chunk_id': 44},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'In our implementation, MT generates three mutated questions each time in CHIME. For instance, the original query might be What causes error X in module Y?, and its mutated equivalent could be Why does error X occur in module Y?. 2) The responses ChatGPT provides for both the original and mutated queries are compared to assess consistency with MR expectations. If an MR implies that adding specific details should refine the response, the eval- uation checks for this level of refinement. After the refinement via MR, CHIME generates the final response for the users. The user in CHIME does not see the mutated questions from CoVe or the mutated questions from MRs. Instead, the user only sees the final response from CHIME. However, CHIME can print the mutated questions to the user if needed. Listing 3: How CoVe + MT validates in CHIME Initial Question: Does Elasticsearch require the UseG1GC option to be present during issue 18151 startup stage? Initial ChatGPT Response: No, Elasticsearch',\n",
       "  'chunk_id': 45},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'option to be present during issue 18151 startup stage? Initial ChatGPT Response: No, Elasticsearch does not require the UseG1GC option to be present during its startup stage in issue 18151. Follow-up Questions Generated by CoVE : 1. Is Elasticsearch the software mentioned in the response? 2. Is the UseG1GC option not required? 3. Is issue 18151 not requiring the UseG1GC option? ChatGPT Responses to Follow-up Questions : 1. Yes, Elasticsearch is mentioned in the response. 2. No, the UseG1GC option is not required. 3. No, issue 18151 is not requiring the UseG1GC option. CoVE : No, the UseG1GC option is not required during Mutated Questions Generated by MT : 1. Is the UseG1GC option necessary for Elasticsearch to be present during its startup stage in issue 18151? 2. Must the UseG1GC option be included during Elasticsearch s startup stage for issue 18151? 3. Is it required to have the UseG1GC option present during Elasticsearchs startup stage for issue 18151? ChatGPT Responses to Mutated',\n",
       "  'chunk_id': 46},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'option present during Elasticsearchs startup stage for issue 18151? ChatGPT Responses to Mutated Questions : 1. No, the UseG1GC option is not necessary for Elasticsearch s to be present during its startup stage in issue 18151. 2. Yes, the UseG1GC option must be included during Elasticsearchs startup stage for issue 18151. 3. Yes, it is required to have the UseG1GC option present during Elasticsearchs startup stage for issue 18151. Final Response: Yes, it is required to have the UseG1GC option during Elasticsearchs startup stage .VII. E FFECTIVENESS OF CHIME We evaluate CHIME by answering three RQs: RQ3. How well can CHIME fix inaccuracies in ChatGPT RQ4. How well do individual components in CHIME per- RQ5. Would responses from CHIME be favored like those from ChatGPT when both are correct? RQ3 and RQ4 investigate whether CHIME and its components can fix inaccuracies in ChatGPT while we use our benchmark. Similar to RQ2, we use the correctness metric from Equation 1 to answer RQ3 and',\n",
       "  'chunk_id': 47},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'use our benchmark. Similar to RQ2, we use the correctness metric from Equation 1 to answer RQ3 and RQ4. RQ5 assesses the usability of CHIME when it is used by developers instead of a RAG-based ChatGPT. We conduct a user study to answer RQ5. TABLE VII: Correctness of CHIME. Column Improv shows percent improvement over RAG-based ChatGPT T# Y/N Fact Sum Total Improv Total 80.0% 61.4% 65.5% 66.7% +30.3% T1 - Issue Anlys(S) 83.3% 67.1% 66.7% 70.8% +36.3% T1 - Issue Anlys(M) 58.3% 30.0% 50.0% 42.5% +20.0% T2 - Issue Trend 68.8% 43.8% 50.0% 55.0% +22.5% T3 - Issue Summary 0.0% 87.5% 68.8% 72.5% +22.5% T4 - Issue Labeling 83.3% 60.0% 87.5% 72.5% +22.5% T5 - Issue Backlog 100.0% 58.3% 50.0% 70.0% +32.5% A. How well can CHIME fix ChatGPT inaccuracies? (RQ3) Table VII presents the assessments of the correctness of CHIME in our benchmark by offering overall results, across the three types of queries and also across the five task types in our benchmark. Overall, CHIME offers around 30.3%',\n",
       "  'chunk_id': 48},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'of queries and also across the five task types in our benchmark. Overall, CHIME offers around 30.3% improvement over the RAG-based ChatGPT pipeline from Section V. The improvement is consistent across all five task types, with issue analytics and backlog tasks benefiting the most from CHIME. CHIME showcases enhancements over ChatGPT across all tasks for both OpenJ9 and ElasticSearch: 29.6% and 31.1% improvement over ChatGPT for OpenJ9 and ElasticSearch respectively. The detailed result for each project is provided in our online appendix. We manually assessed the responses where CHIME was inaccurate and observed three main reasons as follows. Query-Directed Retrieval Failure (60.6%) : CHIME relies on its ability to query a database of stored data and generate SQL queries based on the provided questions. However, when user or verifying queries lack clarity, the query fails to provide clear instructions for formulating SQL queries. In such cases, CHIME produces incorrect or irrelevant',\n",
       "  'chunk_id': 49},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'instructions for formulating SQL queries. In such cases, CHIME produces incorrect or irrelevant responses. Logical Inference Errors (27%) : This pertains to cases where CHIME fails to accurately apply logical inference principles. It occurs when CHIME incorrectly deduces information from the data or makes faulty assumptions during reasoning. Semantic Discrepancy (10.9%) : CHIME relies on similarity scores to match user queries with existing data or responses. However, discrepancies in semantic similarity assessments can lead to incorrect matches or associations.TABLE VIII: Correctness of components of CHIME. Column Improv shows percent improvement over RAG-based Chat- Component Y/N Fact Sum Total Improv Issue Preprocessor 59.0% 43.9% 46.4% 48.1% +11.7% Query Preprocessor 57.0% 35.1% 45.2% 42.5% +6.1% Response Validator 55.0% 36.0% 47.6% 43.0% +6.6% CoVe 58.0% 28.9% 38.1% 37.9% +1.5% MT 69.0% 37.7% 53.6% 48.5% +12.1% Summary of RQ3. CHIME achieves an average correct- ness of 66.7% and',\n",
       "  'chunk_id': 50},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '69.0% 37.7% 53.6% 48.5% +12.1% Summary of RQ3. CHIME achieves an average correct- ness of 66.7% and an improvement of 30.3% over a RAG- based ChatGPT on our benchmark of bug report questions. B. How do individual components in CHIME perform? (RQ4) We ran each component of CHIME individually and deter- mined the contribution of the component within the pipeline. In Table VIII, we show the performance of each component per query type and also show whether the component offered an improvement over a RAG-based ChatGPT. We discuss how we ran each component while analyzing the results below. Issue Preprocessor. In our CHIME pipeline, we kept this component and removed the other two components (i.e., Query Processor and Response Validator). Hence, issue reports are preprocessed by this component and then stored in the database. From here, we utilize RAG-based ChatGPT for Q&A. The integration of the Issue Processor enhances the accuracy of the baseline GPT model by 11.7% on average for both',\n",
       "  'chunk_id': 51},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'of the Issue Processor enhances the accuracy of the baseline GPT model by 11.7% on average for both projects. This improvement is particularly notable in technical question comprehension and analysis tasks. Query Preprocessor. Similar to the above setup, we only kept this component and removed the other two components in our CHIME pipeline (i.e., Issue Preprocessor and Response Val- idator). On average, this process demonstrates an improvement of 6.1% over a RAG-based ChatGPT. Response Validator. We used it to validate responses from an RAG-based ChatGPT. Overall, this component contributed to a 6.6% improvement over a RAG-based ChatGPT. The bottom two rows in Table VIII further illustrate the performance of the two modules in the Response Validator, i.e., CoVe and MT. Interestingly, MT as an individual module worked even better than the Response Validator component. MT offered a 12.1% improvement over RAG-ChatGPT while CoVe offered a 1.5% improvement. However, we kept the',\n",
       "  'chunk_id': 52},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'a 12.1% improvement over RAG-ChatGPT while CoVe offered a 1.5% improvement. However, we kept the combinations of CoVe and MT in the response validator, because CoVe + MT may become more useful for other repos where responses may need a sequence of challenges via both CoVe and MT. For example, when the responses from CoVe contain references to the fact (but with incorrect summarization), MT can double- check those facts via follow-up mutated questions. As we can see from Tables VII and VIII, CHIME as an end-to-end pipeline offers 30.3% improvement over RAG-based ChatGPT, while none of the individual components in CHIME could offer more than 12% improvement over RAG-based ChatGPT. This means that the ensemble of all the components in CHIMEs pipeline helped the fixing of ones mistake by others. For instance, when a user queries List all pending issues rather than simply providing the count of pending issues, the transformed query from Query Preprocessor prompts the system to generate a',\n",
       "  'chunk_id': 53},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'of pending issues, the transformed query from Query Preprocessor prompts the system to generate a list of issue numbers, which increases its accuracy. Summary of RQ4. Each component in CHIME can offer an improvement over a RAG-based ChatGPT by correcting the inaccuracies in ChatGPT responses. The components work best when they are all put together in CHIME as an C. Would responses from CHIME favored like those from ChatGPT when both are correct? (RQ5) A comparative study was conducted to assess the practical efficacy of CHIME, involving 31 participants. The majority (93%) had 0-5 years of experience in the software industry, with 57% being software developers and 33% researchers. 1) Survey Setup: Participants were presented with two random questions from each task in our benchmark dataset. The questions include a summarization of the failure of an issue, similarities between multiple issues, recurring themes in a component, pending issues, identification of error-prone components,',\n",
       "  'chunk_id': 54},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'issues, recurring themes in a component, pending issues, identification of error-prone components, unresolved or blocker issues, guidelines for label- ing, and label suggestions. For these questions, responses from both CHIME and ChatGPT, along with links to associated bug reports, were provided for evaluation. To ensure a fair comparison, only questions with correct responses from both systems were selected. Participants were then asked to rate the correctness and perceived usefulness of the responses in addressing software bug-related queries. The survey questions are provided in our online appendix. 2) Survey Result: Participant feedback in Table IX indi- cates that CHIME was the preferred choice for the majority of tasks when the answers were correct and selected more frequently for 6 out of 10 questions. It was favored in issue analysis (T1) with a 79% participant preference. On average, for this task, 63.6% of participants found the responses to be comprehensive and covering all',\n",
       "  'chunk_id': 55},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'for this task, 63.6% of participants found the responses to be comprehensive and covering all necessary aspects, 45.1% felt that they provided additional information helpful for a better understanding of the problem, and 33.3% thought the responses were clear and easy to follow. Their preference also extended to issue summarization (T3) with a 65% preference and to issue labeling (T4), with a 63% preference. However, for the issue trending task (T2), there was a slight preference for ChatGPT. Nonetheless, for two questions, participants seemed undecided, indicating a comparable level of usefulness between CHIME and ChatGPT when the responses were correct.0.60.65 0.70.75 0.80.85 0.90.9510%20%30%40%50%60%70%80%90% ThresholdGPT Issue Pre. Query Pre. RV Fig. 9: Impact of Threshold on Similarity Analysis. Here, Issue Pre. = Issue Preprocessor, Query Pre. = Query Preprocessor, RV = Response validator TABLE IX: Selection Preference of CHIME and ChatGPT Provided Correct Responses across Tasks',\n",
       "  'chunk_id': 56},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'TABLE IX: Selection Preference of CHIME and ChatGPT Provided Correct Responses across Tasks T1 - Issue Analytics21% 79% T2 - Issue Trend58% 42% T3 - Issue Summary35% 65% T4 - Issue Labeling37% 63% T5 - Issue Backlog48% 52% Summary of RQ5. In a comparative study with 31 partici- pants, CHIME responses were preferred over a stand-alone ChatGPT for the majority of tasks when both provided correct answers. This preference was particularly evident for tasks related to issue analysis, summarization, and A. Accuracy of our CFG We evaluated the CFG-based stack trace parsing by as- sessing the 80 issue reports that we used to create our benchmark dataset. The CFG is designed to identify key elements, such as exception types, messages, and code details (e.g., class/method/file names, etc.). For each stack trace, we checked whether the parser found all key elements as expected. We used three metrics to compute accuracy: precision, recall, and F1-score. Precision is the ratio of correctly',\n",
       "  'chunk_id': 57},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'metrics to compute accuracy: precision, recall, and F1-score. Precision is the ratio of correctly identified elements to the total elements identified by the parser. Recall is the ratio of correctly identified elements to the total actual elements in the stack trace. F1-score ( F1) is the harmonicmean of precision and recall. We manually created a list of the expected elements for accurate comparison for each of the 80 issue reports. We observed an average precision of 0.99 and recall of 0.91 (F1-score = 0.93). The few errors in parsing were mainly due to the limitations in our regular expressions used in the CFG parser, and the variations in stack trace formats across issue reports. Our replication package contains the details of B. Threats to Validity Concerns regarding construct validity arise from the bench- marks design. However, we derived our benchmark queries from survey responses. The selection of issues from OpenJ9 and ElasticSearch may affect the generalizability of the',\n",
       "  'chunk_id': 58},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'The selection of issues from OpenJ9 and ElasticSearch may affect the generalizability of the find- ings across various software engineering contexts. The partic- ipant pool in surveys might not comprehensively represent the diverse perspectives in the broader software engineering com- munity. Finally, the methodology used for evaluating CHIME could affect the accuracy and objectivity of our effectiveness assessment and may introduce methodological bias . However, we have analyzed the accuracy rates of summary queries across various similarity threshold values of CHIME, ranging from 0.60 to 0.95, and chose the threshold of 0.7 as it gives the highest accuracy (see Figure 9), an optimal trade-off between capturing relevant information and minimizing false positives. We have introduced CHIME to mitigate the inaccuracy of ChatGPT response during bug report exploration. CHIME demonstrates 30.3% improvements over ChatGPT in terms of providing more correct responses for bug exploration',\n",
       "  'chunk_id': 59},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': '30.3% improvements over ChatGPT in terms of providing more correct responses for bug exploration tasks. Our industrial partner is working on deploying CHIME as a Slack bot. The conceptualization of CHIME originated from an internal demo of a similar chatbot created by our partner one year ago. Their initial chatbot lacked the required accuracy, which we sought to address by developing CHIME. To further motivate the need for such a chatbot beyond our industrial partner, we conducted a survey of 47 software practitioners (see Section III). The survey findings highlight the necessity of such chatbots in the real world. Feedback from 31 industry participants, presented in Section VII-C shows that CHIME is preferred for its ability to analyze, summarise, and label issues. Our industrial partner was involved in the design and evalu- ation of CHIME, which was crucial for advancing CHIME from the proof-of-concept stage to the current deployment stage within the company. Like any innovation,',\n",
       "  'chunk_id': 60},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'the proof-of-concept stage to the current deployment stage within the company. Like any innovation, we expect to improve CHIME in an agile manner, i.e., based on user In the future, we will also focus on expanding CHIMEs grasp of more technical terminologies and other documents. To handle other documents, in CHIME we will improve the issue preprocessor module e.g., to separate code and textual contents, and to adapt the CFG to handle code snippet- s/traces/crash dumps in those documents or using a static partial program analyzer to handle code examples in API documentation. We expect that the other modules in CHIME can be used with minimal changes.DATA AVAILABILITY The code and data used for this study can be found here: [1] Z. Ji et al. , Survey of hallucination in natural language generation, ACM Computing Surveys , vol. 55, no. 12, pp. 1 38, 2023. [2] K. Filippova, Controlled hallucinations: Learning to generate faithfully from noisy data, in Findings of the Association for',\n",
       "  'chunk_id': 61},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'hallucinations: Learning to generate faithfully from noisy data, in Findings of the Association for Computational Linguistics: EMNLP 2020 , T. Cohn, Y . He, and Y . Liu, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 864 870. [3] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, ArXiv , vol. abs/2311.05232, 2023. [Online]. Available: [4] M. Jang, D. S. Kwon, and T. Lukasiewicz, BECEL: Benchmark for consistency evaluation of language models, in Proceedings of the 29th International Conference on Computational Linguistics , N. Calzolari, C.-R. Huang, H. Kim, J. Pustejovsky, L. Wanner, K.-S. Choi, P.-M. Ryu, H.-H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim, Y . Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S.-H. Na, Eds. Gyeongju, Republic of Korea: International Committee on',\n",
       "  'chunk_id': 62},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Lee, E. Santus, F. Bond, and S.-H. Na, Eds. Gyeongju, Republic of Korea: International Committee on Computational Linguistics, Oct. 2022, pp. 3680 3696. [Online]. [5] M. Jang and T. Lukasiewicz, Consistency analysis of chatgpt, in Conference on Empirical Methods in Natural Language Processing , 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID: [6] R. Cohen, M. Hamri, M. Geva, and A. Globerson, LM vs LM: Detecting factual errors via cross examination, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 12 621 12 640. [Online]. Available: https://aclanthology.org/2023.emnlp-main. [7] B. A. Galitsky, Truth-o-meter: Collaborating with llm in fighting its [8] OpenJ9. (2024, February) Issue 18151: To accept openjdk option +useg1gc to enable startup of elasticsearch application]. [Online]. [9] D. Arya, W. Wang, J. L. C. Guo,',\n",
       "  'chunk_id': 63},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'to enable startup of elasticsearch application]. [Online]. [9] D. Arya, W. Wang, J. L. C. Guo, and J. Cheng, Analysis and detection of information types of open source software issue discussions, inProceedings of the 41st International Conference on Software Engineering , ser. ICSE 19. IEEE Press, 2019, p. 454 464. [Online]. [10] Z. Jiang, F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang, J. Callan, and G. Neubig, Active retrieval augmented generation, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 7969 7992. [Online]. Available: https: [11] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K uttler, M. Lewis, W.-t. Yih, T. Rockt aschel, S. Riedel, and D. Kiela, Retrieval-augmented generation for knowledge-intensive nlp tasks, in Proceedings of the 34th International Conference on Neural Information',\n",
       "  'chunk_id': 64},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'nlp tasks, in Proceedings of the 34th International Conference on Neural Information Processing Systems , ser. NIPS20. Red Hook, NY , USA: Curran [12] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang, Realm: retrieval-augmented language model pre-training, in Proceedings of the 37th International Conference on Machine Learning , ser. ICML20. [13] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, Retrieval augmentation reduces hallucination in conversation, in Findings of the Association for Computational Linguistics: EMNLP 2021 , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3784 3803. [Online]. Available: https: [14] K. Beelen and D. van Strien, Metadata might make language models better, 2022. [Online]. Available: https://arxiv.org/abs/2211.10086[15] X. Ma, Y . Gong, P. He, hai zhao, and N. Duan, Query rewriting in retrieval-augmented large language models, in The 2023',\n",
       "  'chunk_id': 65},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'hai zhao, and N. Duan, Query rewriting in retrieval-augmented large language models, in The 2023 Conference on Empirical Methods in Natural Language Processing , 2023. [Online]. [16] S. Z. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. E. Weston, Chain-of-verification reduces hallucination in large language models, 2024. [Online]. Available: [17] T. Chen, F. Kuo, H. Liu, P. Poon, D. Towey, T. Tse, and Z. Zhou, Metamorphic testing: A review of challenges and opportunities, ACM Computing Surveys , vol. 51, no. 1, pp. 1 27, 2018. [18] C. Wang and R. Sennrich, On exposure bias, hallucination and domain shift in neural machine translation, 07 2020. [19] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang, Exposing attention glitches with flip-flop language modeling, in Thirty-seventh Conference on Neural Information Processing Systems , 2023. [Online]. [20] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, Deduplicating',\n",
       "  'chunk_id': 66},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, Deduplicating training data makes language models better, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 8424 8445. [Online]. [21] N. Dziri, A. Madotto, O. Za ane, and A. J. Bose, Neural path hunter: Reducing hallucination in dialogue systems via path grounding, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 2197 2214. [Online]. [22] H.-S. Chang and A. McCallum, Softmax bottleneck makes language models unable to represent multi-mode word distributions, in Proceedings of the 60th Annual Meeting of the Association',\n",
       "  'chunk_id': 67},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'multi-mode word distributions, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 8048 8073. [Online]. [23] B. Dhingra, M. Faruqui, A. P. Parikh, M.-W. Chang, D. Das, and W. W. Cohen, Handling divergent reference texts when evaluating table-to-text generation, ArXiv , vol. abs/1906.01081, 2019. [Online]. [24] S. Lin, J. Hilton, and O. Evans, TruthfulQA: Measuring how models mimic human falsehoods, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 3214 3252. [25] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee, A. Roberts, T. B. Brown, D. X. Song, U. Erlingsson, A. Oprea, and C. Raffel,',\n",
       "  'chunk_id': 68},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Herbert-V oss, K. Lee, A. Roberts, T. B. Brown, D. X. Song, U. Erlingsson, A. Oprea, and C. Raffel, Extracting training data from large language models, in USENIX Security Symposium , 2020. [Online]. Available: [26] Z. Li, S. Zhang, H. Zhao, Y . Yang, and D. Yang, Batgpt: A bidirectional autoregressive talker from generative pre-trained transformer, ArXiv preprint , vol. abs/2307.00360, 2023. [27] Y . Liu, Y . Yao, J.-F. Ton, X. Zhang, R. Guo, H. Cheng, Y . Klochkov, M. F. Taufiq, and H. Li, Trustworthy llms: A survey and guideline for evaluating large language models alignment, ArXiv preprint , vol. [28] N. Lee, W. Ping, P. Xu, M. Patwary, M. Shoeybi, and B. Catanzaro, Factuality enhanced language models for open-ended text generation, ArXiv , vol. abs/2206.04624, 2022. [Online]. Available: https://api. [29] W. Shi, X. Han, M. Lewis, Y . Tsvetkov, L. Zettlemoyer, and S. W.-t. Yih, Trusting your evidence: Hallucinate less with context-aware decoding, ArXiv preprint , vol.',\n",
       "  'chunk_id': 69},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Yih, Trusting your evidence: Hallucinate less with context-aware decoding, ArXiv preprint , vol. abs/2305.14739, 2023. [30] A. Radford et al. , Language models are unsupervised multitask learn- ers, OpenAI blog , vol. 1, no. 8, p. 9, 2019. [31] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al. , The pile: An 800gb dataset of diverse text for language modeling, arXiv preprint arXiv:2101.00027 , 2021.[32] H. Touvron et al. , Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2306.00186 , 2023. [33] A. Sinitsin, V . Plokhotnyuk, D. Pyrkin, S. Popov, and A. Babenko, Editable neural networks, in 8th International Conference on Learning Representations, ICLR 2020 . Addis Ababa, Ethiopia: OpenReview.net, [34] Y . Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, Editing large language models: Problems, methods, and opportunities, in Proceedings of the 2023 Conference on Empirical Methods',\n",
       "  'chunk_id': 70},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Problems, methods, and opportunities, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 10 222 10 240. [Online]. Available: [35] Z. Jiang et al. , Active retrieval augmented generation, arXiv preprint [36] D. Sobania, M. Briesch, C. Hanna, and J. Petke, An analysis of the automatic bug fixing performance of chatgpt, in 2023 IEEE/ACM International Workshop on Automated Program Repair (APR) . Los Alamitos, CA, USA: IEEE Computer Society, may 2023, pp. 23 30. [Online]. Available: https://doi.ieeecomputersociety.org/10. [37] A. Radford and K. Narasimhan, Improving language understanding by generative pre-training, 2018. [Online]. Available: https://api. [38] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. GONG, M. Zhou, N. Duan, N.',\n",
       "  'chunk_id': 71},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. GONG, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. LIU, CodeXGLUE: A machine learning benchmark dataset for code understanding and generation, inThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1) , 2021. [Online]. Available: [39] J. Lanchantin et al. , Learning to reason and memorize with self-notes, arXiv preprint arXiv:2305.00833 , 2023. [40] B. Peng et al. , Check your facts and try again: Improving large language models with external knowledge and automated feedback, arXiv preprint arXiv:2302.12813 , 2023. [41] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, Retrieval augmentation reduces hallucination in conversation, in Findings of the Association for Computational Linguistics: EMNLP 2021 , M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3784',\n",
       "  'chunk_id': 72},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 3784 3803. [Online]. Available: https: [42] A. Madaan et al. , Self-refine: Iterative refinement with self-feedback, arXiv preprint arXiv:2303.17651 , 2023. [43] C. Malaviya et al. , Quest: A retrieval dataset of entity-seeking queries with implicit set operations, arXiv preprint arXiv:2305.11694 , 2023. [44] P. Manakul et al. , Selfcheckgpt: Zero-resource black-box hallucina- tion detection for generative large language models, arXiv preprint [45] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in North American Chapter of the Association for Computational Linguistics , 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID: [46] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, CodeBERT: A pre-trained model for programming and natural',\n",
       "  'chunk_id': 73},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'B. Qin, T. Liu, D. Jiang, and M. Zhou, CodeBERT: A pre-trained model for programming and natural languages, in Findings of the Association for Computational Linguistics: EMNLP 2020 , T. Cohn, Y . He, and Y . Liu, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 1536 1547. [Online]. Available: https://aclanthology.org/2020.findings-emnlp.139[47] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, J. Yin, D. Jiang, and M. Zhou, Graphcodebert: Pre-training code representations with data flow, ArXiv , vol. abs/2009.08366, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID: [48] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, Unified pre-training for program understanding and generation, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani- Tur, I. Beltagy, S. Bethard, R. Cotterell, T.',\n",
       "  'chunk_id': 74},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani- Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y . Zhou, Eds. Online: Association for Computational Linguistics, Jun. 2021, pp. 2655 2668. [Online]. Available: https://aclanthology.org/2021. [49] C. Raffel, N. M. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, J. Mach. Learn. Res. , vol. 21, pp. 140:1 140:67, 2019. [Online]. Available: [50] L. A. Goodman, Snowball sampling, Annals of Mathematical Statis- tics, vol. 32, no. 1, pp. 148 170, 1961. [51] G. Rodriguez-Perez, G. Robles, and J. M. Gonzalez-Barahona, How much time did it take to notify a bug? two case studies: Elasticsearch and nova, in 2017 IEEE/ACM 8th Workshop on Emerging Trends in Software Metrics (WETSoM) , 2017, pp. 29 35. [52] G. Rodr guez-P erez, Gregorio, Robles, A. Serebrenik, Andy, Zaidman, D. M. Germ an, Jesus, and J. M.',\n",
       "  'chunk_id': 75},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'Rodr guez-P erez, Gregorio, Robles, A. Serebrenik, Andy, Zaidman, D. M. Germ an, Jesus, and J. M. Gonzalez-Barahona, How bugs are born: a model to identify how bugs are introduced in software components, Empirical Software Engineering , vol. 25, pp. 1294 1340, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID: [53] T. Zhang, D. Han, V . Vinayakarao, I. C. Irsan, B. Xu, F. Thung, D. Lo, and L. Jiang, Duplicate bug report detection: How far are we? ACM Trans. Softw. Eng. Methodol. , vol. 32, no. 4, may 2023. [Online]. [54] Z. Rasool, S. Barnett, S. Kurniawan, S. Balugo, R. Vasa, C. Chesser, and A. Bahar-Fuchs, Evaluating llms on document-based qa: Exact answer selection and numerical extraction using cogtale dataset, ArXiv , vol. abs/2311.07878, 2023. [Online]. Available: [55] C. Wang, X. Liu, Y . Yue, X. Tang, T. Zhang, C. Jiayang, Y . Yao, W. Gao, X. Hu, Z. Qi, Y . Wang, L. Yang, J. Wang, X. Xie, Z. Zhang, and Y . Zhang, Survey on factuality in large language models:',\n",
       "  'chunk_id': 76},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'content': 'L. Yang, J. Wang, X. Xie, Z. Zhang, and Y . Zhang, Survey on factuality in large language models: Knowledge, retrieval and domain-specificity, ArXiv , vol. abs/2310.07521, 2023. [Online]. Available: https://api.semanticscholar. [56] M. P. Polak and D. Morgan, Extracting accurate materials data from research papers with conversational language models and prompt engineering, Nature Communications , vol. 15, 2023. [Online]. [57] L. Basyal and M. Sanghvi, Text summarization using large language models: A comparative study of mpt-7b-instruct, falcon-7b-instruct, and openai chat-gpt models, arXiv preprint arXiv:2310.10449 , October 2023. [Online]. Available: https://arxiv.org/abs/2310.10449 [58] LangChain, SQLDatabaseChain, Webpage. [Online]. [59] Hugging Face, all-MiniLM-L6-v2, Webpage. [Online]. Available: [60] F. Rahutomo, T. Kitasuka, and M. Aritsugi, Semantic cosine similarity, 2012. [Online]. Available: https://api.semanticscholar.org/CorpusID:',\n",
       "  'chunk_id': 77},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'Tool-Integrated Reasoning for Bengali Mathematical Department of Computer Science Bangladesh University of Engineering and Technology Department of Computer Science Bangladesh University of Engineering and Technology Abstract In this research paper, we present an innovative approach tailored to solving mathematical problems in Bengali, developed for the DL Sprint 3.0 - BUET CSE Fest 2024 Competition. Our methodology harnesses the power of advanced deep learning models, notably the Qwen 2.5 series, with iter- ative improvements made through prompt engineering, model quantization, and Tool Integrated Reasoning (TIR) to handle complex calculations. Initially, we explored various model archi- tectures, such as fine-tuned Mistral and quantized Qwen models, progressively refining them through translation techniques, RAG (Retrieval-Augmented Generation), and custom dataset curation. Through manual hyperparameter tuning, we optimized param- eters like temperature and top-p to improve model',\n",
       "  'chunk_id': 0},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'manual hyperparameter tuning, we optimized param- eters like temperature and top-p to improve model adaptability and response accuracy. Additionally, the removal of RAG and careful parameter adjustments further contributed to our final models robustness. Our approach demonstrates the potential of advanced NLP techniques in effectively interpreting and solving Index Terms Keywords: Bengali Mathematical Problem Solv- ing, Qwen 2.5, Mathematical Reasoning in Bengali, Tool Integrated Reasoning (TIR), Retrieval-Augmented Generation (RAG), Model Quantization, Prompt Engineering, Self-Consistent The ability to understand and solve mathematical problems is a foundational skill for AI, essential for advancements across fields like science, engineering, and finance. However, while AI models have made strides in various languages, they still face significant challenges when tackling mathematical reasoning in low-resource languages, such as Bengali. This gap becomes particularly evident in tasks',\n",
       "  'chunk_id': 1},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'in low-resource languages, such as Bengali. This gap becomes particularly evident in tasks involving complex problem-solving and precise calculations. To address this, the DL Sprint 3.0 - BUET CSE Fest 2024 Competition [1] intro- duced the unique challenge of building an AI model capable of solving mathematical problems in Bengali, targeting issues akin to those in the Bengali Math Olympiad. This competition not only tests participants technical skills but also aims to push the boundaries of AIs adaptability and performance in Bengali. Our work contributes to this pioneering effort, focusing on enhancing AIs mathematical reasoning in Bengali through advanced NLP and deep learning techniques. Weexplore and iteratively refine state-of-the-art models, such as the Qwen series, alongside strategies like prompt engineering, Tool Integrated Reasoning (TIR), and manual hyperparameter tuning to achieve robust problem-solving capabilities. By contributing to this research, we aim to advance AIs',\n",
       "  'chunk_id': 2},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'robust problem-solving capabilities. By contributing to this research, we aim to advance AIs reach into Bengali language processing, ultimately creating models that can assist students, educators, and researchers in tackling complex problems with precision and reliability. For this task, we aimed to select models capable of effi- ciently solving mathematical problems in Bengali, considering both performance and computational efficiency. Our initial approach involved using a fine-tuned Mistral 7B model [2], which, despite being a strong general-purpose model, did not deliver the desired accuracy for mathematical reasoning tasks. This led us to explore other models better suited for handling mathematical challenges, particularly in a low-resource lan- guage like Bengali. The Qwen series emerged as the most promising option due to its strong performance on mathemat- ical reasoning benchmarks. The Qwen-32B-Instruct model [3], with impressive scores on the MATH benchmark (83.1) and GSM8K',\n",
       "  'chunk_id': 3},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'The Qwen-32B-Instruct model [3], with impressive scores on the MATH benchmark (83.1) and GSM8K benchmark (95.9) [4], was especially appealing for its capability in solving high-level mathematical problems. Given its robust performance, we chose to focus on the Qwen-2.5 series, including the 7B, 14B, and 32B models, each offering different trade-offs between accuracy and computational de- mands. We initially fine-tuned the Qwen-14B-Instruct model for a single epoch, which, though promising, did not provide sufficient improvements in performance. To enhance model ef- ficiency, we implemented VLLM (Variable-Length Language Model) for faster inference, which allowed us to speed up the testing process while maintaining model accuracy. Addition- ally, we employed model quantization techniques to reduce memory requirements, making the models more practical for large-scale inference tasks. To further refine performance, we incorporated Tool Integrated Reasoning (TIR), which enabled the model',\n",
       "  'chunk_id': 4},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'refine performance, we incorporated Tool Integrated Reasoning (TIR), which enabled the model to perform complex calculations using Python. This 8 Nov 2024method improved the models ability to handle mathematical operations effectively. Moreover, manual hyperparameter tun- ing of parameters like temperature and top-p helped optimize the models response accuracy and adaptability. In summary, after exploring various models, we selected the Qwen series, particularly the Qwen-32B-Instruct model, for its exceptional performance in mathematical reasoning. Combined with tech- niques like VLLM, TIR, and hyperparameter optimization, we were able to enhance the models ability to solve mathematical problems in Bengali effectively. In this project, preprocessing was crucial to handling Ben- gali mathematical problems and enhancing the models ability to solve them accurately. Given the complexity of understand- ing Bengali text directly in the initial stage, we leveraged the Qwen-32B-Instruct model',\n",
       "  'chunk_id': 5},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'ing Bengali text directly in the initial stage, we leveraged the Qwen-32B-Instruct model to translate Bengali mathematical questions into English for improved processing. This approach enhanced the performance of our model significantly. Prompt tuning played a key role in optimizing the per- formance of our model as we experimented with various reasoning techniques, including Chain of Thought (COT), Tool Integrated Reasoning (TIR), RAG, self-consistent TIR, and self-consistent COT. We initially designed prompts to guide the model through step-by-step reasoning for COT. For TIR, we adapted the prompts to instruct the model to perform calculations using Python tools for more complex problems. With Self-COT and Self-TIR, the prompts were modified to encourage the model to generate multiple reasoning paths and select the most consistent solution. we experimented with Retrieval-Augmented Generation (RAG). The RAG approach was implemented to provide context and improve the quality of',\n",
       "  'chunk_id': 6},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'Generation (RAG). The RAG approach was implemented to provide context and improve the quality of answers in both Bengali and translated English questions. However , after experimenting with Retrieval-Augmented Generation (RAG), we decided to discontinue its use due to its poor performance, as it did not significantly improve the models accuracy compared to other approaches.Additionally, we fine-tuned hyperparameters such as temperature and topp to control the diversity and confidence of the models responses. Lower temperatures were used for more deterministic answers, while higher values promoted creativity.Adjusting topp helped the model select the most plausible solutions.These prompt tuning and hyperparameter adjustments, particularly with TIR and Self-TIR, significantly improved the models ability to solve complex Bengali math problems. In this section, we present the performance of our deep learning model using various approaches. The model was evaluated on the public leaderboard',\n",
       "  'chunk_id': 7},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'our deep learning model using various approaches. The model was evaluated on the public leaderboard of the DL Sprint 3.0, with the baseline score being 28 out of 100. We tested different configurations, including using various versions of the Qwen model, translation, retrieval-augmented generation (RAG), and Tool Integrated Reasoning (TIR).Translation RAG TIR Self-Consistency Score No No No No 49 Yes(Qwen2.5-14B-Instruct) No No No 61 No Yes No Yes 48 Yes(Qwen2.5-14B-Instruct) Yes No Yes 65 Yes(Qwen2.5-32B-Instruct) Yes No Yes 70 No No No Yes 66 Yes(Qwen2.5-32B-Instruct) No No Yes 73 TABLE I: Performance of Qwen2.5-32B-Instruct with Differ- Temperature Top p Number of Candidates Inference Time(s) Score 0.2 0.9 4 7844.8 68 0.35 0.775 4 7327.0 72 0.4 0.8 4 7391.5 77 0.4 0.8 10 19753.3 77 0.3 0.75 4 7579.1 76 0.3 0.75 5 9721.5 74 0.3 0.75 3 5436.7 71 0.7 0.8 4 7567.7 70 TABLE II: Hyperparameters and performance of the model The experiment began with a baseline score of 28, and the',\n",
       "  'chunk_id': 8},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'and performance of the model The experiment began with a baseline score of 28, and the fine-tuned Mistral 7B model scored only 15, indicating the need for a more powerful model architecture and translation capabilities. Scaling to larger Qwen 2.5 models, such as the 7B and 14B-Instruct, improved performance to 40 points but still showed limitations with certain problem types. The quantized Qwen 2.5-32B-Instruct model performed better, reaching a score of 49 without translation, benefiting from its larger Introducing translation using the 14B-Instruct model boosted the score to 61, allowing the model to leverage pre- existing English knowledge. Further improvement occurred when translation was done using the 32B model. Incorporating Retrieval-Augmented Generation (RAG) with translated En- glish questions raised the score to 70, showing that additional context from external datasets enhanced model performance. However, removing RAG led to a score of 73, suggesting that it may have',\n",
       "  'chunk_id': 9},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'enhanced model performance. However, removing RAG led to a score of 73, suggesting that it may have introduced noise, or the larger model could handle The introduction of Tool Integrated Reasoning (TIR), where Python was used for complex calculations, brought the score to 76. This step significantly enhanced the models efficiency and accuracy, especially with Bengali questions, which saw a notable improvement. The final model, optimized with prompt engineering and hyperparameter tuning, achieved a score of 77, marking the culmination of the models refinement in handling diverse problem types.Model Translation TIR Self-Consistency Score Deepseek-math-7b-Instruct(Baseline) No No Yes 28 mCot(Fine-tuned Mistral 7B) No No No 15 Qwen2.5-14B-Instruct No No No 40 Qwen2.5-Math7B-Instruct No Yes No 39 Yes Yes No 48 NuminaMath-7b-TIR(Finetuned Deepseek-math-7b) Yes(Qwen2.5-14B-Instruct) Yes Yes 66 Yes(Qwen2.5-32B-Instruct) Yes Yes 68 TABLE III: Performance of other tried models with Different',\n",
       "  'chunk_id': 10},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': '66 Yes(Qwen2.5-32B-Instruct) Yes Yes 68 TABLE III: Performance of other tried models with Different Configurations Overall pipeline of our solution Fig. 1: Iterative pipeline Throughout the development of our AI model for solving Bengali mathematical problems, we encountered several key Bengali questions presented without translation consistently scored lower than their translated counterparts. Translating questions into English improved model understanding andproblem-solving, leveraging the models richer pre-trained B. Model Size for Translation Using the Qwen 2.5-32B-Instruct model for translation yielded better results compared to the Qwen 2.5-14B-Instruct model. The larger model demonstrated superior language comprehension, which improved translation quality and con- tributed to overall score improvements.C. RAG Limitations Initial experiments with Retrieval-Augmented Generation (RAG) aimed to enhance the models responses by adding contextual information. However, RAG often',\n",
       "  'chunk_id': 11},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': '(RAG) aimed to enhance the models responses by adding contextual information. However, RAG often introduced noise, leading to a decrease in performance. Removing RAG sub- sequently improved scores, indicating that the inherent ca- pabilities of the larger Qwen models were sufficient without additional retrieved context for this task. D. Scarcity of Bengali Datasets The limited availability of high-quality Bengali mathemati- cal datasets restricted the models exposure to diverse problem E. Inference Optimization with VLLM Integrating Variable-Length Language Modeling (VLLM) enabled faster inference, which was crucial for handling the computational demands of larger models. This optimization helped streamline the testing process within our resource F . Prompt Sensitivity The model exhibited a high sensitivity to prompt wording, with minor adjustments in phrasing significantly affecting output quality and accuracy. Effective prompt engineering became essential to guide the model toward',\n",
       "  'chunk_id': 12},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'quality and accuracy. Effective prompt engineering became essential to guide the model toward optimal solutions, underscoring the importance of precision in prompt construc- G. Quantization for Resource Constraints Due to Kaggles limited GPU memory, we had to quantize the larger Qwen models to make them feasible for testing. This process reduced memory requirements, allowing us to utilize the larger 32B model in a resource-limited environment without compromising performance excessively. H. Fine-Tuning Limitations for Larger Models While fine-tuning smaller Qwen models was manageable, memory constraints in Kaggle prevented us from fine-tuning the Qwen 2.5-32B model. This limitation hindered further customization of the 32B model, restricting our ability to fine- tune it for specific Bengali problem-solving tasks. V. C ONCLUSION AND FUTURE WORK This paper presented a novel approach to solving Bengali mathematical problems by leveraging the Qwen 2.5 series models and optimizing through',\n",
       "  'chunk_id': 13},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'Bengali mathematical problems by leveraging the Qwen 2.5 series models and optimizing through prompt engineering, trans- lation, and Tool Integrated Reasoning (TIR). Our methods demonstrated the efficacy of advanced NLP techniques in mathematical problem-solving, particularly for low-resource languages like Bengali. The Qwen 2.5-32B model, combined with techniques such as self-consistency and TIR, achieved significant improvements over baseline models, highlighting the models potential in handling complex reasoning tasks with minimal Bengali-specific data. Despite these achievements, challenges remain, especially regarding translation dependen- cies, limited Bengali datasets, and memory constraints forlarge models. These factors impacted both model performance and adaptability to a wider range of problem types. Our results also revealed the sensitivity of model outputs to prompt phrasing, underscoring the need for refined prompt engineering To build on this work, we propose the',\n",
       "  'chunk_id': 14},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'underscoring the need for refined prompt engineering To build on this work, we propose the following areas for Enhanced Bengali Data Collection: Increasing the availability and diversity of Bengali mathematical datasets will enable better model training and adaptability to complex problem types. Domain-Specific Fine-Tuning: With sufficient resources, fine-tuning the Qwen 2.5-32B model specifically on Ben- gali math problems could further improve accuracy and Optimized Prompt Engineering: Researching prompt optimization strategies for mathematical reasoning tasks, especially in low-resource languages, could make the model responses more consistent and accurate. Exploration of Lightweight Models: Investigating smaller, efficient models tailored for Bengali could bal- ance memory constraints and inference speed without By addressing these areas, we aim to enhance the utility of deep learning models in Bengali problem-solving, making ad- vanced educational tools more accessible to',\n",
       "  'chunk_id': 15},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'content': 'learning models in Bengali problem-solving, making ad- vanced educational tools more accessible to Bengali-speaking learners and educators. This research lays a foundation for further progress in low-resource language applications of AI in mathematical education and reasoning. [1] S. Sushmit, A. Rahman, A. Azad, A. Rahman, M. S. Hossain, N. Tahmid, S. Islam, F. A. Akash, M. Islam, A. Muhtasim, T. Tahsin, and T. T. Dipto, DL Sprint 3.0 Bengali AI Math Olympiad, Kaggle, 2024. [Online]. [2] H. Lai and M. Nissim, mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics , Bangkok, Thailand, Aug. 2024. Association for Computational Linguis- [3] Qwen Team, Qwen2.5: A Party of Foundation Models, Sept. 2024. [4] Qwen Team, Qwen2.5 Overview, Qwen2 Official Website. Available:',\n",
       "  'chunk_id': 16},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'ENHANCING CLUSTER RESILIENCE : LLM- AGENT BASED AUTONOMOUS INTELLIGENT CLUSTER DIAGNOSIS SYSTEM AND EVALUATION FRAMEWORK Honghao Shi1Longkai Cheng1Wenli Wu1Yuhang Wang1Xuan Liu1Shaokai Nie1Weixv Wang1 Xuebin Min1Chunlei Men1Yonghua Lin1 Recent advancements in Large Language Models (LLMs) and related technologies such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have enabled the creation of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have developed an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovations include a knowledge base tailored for cluster diagnostics, enhanced LLM algorithms, practical deployment strategies for agents, and a benchmark specifically designed for evaluating LLM capabilities in this domain. Through extensive experimentation across multiple dimensions, we have demonstrated',\n",
       "  'chunk_id': 0},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'in this domain. Through extensive experimentation across multiple dimensions, we have demonstrated the superiority of our system in addressing the challenges faced in cluster diagnostics, particularly in detecting and rectifying performance issues more efficiently and accurately than traditional methods. Recent advancements in Large Language Models (LLMs) and complementary technologies such as Retrieval- Augmented Generation (RAG) and Diagram of Thought (DoT) have paved the way for the development of autonomous intelligent systems capable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play methodologies, we have created an LLM-agent system designed to autonomously diagnose and resolve issues within AI clusters. Our innovative approach includes the establishment of a specialized knowledge base for cluster diagnostics, the enhancement of LLM algorithms to better suit the demands of the domain, practical deployment strategies for agents',\n",
       "  'chunk_id': 1},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'LLM algorithms to better suit the demands of the domain, practical deployment strategies for agents within real-world environments, and the development of a benchmark specifically tailored to evaluate LLM capabilities in the context of cluster diagnostics. These components collectively contribute to a robust framework that addresses the complexities inherent in managing AI clusters, particularly in scenarios involving performance degradation or other operational anomalies. Through rigorous experimentation, we have validated the effectiveness of our LLM-agent system across multiple dimensions. Our benchmark, which consists of 150 manually crafted advanced questions, serves *Equal contribution1Beijing Academy of Artificial Intelli- gence, Beijing, China. Correspondence to: Yonghua Lin <yh- >.as a comprehensive evaluation tool that highlights the performance differences between our enhanced LLM-agent and baseline open-source models. In practical applications, the LLM-agent demonstrates',\n",
       "  'chunk_id': 2},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'LLM-agent and baseline open-source models. In practical applications, the LLM-agent demonstrates its superior capability to identify and resolve performance issues more efficiently than traditional methods, reducing the troubleshooting time significantly. For instance, in a simulated scenario where one GPU was throttled to a much lower frequency, our system identified and resolved the issue within a matter of minutes, whereas conventional approaches would have taken a senior operations engineer nearly an hour to diagnose and rectify using pre-written automated detection Moreover, the LLM-agents ability to detect and initiate corrective actions even before the performance degrada- tion is noticed by human operators marks a significant advancement in proactive system maintenance. This capability not only mitigates immediate issues but also enhances the overall availability and reliability of the cluster by preemptively addressing potential faults. By leveraging the strengths of RAG and',\n",
       "  'chunk_id': 3},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'of the cluster by preemptively addressing potential faults. By leveraging the strengths of RAG and DoT, the LLM-agent can autonomously execute remediation measures, thereby freeing up engineering resources to focus on more complex and value-driven tasks. Our research underscores the transformative potential of combining AI-driven diagnostics with practical deployment strategies, setting the stage for a new era of intelligent cluster management solutions. 8 Nov 2024Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework 2.1 LLMs Alignment and Enhancement In recent years, generative artificial intelligence centered around large language models(LLMs) has seen rapid development, with powerful natural language generating capabilities demonstrated by proprietary models such as the GPT series(Achiam et al., 2023) and Gemini series(Team et al., 2023), as well as open-source models like Llama(Dubey et al., 2024) and Qwen(Yang et',\n",
       "  'chunk_id': 4},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'et al., 2023), as well as open-source models like Llama(Dubey et al., 2024) and Qwen(Yang et al., There are multiple approaches to enhancing the capabil- ities of LLMs across different stages such as training, inference, and deployment, as well as in areas like data, algorithms, and computational resources. In light of the achievements of autoregressive models like GPT- 2(decoder-only transformers)(Radford et al., 2019) and LLaMA(transformer++)(Touvron et al., 2023), enhancing the quality of the data has become a critical method for improving the efficacy of models during the pre-training process(Adler et al., 2024; Liu et al., 2024). For modern LLMs, there exists several training or fine- tuning works between pre-training and the deployment. ChatGPT(Ouyang et al., 2022) describes this process as Su- pervised Fine-Tuning (SFT), Reward Modeling (RM), and Reinforcement Learning with Human Feedback (RLHF), while LLaMA3.1(Dubey et al., 2024) integrates these into a continuous process',\n",
       "  'chunk_id': 5},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Feedback (RLHF), while LLaMA3.1(Dubey et al., 2024) integrates these into a continuous process known as Continue Training. Besides training, LLMs can leverage Retrieval-Augmented Generation (RAG)(Lewis et al., 2020) to utilize knowledge from data distributions that were not part of the training set. We can refer to the above content as the alignment and 2.2 AI-agent based Applications After the model parameters have been frozen, it is possible to enhance the inherent capabilities of the model through mechanisms such as chain-of-thought(CoT) reasoning(Wei et al., 2022), scaling test time(Snell et al., 2024), and com- bining CoT LLM and AI agents(Castelfranchi, 1998) as LLM-agent(Park et al., 2023). CoT is a prompting technique used to guide LLMs to generate intermediate reasoning steps before arriving at a final conclusion. There are extensions to classic CoT, such as Tree of Thought (ToT)(Yao et al., 2024) for tree- like backtracking, Graph of Thought (GoT)(Besta et al., 2024) for',\n",
       "  'chunk_id': 6},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': '(ToT)(Yao et al., 2024) for tree- like backtracking, Graph of Thought (GoT)(Besta et al., 2024) for graph-based reasoning, and Diagram of Thought (DoT)(Zhang et al., 2024) for a propose-critique-summarize approach based on topos theory. The development of CoT and the scaling of test-time are unified, with CoT applications always aiming to maintain optimal results with limited test-time or scaling test-time toachieve extraordinaire results(Snell et al., 2024). The CoT series technics are also one of the foundations for build- ing LLM-agents. LLM-agents can leverage LLMs as the processing core while integrating traditional AI-agent capa- bilities such as memory, planning, and execution, creating semi-autonomous software entities that are highly adaptive and capable(Xi et al., 2023). 2.3 Diagnosis and Repair for AI Clusters Constructing and utilizing LLM applications typically re- quire hardware infrastructure on a scale costing millions of or more dollars. Meta constructed the LLM',\n",
       "  'chunk_id': 7},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'hardware infrastructure on a scale costing millions of or more dollars. Meta constructed the LLM application core LLaMA 3.1 within 54 days, leveraging a cluster that included 16,000 GPUs(Dubey et al., 2024), with just the GPU costs amounting to over billion dollars. However, such complex and expensive systems face significant chal- lenges in terms of reliability and availability. During the 54-day training, the Meta cluster experienced 419 unex- pected interruptions, averaging one disruption every three hours. At such a frequency of interruptions, the cluster, from the operating system to the AI framework and distributed scheduling software, requires the ability to capture, identify, attribute, and repair exceptions to ensure successful and ef- ficient model training. Microsofts Superbench(Xiong et al., 2024) has systematically built a suite of standard test cases to comprehensively assess the availability of clusters. In terms of capture and repair, the Torch(Paszke et al., 2019)',\n",
       "  'chunk_id': 8},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'assess the availability of clusters. In terms of capture and repair, the Torch(Paszke et al., 2019) Elastic solution aims to enable automatic restarts of model training, while works such as FlashCheckpoint- ing in DLRover(Wang et al., 2023) focus on reducing the cost of checkpoint saving and loading during the automatic restart process. Building upon automatic restart capabili- ties, many works at the AI framework level have conducted research and practical implementations to enhance reliabil- ity and availability, particularly those featuring highly cus- tomized solutions based on Megatron(Shoeybi et al., 2019). ByteDances Megascale(Jiang et al., 2024) and Alibabas Pai-Megatron(Qian et al., 2024) both provide toolkits for cluster diagnostics, which are used to check the health of servers and networks, as well as to perform manual or auto- mated error identification and repair. With the advancement of AI technologies, researchers are beginning to explore the use of AI techniques to',\n",
       "  'chunk_id': 9},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'advancement of AI technologies, researchers are beginning to explore the use of AI techniques to address cluster diagnostic issues. Using big data techniques to ana- lyze log files was an typical approach to automating cluster diagnostics(Jung & Chung, 2021). However, such meth- ods primarily involve static or real-time analysis of files produced by the training process, which limits their attribu- tion capabilities and means they lack intelligent autonomy, relying instead on pre-written execution and planning pro- cedure.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework AI computing tasks: refers to programs or processes designed to achieve intelligence, such as training large language models, inference with large language models, world model inference, and LLM-agent inference. AI chips: processors suitable for or dedicated to performing AI computing tasks, such as NVIDIA GPUs, Intel Gaudi AI accelerators, and',\n",
       "  'chunk_id': 10},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'dedicated to performing AI computing tasks, such as NVIDIA GPUs, Intel Gaudi AI accelerators, and Google TPUs(Jouppi et al., 2017). AI servers: computers equipped with AI chips that are suitable for or specifically designed to perform AI computing tasks, such as the NVIDIA DGX H100. AI servers often have requirements beyond those of classic servers in terms of stability, availability, cooling, and power AI cluster: a distributed server cluster composed of two or more AI servers set up to accomplish a single target task, such as Metas cluster containing 16 thousand GPUs. Additionally, AI servers typically require RDMA or higher bandwidth interconnect protocals, such as InfiniBand RDMA(Shanley, 2003) and RDMA over Converged Ethernet(RoCE)(Guo et al., 2016), and do not usually adopt Cluster diagnosis: ensuring that AI computing tasks can run with normal performance on the AI cluster, promptly detecting task failures, identifying the points of failure, clarifying the reasons for failure,',\n",
       "  'chunk_id': 11},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'detecting task failures, identifying the points of failure, clarifying the reasons for failure, repairing the corresponding faults, and ensuring the overall availability of the AI cluster. We incorporate advanced techniques from the field of LLM alignment and enhancement to creatively develop a solution for building a cluster intelligent maintenance system based on LLM-agents. Figure 1 illustrates the overall process of The upper part of the figure represents the core component of solution: the LLM-agent. The LLM-agent consists of an agent program and an LLM. The LLM interprets the input information provided by the agent as external stimuli and task instructions, and responds appropriately. The agent then directly writes code or calls specific software interfaces based on the feedback from the LLM, thereby operating the cluster. For LLM itself, there are two main challenges. First, how does the LLM acquire domain-specific knowledge of cluster diagnostics, and furthermore, where does',\n",
       "  'chunk_id': 12},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'does the LLM acquire domain-specific knowledge of cluster diagnostics, and furthermore, where does this knowledge come from. Second, how can the LLM reason and plan? For the entire LLM-agent, ensuring that the LLMs inputs and outputs match with the actual operations performed by the agent controlling the cluster is another crucial aspect that needs to be addressed. Figure 1. Overview of the Intelligent Maintenance System Based In order to solve the above problems, we have introduced three innovations. First, we use 250 cluster failure records collected from GitHub as a starting point, and treat the cluster operation failure logs actually managed by the LLM-agent as a continuous source of data. We utilize RAG(Lewis et al., 2020) to enable the LLM to capture detailed knowledge corresponding to specific terms within the context. Figure 1 describes the alert, compute cluster, and storage sections, along with their communication with the LLM-agent, which outlines this process. Second, we',\n",
       "  'chunk_id': 13},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'along with their communication with the LLM-agent, which outlines this process. Second, we use DoT(Zhang et al., 2024) enables the model to effectively handle non-natural language information such as symbols, formulas, and code. Similar to vision-text multimodal models, we effectively leverage textual elements that go beyond the inherent meaning of natural language based on DoT. The planning algorithm section at the top of Figure 1 illustrates this innovation. Third, we use self-play technology(Snell et al., 2024) to enable the LLM to autonomously, also intelligently, devides long tasks or challenging reasoning objectives into multiple steps, self-assess the output of each step, and ultimately The lower part of Figure 1 forms the basis of our work. It includes a mature operations alarm troubleshooting and repair process, as well as several mature or advanced software tools. Based on related works, we have developed a unified, multi-level, multi-dimensional cluster diagnostic toolkit',\n",
       "  'chunk_id': 14},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'works, we have developed a unified, multi-level, multi-dimensional cluster diagnostic toolkit as Figure 2. This tool diagnoses the health status of the cluster fromEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Figure 2. Tools for LLM-agent to Diagnose AI Cluster both the supply side and the demand side simultaneously. The bottom part of Figure 2 lists the various components required to build an AI cluster, including the computing component, storage component, network component, and others. AI clusters following different technical routes provide similar capabilities, as shown in the middle part of Figure 2. We inspect all resource supply items affecting AI computing tasks to determine if their content is correct, if their performance is appropriate, and if they are stable. For example, for the feature of RDMA read/write between two GPUs across servers, our tool checks whether the read/write content is correct,',\n",
       "  'chunk_id': 15},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'between two GPUs across servers, our tool checks whether the read/write content is correct, whether the IOPS, bandwidth, latency, and other performance metrics are appropriate, and the stability under complex scenarios such as long-duration or multi-process read/writes. Most of these tools are improved versions of packages provided by chip, server, or operating system vendors. The top part of Figure 2 takes the demand side into consideration, evaluating the metric of concern for AI computing tasks with various characteristics. In summary, we have built an LLM-agent capable of retriev- ing and utilizing vast amounts of external information, with autonomous planning, learning, reasoning, and execution capabilities. This LLM-agent works alongside either custom-written tools or existing mature tools to perform early warning, troubleshooting, and repair tasks for the 4.2 Cluster Diagnosis Domain-specific Knowledge Our knowledge base consists of two sources. One part is logs, monitoring',\n",
       "  'chunk_id': 16},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Domain-specific Knowledge Our knowledge base consists of two sources. One part is logs, monitoring information, or program output content, come from pre-collected, cleaned, and organized GitHub data, carefully selected to address pain points in the cluster diagnostics and troubleshooting domain, incorporating knowledge from issues in the GitHub community, also come from operational data acquired after the initial deployment and operation of the LLM-agent. We call it Diagnosis Dataset. The second part is composed of symbolicreasoning. These reasoning structures use AI computation tasks and hardware specification information as input, and through a bottom-up modeling approach, predict the theoretical performance of the given AI computation tasks, thereby determining the correctness of the performance. We drew on effective practices from Alibabas experience in managing cluster startup operations(Xu et al., 2024) to build a database. We cleaned, organized, and structured the unstructured',\n",
       "  'chunk_id': 17},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'et al., 2024) to build a database. We cleaned, organized, and structured the unstructured data obtained from GitHub, ultimately forming an effective dataset. We collected over a thousand questions and feedback items from the GitHub issue section. Through automated processes and manual review, we filtered out over 200 entries with substantive knowledge content and well-structured Q&A formats. Each piece of organized data contains four fields: problemkey, rawtext, function, and The problemkey is a domain keyword identified either manually or based on openai o1. Rawtext refers to the original content of a website after simple formatting, stored as a long string containing the questions asked on the web page and the developers responses. The function is based on our cluster diagnosis toolkits and is manually correlated by cluster troubleshooting personnel. This part is used as annotation in the portion of the dataset that the model can perceive, it is not perceived by the model for the',\n",
       "  'chunk_id': 18},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'in the portion of the dataset that the model can perceive, it is not perceived by the model for the answers used in the benchmark evaluation part, and it serves as the starting point for knowledge acquisition after the LLM-agent is deployed. The final results are the causes of the faults extracted from the rawtext based on the developers answers. For an LLM capable of driving an agent to perform cluster diagnostics, we expect it to be able to determine the causes of faults based on real-time operational information from the cluster and to call existing tools or write tool code on-the-fly for cluster repairs, without relying on rawtext containing developer replies. We will demonstrate this capability in subsequent experiments. We use a series of progressive methods to model the correct performance of given AI computation tasks, and through the DoT, we convert this special modal data into tokens to feed into the model. In addition to cluster health check, we have included modules in the',\n",
       "  'chunk_id': 19},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'tokens to feed into the model. In addition to cluster health check, we have included modules in the toolkits to determine whether different AI computing tasks exhibit correct performance. These modules can, on one hand, be invoked by the agent to provide results to the LLM for analysis, and on the other hand, they can be called by the LLM to have the agent check We start modeling with the simplest task types. ConsideringEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework that existing AI clusters are composed of computing devices with the von Neumann architecture, AI computing tasks require the use of computing cores, memory, and I/O ports. It is worth noting that what AI computing tasks occupy are not narrowly defined CPU computing cores, main memory, or input/output ports, but rather in a broader sense, such as computing cores dedicated to matrix multiplication, HBM memory composed of multi-level caches, and',\n",
       "  'chunk_id': 20},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'computing cores dedicated to matrix multiplication, HBM memory composed of multi-level caches, and high-speed I/O ports formed by PCIe or RDMA protocols. To build a unified model, we use the concepts of equivalent computing power, equivalent memory bandwidth, and equivalent I/O We refer to computational tasks that occupy or primarily oc- cupy one type of resource as single-resource computational tasks. We construct a single-variable computational task per- formance model and use experiments based on Khinchins law of large numbers to get the results. We assume that for a certain computational task T, the total amount of resource Rirequired is Mi. The hardware running this task can pro- videNiunits of resource Riper second. Assume that the single-variable task Txdepends only on resource R0. We determine M0based on the mathematical formula used for the tasks computation. For N0, we consider it a random variable. Through a large number of repeated experiments after warm-up, we ensure that',\n",
       "  'chunk_id': 21},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'it a random variable. Through a large number of repeated experiments after warm-up, we ensure that the difference between the measured results and the expected value of the random vari- able approaches zero. We define performance as the number of times a specific task can be executed per unit time. For the aforementioned task Tx, we predict its performance to For non-single-variable tasks, we focus on modeling whether the different resources they depend on can oper- ate in parallel. A widely used method in multivariate task modeling is the roofline model(Ofenbeck et al., 2014). The roofline model introduces a new variable: task characteristic CT. The Roofline model introduces a new variable: the task characteristic CT. Consider a task Txdepends on two resources R0andR1, the effective utilization of resource R0is plotted on the Y-axis, and the ratio of effective utiliza- tion of resource R0to resource R1is plotted on the X-axis. By changing CT, a scatter plot can be drawn, forming a',\n",
       "  'chunk_id': 22},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'R0to resource R1is plotted on the X-axis. By changing CT, a scatter plot can be drawn, forming a shape like a roofline. The Roofline model is equivalent to modeling the performance of multivariable tasks under fully parallel scenarios, which does not align with real-world conditions. Additionally, in the context of existing LLM performance modeling, changes in CTare not about varia- tions in the input size of a single task but about the changing proportions of two different primary resource-consuming tasks within the total task. Therefore, we use the proportion of different subtasks as variables to model multivariable tasks for the three main resources provided by AI clusters: equivalent floating- point computing power for matrix multiplication, memory read/write bandwidth, and I/O port bandwidth. The results 0% computeA B CFigure 3. Multi-variable Task Performance Modeling. A shows compute-memory, B shows interconnect-memory, C shows at figure 3 show that computing and memory are in',\n",
       "  'chunk_id': 23},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'B shows interconnect-memory, C shows at figure 3 show that computing and memory are in domains that are completely non-parallelizable, whereas computing, memory, and I/O ports can approach full parallelization. This conclusion and related figures have been compiled and placed in the RAG documentation. 4.3 Create LLM-agent with RAG-DoT-Selfplay 4.3.1 Using RAG to Build an LLM That Can Utilize RAG integrates two core components: retrieval and genera- tion. The retrieval module is responsible for finding context- relevant information from an external knowledge base, a process that typically involves indexing large volumes of documents to quickly locate the most pertinent segments. The retrieved information is then passed to the generation module as additional input. The generation module builds upon a pre-trained language model, leveraging the retrieved context to enhance its generation capabilities, thereby pro- ducing responses that are more accurate and better aligned Considering',\n",
       "  'chunk_id': 24},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'capabilities, thereby pro- ducing responses that are more accurate and better aligned Considering other similar technologies, SFT requires sub- stantial computing resources and may diminish the models inherent generalization capabilities. In-context learning consumes context length and inference time, making it unsuitable for importing datasets with millions of entries. RAG can acquire relevant knowledge during inference with minimal resources and inference time, without altering the weights of the model itself. 4.3.2 Using DoT to Build an Agent That Can Reason and DoT(Diagram of Thoughts)(Zhang et al., 2024) models iter- ative reasoning in LLMs as constructing a Directed Acyclic Graph (DAG) within a single model. The DAG consists of nodes representing propositions, critiques, refinements, and verifications, with edges indicating the logical relationships or dependencies between them. We use XML to handle mul- timodal special symbol data and perform reasoning based Based on the',\n",
       "  'chunk_id': 25},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'We use XML to handle mul- timodal special symbol data and perform reasoning based Based on the principles of DoT, we use XML tags to sep-Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework arate different types of text, including plain text, special symbols, code, formulas, and inference rules. Thanks to the rope positional encoding adopted by LLama3.1, the model can accurately capture the content within XML pairs. Based on the reasoning graph, our experiments confirmed that this application allows the LLM to correctly reason according to specific rules, achieving the capability to support the agent in completing cluster fault attribution and repair tasks. This significantly exceeds the capabilities of pre-trained or 4.3.3 Using Selfplay Techniques to Construct a With the help of RAG and DoT, the LLM can utilize information from outside the training set as well as abstract symbolic reasoning information. However, this',\n",
       "  'chunk_id': 26},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'from outside the training set as well as abstract symbolic reasoning information. However, this still has limitations for an agent designed for intelligent cluster diagnostics. We permit the LLM to generate content over a longer duration. The quality of solutions to challenging problems can be enhanced through multiple rounds of planned selfplay or spontaneous self-questioning and answering by the agent. Spontaneous self-questioning and answering is applied in DoT reasoning. On the planned selfplay process, we transform the complex problem of cluster fault attribution into a three-round process. In the first round, the agent, based on error logs passed from the cluster, prompts the LLM to identify potential keywords from the error items and corresponding solutions from the knowledge base, performing information extraction and RAG. In the second round, the LLM evaluates its own answers, making corrections or accepting them directly, then proceeds to write or call appropriate tools for',\n",
       "  'chunk_id': 27},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'making corrections or accepting them directly, then proceeds to write or call appropriate tools for the Agent to execute. In the final round, the LLM makes an accurate attribution judgment based on the results of the agents interaction with the actual cluster. Compared to existing selfplay work focused on the text side, we integrate it with the agent, granting it the permissions to operate machines and interact with the environment, fully simulating the capabilities of a human engineer to solve problems. We conducted a three-phase experiment to demonstrate the advanced nature of the proposed LLM-agent in the field of cluster intelligent diagnostics. The first phase involves creating a dataset and benchmark for the field of cluster intelligent diagnostics. First, we define the statistical characteristics of the external data knowledge base and introduce the process of generating an evaluation benchmark from this knowledge base. Next, we describe the features of this benchmark and',\n",
       "  'chunk_id': 28},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'evaluation benchmark from this knowledge base. Next, we describe the features of this benchmark and explain its advanced naturein the field of cluster intelligent diagnostics. Throughout this process, we emphasize fairness and impartiality, strictly distinguishing between the parts of the model that can be perceived and the scoring portions of the evaluation. We further elaborate on the benchmark using the results of the mainstream open-source model LLaMA3.1-70B. The second phase involves evaluating the innovative aspects of the three models we proposed RAG, DoT, and selfplay using the aforementioned benchmark for comparative assessment. The experiments in the second phase are aimed at demonstrating the advanced nature of our proposed models in the field of cluster intelligent In the third phase, we expose the LLM-agent to both the training and testing sets in the benchmark, allowing it to operate in its most complete form to address real-world problems encountered in production',\n",
       "  'chunk_id': 29},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'it to operate in its most complete form to address real-world problems encountered in production environments. We demonstrate the accuracy, efficiency, and autonomous intelligence of this solution through two typical cases. Specifically, we found that this solution can provide early warnings for AI clusters, further enhancing the availability Finally, we will conduct a qualitative analysis and discus- sion on the topics of correctness, safety, and reliability, which are at the forefront of the LLM and LLM-agent fields and have yet to be conclusively resolved, to demonstrate the series of work we have undertaken in these areas. 5.1 Statistics and Evaluation for Dataset and The materials provided to the LLM come from three sources. The first source is automatically collected Q&A data from relevant GitHub communities involved in AI cluster troubleshooting, such as the issue sections of repositories like Megatron, PAI, Deepspeed, and NCCL. This serves as our initial dataset. The data has',\n",
       "  'chunk_id': 30},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'like Megatron, PAI, Deepspeed, and NCCL. This serves as our initial dataset. The data has undergone two rounds of filtering, both automatic and manual, retaining parts with clear solutions and logical dialogues. The second source is the program output obtained by the LLM-agent using RAG+DoT technology on several AI clusters running tasks. These tasks are executed on clusters ranging from 4 to 100 A800 AI servers. The third part consists of special modal data such as symbolic representations and formulas processed using XML according to DoT logic, all of which are unified into the text modality. The total amount of pure text material is 200+ items compared with 1.2GB origin files. This also confirms that if more than 200 items consist of pure text content is fully pre-tokenized to serve as the context for LLM inference, it not only poses a significant challenge to the LLMs capability to handle long texts but also increases theEnhancing Cluster Resilience: LLM-agent Based Autonomous',\n",
       "  'chunk_id': 31},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'to handle long texts but also increases theEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework consumption of inference resources, thereby slowing down the execution speed of the LLM-agent. 5.1.2 Benchmarks Source and Statistics for Benchmark We divided the original dataset into two parts, approximately in a 20%-80% ratio. From the 80%, we manually compiled 150 questions to assess the LLMs capabilities in the field of cluster diagnostics. During comparative experiments, unless otherwise specified, we provide only 20% of the original data to all models. During case studies and practical applica- tions, we provide the entire original dataset to the deployed We designed three evaluation metrics. Metric A evaluates the large models information extraction capabilities, in- cluding extracting the cluster IP addresses and SSH port numbers from conversations, as well as the ability to deter- mine whether further execution is',\n",
       "  'chunk_id': 32},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'port numbers from conversations, as well as the ability to deter- mine whether further execution is needed, evaluated through string matching. The challenge here is to assess the models ability to follow instructions and extract information, since logs are derived from user conversations and may contain unnecessary commands that need to be ignored during the determination process. Metric B evaluates the large models code generation capabilities in the diagnostic domain, in- cluding the ability to generate prescribed code based on descriptions given in conversations, control the input and output of the code, and create unseen test cases, imple- mented in a manner similar to human-eval(Chen et al., 2021) but transferred to a real distributed cluster. Metric C eval- uates the large models information attribution capabilities in the diagnostic domain, including the ability to provide attribution based on users error logs and information. This is currently implemented through',\n",
       "  'chunk_id': 33},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'attribution based on users error logs and information. This is currently implemented through multiple-choice questions. 5.1.3 Evaluation of Benchmark on Standard We applied this benchmark to several of the most widely used open-source LLMs, namely LLaMA3.1-70B, nemotron-70B(Adler et al., 2024), mistral-120B(Jiang et al., 2023), and llama3.2 3B. The results is in table 1. Due to the lack of relevant data and information, as well as reasoning logic such as DoT, all models were only able to complete the first task, scoring zero on the second and third tasks. Since the results of llama3.2 3B did not meet the minimum requirements for building the LLM-agent, and the 120B model is difficult to infer on a single AI server, we opted for the better-performing and more widely used LLama3.1-70B out of the two 70B models as the basis for subsequent SFT (Supervised Fine-Tuning) and the application of RAG, DoT, and selfplay.Table 1. Benchmarks Results on Open-source LLMs 70Bno yes 0.8658 0.0 0.0',\n",
       "  'chunk_id': 34},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'of RAG, DoT, and selfplay.Table 1. Benchmarks Results on Open-source LLMs 70Bno yes 0.8658 0.0 0.0 70Bno yes 0.7315 0.0 0.0 120Bno no 0.7383 0.0 0.0 3Byes yes 0.047 0.0 0.0 Table 2. MMLU Benchmarks Results on LLama3.1 and Nemotron Model SFT or notMMLU We conduct two parts of experiments to comprehen- sively evaluate and compare the innovative effects of our work. In the first part, we use the mature and universal MMLU(Hendrycks et al., 2020) benchmark to evaluate the comprehensive ability of the model in basic text understand- ing after it has been enhanced by RAG, DoT, and self-play. In the second part, through ablation and comparison exper- iments, combined with the focus areas of the sub-items in our proposed benchmark, we quantitatively demonstrate the advantages of our three innovations. 5.2.2 General Capability Evaluation Based on MMLU Firstly, we aim to substantiate why SFT is not advisable in this domain. Although the LLM that supports the agent needs to possess extensive',\n",
       "  'chunk_id': 35},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'not advisable in this domain. Although the LLM that supports the agent needs to possess extensive knowledge in cluster diagnostics, performance modeling, and code writing, we discovered that when the LLM reaches a level where this knowledge can be effectively applied, it often lacks the fundamental interaction capabilities required to engage with the agent. We illustrate this point using the MMLU benchmark. We converted the knowledge repository into tokens compatible with the model and constructed an instruction dataset. We iterated through multiple training rounds until the model could respond correctly to instructions. We thenEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Table 3. Multi Comprehensive Benchmarks Results on LLMs 70Bno 0.6246 0.8691 0.8786 0.3720 0.8230 70Byes 0.6032 0.8649 0.8862 0.3680 0.8007 70Bno 0.6280 0.8620 0.8780 0.3680 0.8234 70Byes 0.6126 0.8653 0.8859 0.3580 0.7917 120Bno 0.6544 0.8788',\n",
       "  'chunk_id': 36},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': '0.6280 0.8620 0.8780 0.3680 0.8234 70Byes 0.6126 0.8653 0.8859 0.3580 0.7917 120Bno 0.6544 0.8788 0.9012 0.3980 0.8229 3Bno 0.4352 0.7428 0.7835 0.2800 0.6040 evaluated the SFT model that reached this state against the original open-source model using the Multi-Machine Learning Understanding (MMLU) benchmark. The results are presented in Table 2. From the above results, it can be seen that Supervised Fine-Tuning (SFT) leads to a decline in performance when evaluated using general assessment methods such as MMLU. Subsequently, in our proposed cluster diagnostics benchmark, we further observed adverse consequences of this performance decline in metric C. As a result, we ultimately decided not to use the SFT approach to construct To avoid the potential risks associated with relying solely on MMLU, we further selected three additional LLM benchmarks that are closely related to the problems we aim to solve in our domain or are entirely generalizable: Abstraction and Reasoning',\n",
       "  'chunk_id': 37},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'the problems we aim to solve in our domain or are entirely generalizable: Abstraction and Reasoning Challenge(ARC)(Peter, 2022), BoolQ(Clark et al., 2019), and OpenbookQA(Mihaylov et al., 2018). The results are presented in the table 3. The results of this set of experiments support the conclusions we drew from the MMLU benchmark. 5.2.3 Results of Our Benchmark Table 4 presents all of our experimental results. The second column of the table indicates whether there was cheating. We define experiments that do not participate fairly in the benchmark as cheating. While this is unfair for the benchmark portion, it is clearly meaningful for our core research objective: to build an LLM-agent system that can autonomously and intelligently perform cluster diagnostics and troubleshooting. When evaluating the benchmark section, the cheating items can be considered as ground These experimental results can illustrate several conclusions.Table 4. Benchmarks Results on Open-source LLMs(baselines)',\n",
       "  'chunk_id': 38},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'can illustrate several conclusions.Table 4. Benchmarks Results on Open-source LLMs(baselines) 70BNone None 0.8658 0.0 0.0 plan)None 0.8658 0.4615 0.6470 70BNone SFT 0.0 0.0 0.0 planSFT 0.0 0.9230 0.0 70BNone RAG 0.8658 0.0 0.0 planRAG 0.8658 0.4615 0.7059 70BNone None 0.7315 0.0 0.0 planNone 0.7315 0.4615 0.7059 120BNone None 0.7383 0.0 0.0 planNone 0.7383 0.7692 0.8235 3BNone None 0.047 0.0 0.0 planNone 0.047 0.2307 0.1176 First, we found that a pre-defined plan can help a naive LLM control the agent. However, this plan was specifically written based on the benchmark questions and cannot beEnhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework used in a production environment. Correspondingly, all experiments utilizing DoT technology and not cheating scored well on metrics B and C for evaluating the agent, although the scores were slightly lower than those achieved with preplanning. This indicates that our proposed',\n",
       "  'chunk_id': 39},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'scores were slightly lower than those achieved with preplanning. This indicates that our proposed knowl- edge processing approach based on DoT and self-play can be used to control cluster troubleshooting agents. Second, we found that SFT significantly improved the scores on metric B, which focuses on evaluating code writing or the invocation of diagnostic tools. However, as a trade-off, all models that underwent SFT, even with preplanning, were unable to control the agent properly, resulting in poor performance on metric C. Third, we found that the results based on LLama3.1-70B were not significantly different from those of Mistral-120B, which has nearly twice the number of parameters. Twice the number of parameters implies double or more inference costs (considering multi-GPU linearity), making it impractical. On the other hand, the 3B smaller model, even with preplanning in a cheating scenario, is still unable to handle the task of We proceeded with subsequent experiments and actual',\n",
       "  'chunk_id': 40},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'scenario, is still unable to handle the task of We proceeded with subsequent experiments and actual deployment using the LLM-agent enhanced with the whole dataset and all of our innovative methods. 5.3 Intelligent Early Warning and Troubleshooting: A To demonstrate the superiority of the LLM-agent system we have built in the context of intelligent cluster diagnostics, we can present a concrete example to illustrate how the system operates and how it is more efficient and accurate compared to traditional methods. In the production environment of AI clusters, abnormal events or interruptions are not the most challenging problems to resolve. Clear information about anomalies or interruptions can effectively guide senior engineers in diagnosing the causes of issues. Current research is also progressively integrating technologies such as automatic restarts and automatic scheduling into the procedures for handling anomalies or interruptions in AI computing tasks. However, once an AI',\n",
       "  'chunk_id': 41},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'the procedures for handling anomalies or interruptions in AI computing tasks. However, once an AI computing task exhibits slow performance, it becomes difficult to quickly identify the problem, and it is even harder to pinpoint the cause of the slowdown. Assume there is an AI training cluster composed of dozens of servers, where one of the servers suddenly experiences a performance drop. This could be due to various reasons, such as increased network latency, memory leaks, high CPU load, or insufficient storage space. Traditionally, administrators or engineers would check the log files of the cluster to manually identify possible issues. This would involve reviewing logs from different nodes, monitoring system metrics, attempting to reproduce the problem, andso on. This method is time-consuming and labor-intensive and may require multiple attempts to pinpoint the root cause. In our system, the LLM-agent automatically gathers relevant log information, performance metrics, and other',\n",
       "  'chunk_id': 42},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'the LLM-agent automatically gathers relevant log information, performance metrics, and other necessary data from the nodes of the cluster. Leveraging the LLM-agents capabilities assessed through the benchmark, the system extracts useful information from the collected data, such as cluster IP addresses, SSH ports, and other crit- ical diagnostic details. Using its diagnostic capabilities in code generation and information attribution, the LLM-agent identifies the root cause of the issue based on the collected data and information. This may include generating new test cases to validate hypotheses. Once the problem is identified, the LLM-agent generates corresponding remediation scripts and requests human review. After approval, the LLM-agent executes the remediation measures in the cluster. Following the execution of remediation measures, the system collects data again to assess the outcome, forming a closed loop of data, algorithm, and hardware to optimize future diagnostic We manually',\n",
       "  'chunk_id': 43},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'forming a closed loop of data, algorithm, and hardware to optimize future diagnostic We manually constructed a scenario. This scenario would lead to slow performance in AI model training tasks and has repeatedly occurred in the development environment. We simulated an extreme heat situation with HV AC failure, throttling the frequency of one of the dozens of GPUs to approximately 200 MHz, rather than the 1410 MHz that the A800 GPUs should operate at. Observing the actual logs shows that the speed of this AI computing task decreased to approximately one-third of its normal performance. Our LLM-system initially flagged the slow AI task through power consumption monitoring and performance modeling results, triggering an automatic alert. Following this, through three rounds of self-play, it recommended checking the GPU core frequencies, a suggestion that the agent then dispatched for execution across all GPUs. Based on the execution results, the LLM accurately pinpointed the GPU with the',\n",
       "  'chunk_id': 44},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'across all GPUs. Based on the execution results, the LLM accurately pinpointed the GPU with the low core frequency that we had specifically altered. The entire troubleshooting process took less than 10 minutes. In contrast, a senior operations engineer would typically need about one hour to correctly identify the problem and then use a pre-written automated detection software tool created by engineers to determine the specific GPU with the low-frequency fault. More importantly, our LLM-agent can identify the fault before algorithm engineers or operations engineers detect the slow-down phenomenon and automatically complete the repair. This achieves resolving the issue before the fault occurs, thereby enhancing the overall availability of the cluster.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework 5.4 Qualitative Analysis of Correctness, Safety, and Based on the existing research that is not yet fully mature, and in',\n",
       "  'chunk_id': 45},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'of Correctness, Safety, and Based on the existing research that is not yet fully mature, and in the context of this specific field of study, we provide reasonable definitions for correctness, safety, and reliability. In this study, we define correctness as whether the process and results of the LLM-agent executing tasks are correct. Compared to evaluating the output of the LLM, assessing the correctness of the LLM-agents actions is more challenging. An apparently incorrect operation process may produce the correct result, whereas seemingly perfect output at the textual level might lead to an erroneous result when executed. Since we focus on the field of cluster diagnostics with the actual output being the execution of procedures by the agent, we do not investigate the potential harmfulness or bias in the textual content generated by the LLM. Instead, we examine the ability of our LLM-agent to avoid performing harmful operations on the cluster when the information fed back to the agent',\n",
       "  'chunk_id': 46},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'to avoid performing harmful operations on the cluster when the information fed back to the agent changes, or even when malicious content is inserted by an attacker, such as deleting files, shutting down, overclocking, or modifying critical system configurations. Regarding reliability, we define it as the overall quality of fault handling by the LLM-agent compared to human engineers or expert human engineers. In addition to whether the attribution is correct, we also consider factors such as the time taken to complete typical fault handling, the resources consumed, and the ability to We incorporate the assessment of correctness into the benchmark evaluation. For the potential risks associated with the LLM-agent, we implement a whitelist plus human review approach. Initially, we ensure the safety of the existing toolkit, followed by creating a whitelist for the program interfaces included in the toolkit and conducting human reviews for the LLM-agents requests to execute self-authored',\n",
       "  'chunk_id': 47},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'in the toolkit and conducting human reviews for the LLM-agents requests to execute self-authored code. Finally, we observed that the LLM-agent can attribute faults with an average of fewer than three test cases across multiple rounds of self-play, which is more efficient than the twelve cases typically required by human experts. However, regarding communication abilities, the LLM-agent currently does not possess such capabilities. The qualitative analysis described above is mainly aimed at reducing the probability of harmful incidents. Quantitative analysis or a comprehensive model still necessitates further advancements in the field of 6 CONCLUSION AND DISCUSSION 6.1 Work Summary and Further Plan Based on our experience and research in the fields of cluster diagnostics, LLM enhancement, and LLM-agent construc-tion, we innovatively proposed a system solution utilizing LLM-agents to autonomously and intelligently perform clus- ter troubleshooting. In terms of LLM algorithms, we intro-',\n",
       "  'chunk_id': 48},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'and intelligently perform clus- ter troubleshooting. In terms of LLM algorithms, we intro- duced a benchmark consisting of 150 advanced problems manually crafted, demonstrating the performance differ- ences between our constructed LLM-agent and the original open-source LLMs under fair data conditions. In the realm of LLM-agent construction, we innovatively proposed inte- grating DoT reasoning mathematics and the ability to handle special symbols and formulas into the agent, enabling the LLM to operate machines at the software level and receive feedback. Ultimately, we applied our innovative achieve- ments to cluster diagnostics, exploring the potential in this field, and were pleasantly surprised to find that the LLM- agent systems, despite being in their extremely early stages, are already capable of handling repetitive and low-end tasks, thus freeing industry practitioners to tackle more challeng- ing and valuable problems. In the future, we will continue our work in four aspects.',\n",
       "  'chunk_id': 49},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'more challeng- ing and valuable problems. In the future, we will continue our work in four aspects. In terms of LLM algorithms, we will expand and upgrade the existing benchmark and build a more comprehensive and valuable metrics system. In the Agent field, we will further unlock the potential of DoT and make self-written code by the LLM gradually become the main execution body, re- ducing reliance on preset tools. At the system application level, we will form a closed loop of data, algorithm, and hardware, enriching the database with results from actual deployments. Finally, in terms of safety and reliability, we will continue to work with researchers in related fields to ensure and evaluate the safety and reliability of the agents. 6.2 Shortcomings and Limitations Our research still has shortcomings and limitations. In terms of shortcomings, our agent currently relies on a mechanism of human review to ensure safety, depends on pre-written tools for code, and relies on data sourced',\n",
       "  'chunk_id': 50},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'of human review to ensure safety, depends on pre-written tools for code, and relies on data sourced from GitHub as a starting point. An ideal LLM-agent system should form a self-sustained relationship with the AI cluster, maintaining In terms of limitations, our work depends on the LLM within the LLM-agent, but smaller models like llama3.2-3B cur- rently cannot support the capabilities of the agent. There- fore, our work can only be applied to data centers or large- scale distributed clusters and cannot be deployed in edge computing or personal computer scenarios. We need to continuously monitor the development of smaller models and explore the possibility of teaching the capabilities of the LLM-agent to smaller models in the form of DoT when appropriate.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman,',\n",
       "  'chunk_id': 51},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Co- hen, J., et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704 , 2024. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Pod- stawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. InProceedings of the AAAI Conference on Artificial In- telligence , volume 38, pp. 17682 17690, 2024. Castelfranchi, C. Modelling social action for ai agents. Artificial intelligence , 103(1-2):157 182, 1998. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint',\n",
       "  'chunk_id': 52},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint Guo, C., Wu, H., Deng, Z., Soni, G., Ye, J., Padhye, J., and Lipshteyn, M. Rdma over commodity ethernet at scale. InProceedings of the 2016 ACM SIGCOMM Conference , Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring mas- sive multitask language understanding. arXiv preprint Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint Jiang, Z., Lin, H., Zhong, Y ., Huang,',\n",
       "  'chunk_id': 53},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint Jiang, Z., Lin, H., Zhong, Y ., Huang, Q., Chen, Y ., Zhang, Z., Peng, Y ., Li, X., Xie, C., Nong, S., et al. {MegaScale }: Scaling large language model training to more than 10,000 {GPUs }. In 21st USENIX Symposium on Net- worked Systems Design and Implementation (NSDI 24) , pp. 745 760, 2024.Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual inter- national symposium on computer architecture , pp. 1 12, Jung, H. and Chung, K. Social mining-based clustering process for big-data integration. Journal of Ambient In- telligence and Humanized Computing , 12(1):589 600, Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., K uttler, H., Lewis, M., Yih, W.-t., Rockt aschel, T., et al. Retrieval-augmented generation for knowledge-',\n",
       "  'chunk_id': 54},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'H., Lewis, M., Yih, W.-t., Rockt aschel, T., et al. Retrieval-augmented generation for knowledge- intensive nlp tasks. Advances in Neural Information Pro- cessing Systems , 33:9459 9474, 2020. Liu, Y ., Tao, S., Zhao, X., Zhu, M., Ma, W., Zhu, J., Su, C., Hou, Y ., Zhang, M., Zhang, M., et al. Coachlm: Automatic instruction revisions improve the data quality in llm instruction tuning. In 2024 IEEE 40th International Conference on Data Engineering (ICDE) , pp. 5184 5197. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint Ofenbeck, G., Steinmann, R., Caparros, V ., Spampinato, D. G., and P uschel, M. Applying the roofline model. In2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS) , pp. 76 85. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to',\n",
       "  'chunk_id': 55},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems , 35:27730 27744, 2022. Park, J. S., OBrien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology , pp. 1 22, 2023. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems , 32, 2019. Peter, E. Abstraction and reasoning challenge. 2022.Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework Qian, K., Xi, Y ., Cao, J., Gao, J., Xu, Y ., Guan, Y ., Fu, B., Shi, X., Zhu, F., Miao, R., et al.',\n",
       "  'chunk_id': 56},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Qian, K., Xi, Y ., Cao, J., Gao, J., Xu, Y ., Guan, Y ., Fu, B., Shi, X., Zhu, F., Miao, R., et al. Alibaba hpn: a data center network for large language model training. In Proceedings of the ACM SIGCOMM 2024 Conference , Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. Shanley, T. InfiniBand network architecture . Addison- Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multi- billion parameter language models using model paral- lelism. arXiv preprint arXiv:1909.08053 , 2019. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test- time compute optimally can be more effective than scal- ing model parameters. arXiv preprint arXiv:2408.03314 , Team, G., Anil, R., Borgeaud, S., Wu, Y ., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models.',\n",
       "  'chunk_id': 57},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971 , 2023. Wang, Q., Sang, B., Zhang, H., Tang, M., and Zhang, K. Dlrover: An elastic deep training extension with auto job resource recommendation. arXiv preprint Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824 24837, Xi, Z., Chen, W., Guo, X., He, W., Ding, Y ., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864 , 2023. Xiong, Y ., Jiang, Y .,',\n",
       "  'chunk_id': 58},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'model based agents: A survey. arXiv preprint arXiv:2309.07864 , 2023. Xiong, Y ., Jiang, Y ., Yang, Z., Qu, L., Zhao, G., Liu, S., Zhong, D., Pinzur, B., Zhang, J., Wang, Y ., et al. {SuperBench }: Improving cloud {AI}infrastructure reli- ability with proactive validation. In 2024 USENIX Annual Technical Conference (USENIX ATC 24) , pp. 835 850, Xu, Y ., Chen, Y ., Zhang, X., Lin, X., Hu, P., Ma, Y ., Lu, S., Du, W., Mao, Z., Zhai, E., et al. Cloudeval-yaml: Apractical benchmark for cloud configuration generation. Proceedings of Machine Learning and Systems , 6:173 Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y ., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems , 36, 2024. Zhang, Y ., Yuan, Y ., and Yao, A. C.-C. On',\n",
       "  'chunk_id': 59},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'content': 'in Neural Information Processing Systems , 36, 2024. Zhang, Y ., Yuan, Y ., and Yao, A. C.-C. On the diagram of thought. arXiv preprint arXiv:2409.10038 , 2024. APLEASE ADD SUPPLEMENTAL MATERIAL Put anything that you might normally include after the refer- ences as an appendix here, not in a separate supplementary file. Upload your final camera-ready as a single pdf, includ-',\n",
       "  'chunk_id': 60},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification Yohei Seki1, Hakusen Shu2, Ana s Lhuissier3, Hanwool Lee4, Juyeon Kang3,Min-Yuh Day5,Chung-Chi Chen6 1Institute of Library, Information, and Media Science, University of Tsukuba, Japan, 2College of Knowledge and Library Sciences, School of Informatics, University of Tsukuba, Japan, 33DS Outscale, France,4Shinhan Securities Co., Korea, 5Graduate Institute of Information Management, National Taipei University, Taiwan6AIST, Japan Promises made by politicians, corporate lead- ers, and public figures have a significant impact on public perception, trust, and institutional rep- utation. However, the complexity and volume of such commitments, coupled with difficul- ties in verifying their fulfillment, necessitate innovative methods for assessing their credi- bility. This paper introduces the concept of Promise Verification, a systematic approach in- volving steps such as promise identification, evidence assessment, and the',\n",
       "  'chunk_id': 0},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'systematic approach in- volving steps such as promise identification, evidence assessment, and the evaluation of tim- ing for verification. We propose the first mul- tilingual dataset, ML-Promise, which includes English, French, Chinese, Japanese, and Ko- rean, aimed at facilitating in-depth verification of promises, particularly in the context of En- vironmental, Social, and Governance (ESG) reports. Given the growing emphasis on corpo- rate environmental contributions, this dataset addresses the challenge of evaluating corpo- rate promises, especially in light of practices like greenwashing. Our findings also explore textual and image-based baselines, with promis- ing results from retrieval-augmented generation (RAG) approaches. This work aims to foster further discourse on the accountability of pub- lic commitments across multiple languages and In a world where promises shape perceptions and drive decisions, the integrity of commitments made by politicians, corporate leaders, and',\n",
       "  'chunk_id': 1},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'and drive decisions, the integrity of commitments made by politicians, corporate leaders, and public fig- ures must be scrutinized. These promises, ranging from environmental sustainability to social respon- sibility and governance ethics, significantly influ- ence the general publics and stakeholders trust, as well as government and corporate reputations. Yet, the complexity and abundance of such commit- ments, coupled with the challenge of verifying their fulfillment, create a pressing need for innovative approaches to assess their strength and verifiabil- ity. Recognizing the critical role of transparencyand accountability in todays society, we propose a groundbreaking task: Promise Verification. To perform promise verification, several steps are required, including (1) identifying the promise, (2) linking the promise with supporting evidence, (3) assessing the clarity of the promise-evidence pair, and (4) inferring the timing for verifying the promise. For example, after',\n",
       "  'chunk_id': 2},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'promise-evidence pair, and (4) inferring the timing for verifying the promise. For example, after identifying a promise, the availability of evidence to support that the speaker is taking action to fulfill the promise could serve as a coarse-grained evaluation of the promises quality. However, the clarity of the ev- idence may also affect the quality of the promise, which would be a fine-grained evaluation. Addi- tionally, whether the speaker provides a clear time- line for verifying the promise is an important crite- rion. For instance, we will achieve net zero carbon emissions within five years is a stronger promise than we will achieve net zero carbon emissions. Following this line of thought, this paper proposes the first multilingual dataset for in-depth promise verification, including Chinese, English, French, In recent years, increasing emphasis has been placed on companies environmental contributions, especially in addressing climate change, deforesta- tion, and compliance',\n",
       "  'chunk_id': 3},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'contributions, especially in addressing climate change, deforesta- tion, and compliance with labor conditions and gov- ernance, when evaluating their investment value. In the evolving landscape of ESG (environmental, social, and governance) criteria, the ability to accu- rately assess a companys promises and adherence to its ESG promises has become paramount. How- ever, unlike traditional financial statements, ESG reports still lack clear standards regarding corporate promises. This allows some companies to use mis- leading information to project an overly positive environmental image, a practice known as green- washing. As Gorovaia and Makrominas (2024) points out, companies involved in environmental misconduct tend to produce longer, more positive, and more frequent reports. We hypothesize that 7 Nov 2024Task Label English French Chinese Japanese Korean Promise IdentificationYes 84.5 80.5 40.2 74.9 77.5 No 15.5 19.5 59.8 25.1 22.5 Supporting EvidenceYes 20.1 71.6 20.1 66.4 75.6 No',\n",
       "  'chunk_id': 4},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': '80.5 40.2 74.9 77.5 No 15.5 19.5 59.8 25.1 22.5 Supporting EvidenceYes 20.1 71.6 20.1 66.4 75.6 No 79.9 28.4 79.9 33.6 24.4 Clarity of Promise-Evidence PairClear 53.3 56.6 64.7 61.2 94.8 Not Clear 42.9 41.9 35.3 34.7 5.2 Misleading 3.8 1.5 0.0 4.1 0.0 Timing for VerificationWithin 2 years 1.9 12.4 37.5 7.3 45.5 2-5 years 14.1 15.0 10.0 9.3 8.4 Longer than 5 years 9.0 21.6 15.0 18.7 17.5 Other 75.0 51.0 37.5 64.7 28.7 Table 1: Label distribution in each language. (%) such reports may lack substantive evidence, or the information presented may be irrelevant or ambigu- ous, leading to misinterpretation. To this end, the proposed dataset, ML-Promise, focuses on ESG reports released by corporations in five countries: the U.K., France, Taiwan, Japan, and Korea. In addition to exploring text-based baselines, we also provide pilot results on image-based experi- ments, as most reports are released in PDF format. Our experiment further shows that the retrieval- augmented generation (RAG)',\n",
       "  'chunk_id': 5},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'released in PDF format. Our experiment further shows that the retrieval- augmented generation (RAG) approach (Lewis et al., 2020) can help in some language cases. Al- though we do not find a silver bullet for all lan- guages and tasks, we hope the proposed dataset can open a new chapter in discussions on the respon- sibility of promises, especially those from public Recent studies have sought to improve the anal- ysis of ESG or sustainability reports for estimat- ing company values using contextual embedding approaches. For example, Gutierrez-Bustamante and Espinosa-Leal (2022) evaluated sustainability reports from publicly listed companies in Nordic countries using latent semantic analysis (LSA) and the global vectors for word representation (GloVe) model, enhancing document retrieval performance based on similarity. Garigliotti (2024) explored the integration of sustainable development goals (SDGs) into environmental impact assessments (EIAs) using a RAG framework powered by large',\n",
       "  'chunk_id': 6},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'goals (SDGs) into environmental impact assessments (EIAs) using a RAG framework powered by large language models (LLMs). Their work focused on two tasks: detecting SDG targets within EIA reports and identifying relevant textual evidence, specifically in European contexts. Hillebrand et al. (2023) introduced sustain.AI, a context-aware rec- ommender system designed to analyze sustainabil- ity reports in response to increasing corporate so-cial responsibility (CSR) regulations. The system, based on a BERT architecture, identified relevant sections of lengthy reports using global reporting initiative (GRI) indicators and demonstrated strong performance on datasets from German companies. Previous studies have a few shortcomings. First, most of them focus solely on reports from one country. Second, none of them attempt to analyze corporate promises, despite the abundance of sus- tainability reports. To address these problems, our study extends these works by focusing on multi- lingual',\n",
       "  'chunk_id': 7},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'reports. To address these problems, our study extends these works by focusing on multi- lingual companies from both European and Asian regions, including Taiwan, the UK, France, Japan, and Korea. With the proposed new task, we aim to highlight the importance of anti-greenwashing by evaluating corporate promises in ESG reports. We collect ESG reports from five countries: the UK, France, Taiwan, Japan, and Korea. The annotators are native speakers of the target language or are familiar with the language at the work level. The task designs are as follows when given a paragraph in the ESG reports. 1.Promise Identification : This is a boolean label (Yes/No) based on whether a promise exists. 2.Supporting Evidence : This is a boolean label (Yes/No) based on whether supporting evidence exists. 3.Clarity of the Promise-Evidence Pair : We designed three labels (Clear/Not Clear/Misleading) for this task, which should depend on the clarity of the given evidence in relation to the promise.',\n",
       "  'chunk_id': 8},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'for this task, which should depend on the clarity of the given evidence in relation to the promise. 4.Timing for Verification : Following the MSCI guide- lines and previous work (Tseng et al., 2023), we set timing labels (within 2 years/2-5 years/longer than 5 years/other) to indicate when readers/investors should return to verify the promise. Here, other denotes the promise has already been verified or doesnt have a specific timing to verify it.Approach Task English French Chinese Japanese Korean w/o RAGPromise Identification (PI) 0.842 0.816 0.521 0.670 0.849 Supporting Evidence (SE) 0.680 0.746 0.163 0.720 0.792 Clarity of Promise-Evidence Pair (CPEP) 0.411 0.443 0.569 0.450 0.897 Timing for Verification (TV) 0.636 0.523 0.317 0.632 0.406 w/ RAGPromise Identification 0.866 0.798 0.540 0.659 0.807 Supporting Evidence 0.757 0.732 0.503 0.850 0.774 Clarity of Promise-Evidence Pair 0.467 0.487 0.628 0.465 0.939 Timing for Verification 0.693 0.601 0.469 0.684 0.571 Table 2: Experimental',\n",
       "  'chunk_id': 9},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': '0.487 0.628 0.465 0.939 Timing for Verification 0.693 0.601 0.469 0.684 0.571 Table 2: Experimental Results (F1-Score). The best performance in each language is denoted in bold . Finally, we obtained 3,010 instances, i.e., 600 for each language and 10 additional instances in the Chinese dataset. The Cohens agreement (Co- hen, 1960; McHugh, 2012) for these tasks is ap- proximately 0.65-0.96, 0.71-0.88, 0.62-0.80, and 0.60-0.89, respectively. Table 1 presents the distri- bution of the proposed ML-Promise dataset. First, we observe that around 35-40% of the evidence is not clear in supporting the associated promises in four out of five languages. This highlights the necessity of the proposed task for evaluating the quality of the promise-evidence pairs from corpora- tions. Furthermore, about 4% of instances contain (potentially) misleading evidence in the English and Japanese datasets. It is crucial for corporations to re-examine this evidence, and it is also essen- tial for supervisory',\n",
       "  'chunk_id': 10},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'is crucial for corporations to re-examine this evidence, and it is also essen- tial for supervisory authorities to monitor these instances. Second, we noted that corporations in Taiwan and Korea tend to make more short-term promises (within 2 years), whereas corporations in the remaining countries tend to make longer-term promises. This finding shows the need for a multi- lingual comparison of ESG reports across different countries, as the narrative styles vary among them. RAG (Lewis et al., 2020) was introduced as a method to enhance LLMs by integrating external knowledge sources. This approach combines re- trieval mechanisms with generative models, pro- ducing more accurate and contextually relevant outputs. Yu et al. (2024) highlights the advantages of RAG systems, particularly their ability to extract domain-specific information. By incorporating ex- ternal retrieval processes, RAG enables generative models to access a broader, field-specific knowl- edge base, improving the',\n",
       "  'chunk_id': 11},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'RAG enables generative models to access a broader, field-specific knowl- edge base, improving the accuracy and relevance of responses. This capability is especially impor- tant for handling domain-specific queries, an areaRAG TaskChinese Korean Image-Based Text-Based Image-Based Text-Based w/oPI 0.530 0.521 0.837 0.849 SE 0.124 0.163 0.812 0.792 CPEP 0.510 0.569 0.922 0.897 TV 0.202 0.317 0.201 0.406 w/PI 0.580 0.540 0.843 0.807 SE 0.512 0.503 0.845 0.774 CPEP 0.618 0.628 0.893 0.939 TV 0.297 0.469 0.330 0.571 Table 3: Image-based experimental results. Bolded denotes the best performance in each language. Underlined denotes performance with RAG better than where existing LLMs often encounter difficulties. Fan et al. (2024) discusses training strategies for RAG, including independent, sequential, and joint methods, which can be tailored to optimize retrieval and generation for specific domains. For Chinese language applications, Wang et al. (2024b) empha- sizes the importance of',\n",
       "  'chunk_id': 12},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'domains. For Chinese language applications, Wang et al. (2024b) empha- sizes the importance of domain-specific corpora over general knowledge sources. Ardic et al. (2024) applied RAG to analyze sustainability reports from ten Turkish companies, focusing on ESG factors. The study found BM25 outperformed BERTurk in retrieving relevant sections, highlighting the effec- tiveness of advanced retrieval techniques. Following the findings of previous studies, we also explore and design the RAG approach for the proposed tasks. Specifically, when given a para- graph, we first retrieve the six most similar samples in the training set. We leveraged Multilingual E5 Text Embeddings (Wang et al., 2024a) to calcu- late the cosine similarity between target paragraphs and instances from the training set. Then, we pro- vide the top-six examples for the LLM to perform in-context learning (Dong et al., 2022). In our ex- periment, we use GPT-4o as the base LLM. In the experiment, we randomly select 200 in-',\n",
       "  'chunk_id': 13},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'In our ex- periment, we use GPT-4o as the base LLM. In the experiment, we randomly select 200 in- stances from each language as the test set, and the remaining instances are used for training. We use the F1 score to evaluate the performance of eachtask. Table 2 shows the performance of each task in each language. First, the performance of most tasks improves when adopting RAG. Specifically, for En- glish and Chinese, all tasks perform better when using RAG. Second, RAG enhances performance in estimating the clarity of the promise-evidence pair and inferring the timing for verification, regard- less of the language used. These results suggest the usefulness of RAG in these two novel tasks. Addi- tionally, the findings demonstrate the value of the proposed annotations. With the proposed dataset, the performance of fine-grained promise evaluation can be improved. Third, although the performance in promise identification and supporting evidence identification tasks may slightly decrease',\n",
       "  'chunk_id': 14},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'in promise identification and supporting evidence identification tasks may slightly decrease in French, Japanese, and Korean, the declines are minimal (less than 2% in most cases). These results suggest that the method for retrieving and suggesting sam- ples similar to the paragraph requires refinement for imbalanced boolean datasets. In future work, we will focus on improving the RAG approach by extracting balanced samples, particularly for minor We noticed a significant difference between Tai- wan/Korea reports and the reports from other coun- tries.1The reports from these two countries uti- lize a large number of graphs instead of textual descriptions. This observation raises the question of whether we could use multimodal LLMs to read PDF files directly instead of relying on extracted text. To explore this, we align the annotations with a PDF page and employ GPT-4o to reassess the tasks using an image as input. For RAG, we lever- aged E5-V Universal Embeddings (Jiang et al., 2024)',\n",
       "  'chunk_id': 15},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'using an image as input. For RAG, we lever- aged E5-V Universal Embeddings (Jiang et al., 2024) to calculate the cosine similarity between target pages and instances from the training set. Table 3 presents the performance. First, using GPT-4o with image input reduces performance in three out of four tasks in the Chinese dataset and in two out of four tasks in the Korean dataset. Sec- ond, RAG improves the performance of most tasks when using image input. Third, with RAG, the per- formance in promise identification and supporting evidence identification tasks improves with Chi- nese image input, and the performance of support- ing evidence identification improves with Korean image input. However, for estimating the clarity of 1We provide some examples in Appendix A.Input RAG Task ROUGE-L Table 4: Results of promise and evidence extraction. the promise-evidence pair and inferring the timing for verification, using text input with RAG remains superior. In summary, our experimental',\n",
       "  'chunk_id': 16},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'timing for verification, using text input with RAG remains superior. In summary, our experimental results sug- gest that image input should be used for PI and SE tasks, while text input is preferable for CPEP and TV tasks. Additionally, RAG performs well regardless of input type. 5.2 Promise and Evidence Extraction In the previous section, we explored the promise and evidence identification tasks. However, the task can also be formulated in an extractive man- ner. Instead of only outputting a yes or no, we can also ask models to extract the promise and evidence from the report. We provide additional annotations in the Chinese dataset and experiment in multimodal settings with and without RAG. The ROUGE-L (Lin, 2004) score is used to evaluate ex- traction performance. Table 4 presents the results. These results indicate that the best performance is achieved in the image-based setting with RAG for both promise and evidence extraction. This empha- sizes the importance of exploring',\n",
       "  'chunk_id': 17},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'with RAG for both promise and evidence extraction. This empha- sizes the importance of exploring multimodal input for ESG report understanding. In this paper, we introduce the concept of Promise Verification, a novel task aimed at evaluating the credibility and fulfillment of promises made by corporations, particularly in the context of ESG reports. We propose the first multilingual dataset, ML-Promise, to emphasize the importance of assessing corporate environmental and social promises. Our results demonstrate that RAG im- proves performance, while also showing the po- tential of multimodal approaches in promise ver- ification. Our annotations will be released under the CCBY-NC-SA 4.0 license. We hope this work serves as a foundation for the robustness of promise verification systems and contributes to greater ac- countability in corporate and public disclosures.Limitation Several limitations warrant discussion. First, al- though the ML-Promise dataset includes five lan-',\n",
       "  'chunk_id': 18},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'Several limitations warrant discussion. First, al- though the ML-Promise dataset includes five lan- guages?Chinese, English, French, Japanese, and Korean?its scope is still limited to a few countries and may not fully capture the diversity of corpo- rate promise communication styles globally. The dataset focuses on ESG reports from specific re- gions, which may limit the generalizability of the findings to other languages and cultural contexts. Future studies can follow our design to expand the dataset to include more regions and languages, which could enhance the robustness and applicabil- ity of the proposed methods. Second, although the study uses RAG to improve performance, the re- sults show that this approach does not consistently outperform baseline models across all languages and tasks. These inconsistencies suggest that RAG may require further optimization or task-specific adjustments, particularly in handling the nuances of each language and dataset structure. These',\n",
       "  'chunk_id': 19},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'adjustments, particularly in handling the nuances of each language and dataset structure. These limitations and our findings highlight ar- eas for future research, including expanding the dataset, refining the RAG approach, enhancing mul- timodal learning, and addressing the inherent am- biguities in corporate ESG reporting. Ozgur Ardic, Mahiye Uluyagmur Ozturk, Irem Demir- tas, and Secil Arslan. 2024. Information Extraction from Sustainability Reports in Turkish through RAG Approach. In 2024 32nd Signal Processing and Com- munications Applications Conference (SIU) , pages Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhi- fang Sui. 2022. A Survey on In-context Learning. arXiv preprint arXiv:2301.00234 . Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs: To-',\n",
       "  'chunk_id': 20},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs: To- wards Retrieval-Augmented Large Language Models. Dario Garigliotti. 2024. SDG target detection in envi- ronmental reports using Retrieval-augmented Gener- ation with LLMs. In Proceedings of the 1st Work- shop on Natural Language Processing Meets Cli- mate Change (ClimateNLP 2024) , pages 241 250, Bangkok, Thailand. Association for Computational Linguistics.Nina Gorovaia and Michalis Makrominas. 2024. Identi- fying greenwashing in corporate-social responsibility reports using natural-language processing. European Marcelo Gutierrez-Bustamante and Leonardo Espinosa- Leal. 2022. Natural Language Processing Meth- ods for Scoring Sustainability Reports? A Study of Nordic Listed Companies. Sustainability , 14(15). Lars Hillebrand, Maren Pielka, David Leonhard, To- bias Deu er, Tim Dilmaghani, Bernd Kliem, R diger Loitz, Milad Morad, Christian Temath, Thiago Bell, Robin Stenzel, and Rafet Sifa.',\n",
       "  'chunk_id': 21},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'Kliem, R diger Loitz, Milad Morad, Christian Temath, Thiago Bell, Robin Stenzel, and Rafet Sifa. 2023. sustain.AI: a Recommender System to analyze Sustainability Re- ports. In Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law , ICAIL 23, page 412?416, New York, NY , USA. Association Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. 2024. E5-V: Universal Embeddings with Multimodal Large Language Mod- Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich K ttler, Mike Lewis, Wen-tau Yih, Tim Rock- t schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge- intensive NLP tasks. In Proceedings of the 34th International Conference on Neural Information Pro- cessing Systems , Red Hook, NY , USA. Curran Asso- Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text',\n",
       "  'chunk_id': 22},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'Curran Asso- Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out , pages 74 81, Barcelona, Spain. Association for Computational Linguistics. Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica , 22(3):276 282. Yu-Min Tseng, Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen. 2023. DynamicESG: A Dataset for Dynamically Unearthing ESG Ratings from News Articles. In Proceedings of the 32nd ACM Interna- tional Conference on Information and Knowledge Management , pages 5412 5416. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024a. Multi- lingual E5 Text Embeddings: A Technical Report. arXiv preprint arXiv:2402.05672 . Shuting Wang, Jiongnan Liu, Shiren Song, Jiehan Cheng, Yuqi Fu, Peidong Guo, Kun Fang, Yu- tao Zhu, and Zhicheng Dou. 2024b. Domain- RAG: A Chinese Benchmark for Evaluating Domain- specific Retrieval-Augmented Generation. Preprint , Hao Yu, Aoran',\n",
       "  'chunk_id': 23},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'content': 'Benchmark for Evaluating Domain- specific Retrieval-Augmented Generation. Preprint , Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of Retrieval- Augmented Generation: A Survey. Preprint , We provide five ESG report exam- ples in this section, and please refer to our training set for more instances:',\n",
       "  'chunk_id': 24}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "GOOGLE_API_KEY = \"AIzaSyDLBN8OD_rJg93yVAyFP9mNKZQ0YX3IPL4\"  # You'll need to set this\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\",  # or \"text-embedding-004\" depending on access\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the metadata structure when creating the FAISS index\n",
    "texts = [doc[\"content\"] for doc in documents]\n",
    "metadata = [{\n",
    "    \"source\": f\"Paper_{doc['paper_id']}_chunk_{doc['chunk_id']}\", # Add source field\n",
    "    \"paper_id\": doc[\"paper_id\"],\n",
    "    \"title\": doc[\"title\"],\n",
    "    \"chunk_id\": doc[\"chunk_id\"]\n",
    "} for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vector Embeddings of the Chunks\n",
    "\n",
    "vectorstore = FAISS.from_texts(texts, embeddings, metadatas=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index for reuse\n",
    "vectorstore.save_local(\"data/RAG/embeddings/top5_papers_faiss_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining RAG components and implementing the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"filter\": lambda metadata: metadata[\"paper_id\"] == current_paper_id  # We'll set this dynamically\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the LLM using ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Create the RAG pipeline with the correct chain type\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"map_reduce\"  # or  if dealing with longer texts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a structured prompt for each paper\n",
    "\n",
    "def create_paper_prompt(question, title):\n",
    "    return f\"\"\"You are an expert at analysing research papers. \n",
    "    For the paper with the title {title}, please answer this query.\n",
    "    {question}.\n",
    "    Be specific to this paper only.\"\"\"\n",
    "\n",
    "# Define the base questions\n",
    "questions = [\n",
    "    \"Provide a comprehensive summary of the paper from the info that you have\",\n",
    "    \"what specific problem does this paper solve?\",\n",
    "    \"how does the paper solve this problem?\",\n",
    "    \"what are the next steps or future work suggested in this paper?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chunking, embedding and RAG pipeline for each paper separately\n",
    "\n",
    "results = {}\n",
    "for paper in top_5_rag_papers:\n",
    "    paper_id = paper[\"paper_id\"]\n",
    "    title = paper[\"title\"]\n",
    "    paper_text = paper[\"text\"]\n",
    "    results[paper_id] = {}\n",
    "    \n",
    "    # Create a temporary vector store just for this paper\n",
    "    temp_chunks = text_splitter.split_text(paper_text)\n",
    "    temp_metadata = [{\"source\": f\"Paper_{paper_id}_chunk_{i}\", \"paper_id\": paper_id, \"title\": title, \"chunk_id\": i} \n",
    "                    for i in range(len(temp_chunks))]\n",
    "    \n",
    "    temp_vectorstore = FAISS.from_texts(\n",
    "        temp_chunks, \n",
    "        embeddings, \n",
    "        metadatas=temp_metadata\n",
    "    )\n",
    "    \n",
    "    # Create a new retriever and chain for this paper\n",
    "    temp_retriever = temp_vectorstore.as_retriever(\n",
    "        search_type=\"similarity\", \n",
    "        search_kwargs={\"k\": 10}\n",
    "    )\n",
    "    \n",
    "    temp_qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=temp_retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type=\"map_reduce\"\n",
    "    )\n",
    "    \n",
    "    for question in questions:\n",
    "        # Create a specific prompt for this paper\n",
    "        specific_prompt = create_paper_prompt(question, title)\n",
    "        \n",
    "        # Query the RAG system\n",
    "        result = temp_qa_chain({\"question\": specific_prompt})\n",
    "        results[paper_id][question] = {\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"sources\": result[\"sources\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON results file\n",
    "with open('data/RAG/results/top_5_paper_analysis_results.json', 'r') as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'Provide a comprehensive summary of the paper from the info that you have': {'answer': 'The paper titled \"Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation\" explores the integration of multiple subsystems in dynamic environments using OpenAPI descriptions. The study focuses on the challenges of preprocessing API documentation to fit within the input token limitations of Large Language Models (LLMs) while maintaining relevant information. The authors propose the use of Retrieval-Augmented Generation (RAG) for endpoint discovery, employing various chunking strategies to optimize the retrieval process.\\n\\nThe paper introduces two main approaches: the OpenAPI RAG and the Discovery Agent. The OpenAPI RAG applies RAG for endpoint discovery using seven chunking strategies, validated through the RestBench benchmark. The Discovery Agent further enhances the process by breaking down queries into smaller tasks, improving precision and F1 scores while reducing recall. The study demonstrates that LLM-based and format-specific preprocessing methods outperform naive chunking strategies, and the use of an agent improves overall performance.\\n\\nThe research highlights the effectiveness of endpoint split-based chunking strategies and the need for further research to enhance retrieval performance. The paper provides open-source prototypes and results, emphasizing the potential of automated service composition using LLMs for integrating Information Systems.\\n\\n',\n",
       "   'sources': 'Paper_1_chunk_0, Paper_1_chunk_24, Paper_1_chunk_35, Paper_1_chunk_6, Paper_1_chunk_2, Paper_1_chunk_19, Paper_1_chunk_34, Paper_1_chunk_1'},\n",
       "  'what specific problem does this paper solve?': {'answer': 'The paper titled \"Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation\" addresses the problem of improving preprocessing methods for retrieval-augmented generation (RAG) by using LLM-based and format-specific approaches. It specifically focuses on how to optimally preprocess, or chunk, OpenAPIs for RAG endpoint discovery to reduce input token length while preserving relevant information. The paper also proposes the use of a Discovery Agent to further enhance performance metrics such as token count, precision, and F1 score by splitting tasks into finer subtasks and retrieving specification details on demand.\\n\\n',\n",
       "   'sources': 'Paper_1_chunk_0, Paper_1_chunk_2, Paper_1_chunk_4, Paper_1_chunk_1'},\n",
       "  'how does the paper solve this problem?': {'answer': 'The paper titled \"Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation\" addresses the problem by implementing an OpenAPI RAG (Retrieval-Augmented Generation) system that uses various chunking strategies to manage the integration of multiple subsystems. The system applies token-based and LLM-based chunking strategies to split service descriptions into manageable parts, such as endpoints, to fit within the input token limitations of Large Language Models (LLMs). The paper evaluates these strategies based on retrieval quality using the RestBench benchmark. The LLM-based strategies include generating summaries and queries for each OpenAPI endpoint to create embeddings, while the chunk content remains the original endpoint information. The Discovery Agent further enhances the system by breaking down queries into smaller tasks, retrieving relevant chunks, and improving precision and F1 scores, although it may reduce recall.\\n\\n',\n",
       "   'sources': 'Paper_1_chunk_0, Paper_1_chunk_35, Paper_1_chunk_24, Paper_1_chunk_19, Paper_1_chunk_6, Paper_1_chunk_2, Paper_1_chunk_4, Paper_1_chunk_23'},\n",
       "  'what are the next steps or future work suggested in this paper?': {'answer': 'The paper suggests several next steps or future work, including:\\n\\n1. Further research to improve retrieval performance and address the decline in recall due to processing through the LLM (Large Language Model).\\n2. Evaluation in larger real-world scenarios remains open for future research.\\n3. Exploring the automatic generation of service documentation using an LLM.\\n4. Creating a custom chunking strategy for specific kinds of service documentation.\\n5. Further research is necessary to retrieve all requisite endpoints.\\n\\n',\n",
       "   'sources': 'Paper_1_chunk_35, Paper_1_chunk_37, Paper_1_chunk_36, Paper_1_chunk_2'}},\n",
       " '66': {'Provide a comprehensive summary of the paper from the info that you have': {'answer': 'The paper titled \"ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet?\" investigates the challenges and solutions related to the inaccuracies of ChatGPT when processing technical reports, specifically software bug reports. These reports often contain a mix of descriptive text, technical terminology, and code snippets, which can be complex for AI models to interpret accurately. The study identifies two main limitations of ChatGPT: its difficulty in understanding complex technical content like stack traces and its challenges in integrating context from technical terms and texts.\\n\\nTo address these issues, the authors developed CHIME (ChatGPT Inaccuracy Mitigation Engine), which preprocesses technical reports and guides the query validation process in ChatGPT. CHIME uses context-free grammar to parse stack traces and applies metamorphic testing and query transformation to verify and correct ChatGPT\\'s responses. The study found that CHIME improved the accuracy of ChatGPT\\'s responses by 30.3% over the baseline. The paper highlights the importance of preprocessing and guided prompting to mitigate hallucinations and improve the reliability of AI models in technical domains.\\n\\n',\n",
       "   'sources': 'Paper_66_chunk_0, Paper_66_chunk_1, Paper_66_chunk_2, Paper_66_chunk_4, Paper_66_chunk_5'},\n",
       "  'what specific problem does this paper solve?': {'answer': 'The paper \"ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet?\" addresses the problem of inaccuracies, specifically hallucinations, in ChatGPT\\'s responses when processing technical reports. The paper identifies two main limitations of ChatGPT: 1) difficulty in understanding complex technical content, such as code snippets and stack traces, and 2) challenges in integrating contexts denoted by technical terms. To solve these issues, the paper introduces CHIME (ChatGPT Inaccuracy Mitigation Engine), which preprocesses technical reports and guides the query validation process to improve the accuracy of ChatGPT\\'s responses. CHIME uses context-free grammar to parse stack traces and applies metamorphic testing and query transformation to verify and correct ChatGPT\\'s outputs, resulting in a 30.3% improvement in accuracy over standard ChatGPT responses.\\n\\n',\n",
       "   'sources': 'Paper_66_chunk_0, Paper_66_chunk_1, Paper_66_chunk_4, Paper_66_chunk_5'},\n",
       "  'how does the paper solve this problem?': {'answer': 'The paper \"ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet?\" addresses the problem of inaccuracies in ChatGPT\\'s understanding of technical reports by introducing CHIME (ChatGPT Inaccuracy Mitigation Engine). CHIME uses context-free grammar (CFG) to parse stack traces in technical reports and applies metamorphic testing and query transformation to verify and correct ChatGPT responses. The approach involves preprocessing technical reports to store relevant information as metadata and guiding the query validation process with iterative prompting. This method shows a 30.3% improvement in correction over standard ChatGPT responses.\\n\\n',\n",
       "   'sources': 'Paper_66_chunk_0, Paper_66_chunk_1, Paper_66_chunk_5'},\n",
       "  'what are the next steps or future work suggested in this paper?': {'answer': \"In the future, the paper suggests focusing on expanding CHIME's grasp of more technical terminologies and other documents. This includes improving the issue preprocessor module to separate code and textual contents, adapting the CFG to handle code snippets/traces/crash dumps, and using a static partial program analyzer to handle code examples in API documentation. The expectation is that other modules in CHIME can be used with minimal changes.\\n\\n\",\n",
       "   'sources': 'Paper_66_chunk_61'}},\n",
       " '77': {'Provide a comprehensive summary of the paper from the info that you have': {'answer': 'The paper titled \"Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving\" presents an innovative approach to solving mathematical problems in Bengali, specifically developed for the DL Sprint 3.0 - BUET CSE Fest 2024 Competition. The methodology utilizes advanced deep learning models from the Qwen 2.5 series, with enhancements through prompt engineering, model quantization, and Tool Integrated Reasoning (TIR) to manage complex calculations. The research involved exploring various model architectures, including fine-tuned Mistral and quantized Qwen models, and refining them through translation techniques, Retrieval-Augmented Generation (RAG), and custom dataset curation. Manual hyperparameter tuning was employed to optimize parameters like temperature and top-p, improving model adaptability and response accuracy.\\n\\nThe Qwen 2.5-32B model, combined with techniques such as self-consistency and TIR, showed significant improvements over baseline models, demonstrating its potential in handling complex reasoning tasks with minimal Bengali-specific data. However, challenges such as translation dependencies, limited Bengali datasets, and memory constraints for large models were noted, impacting model performance and adaptability. The paper highlights the sensitivity of model outputs to prompt phrasing, emphasizing the need for refined prompt engineering.\\n\\nThe research also involved translating Bengali mathematical questions into English to enhance processing, with the Qwen-32B-Instruct model showing superior performance in translation and problem-solving compared to smaller models. Despite the achievements, the paper acknowledges the limitations due to Kaggle\\'s limited GPU memory, which restricted fine-tuning of larger models like Qwen 2.5-32B.\\n\\n',\n",
       "   'sources': 'Paper_77_chunk_0, Paper_77_chunk_1, Paper_77_chunk_14, Paper_77_chunk_6, Paper_77_chunk_13, Paper_77_chunk_5, Paper_77_chunk_11, Paper_77_chunk_4, Paper_77_chunk_12'},\n",
       "  'what specific problem does this paper solve?': {'answer': 'The paper \"Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving\" addresses the specific problem of solving mathematical problems in Bengali, particularly those akin to the Bengali Math Olympiad. It tackles the challenge of mathematical reasoning in low-resource languages like Bengali by leveraging the Qwen 2.5 series models, optimizing through prompt engineering, translation, and Tool Integrated Reasoning (TIR). The approach includes translating Bengali mathematical questions into English to improve processing and using advanced NLP techniques to enhance problem-solving capabilities in this context.\\n\\n',\n",
       "   'sources': 'Paper_77_chunk_0, Paper_77_chunk_1, Paper_77_chunk_2, Paper_77_chunk_14'},\n",
       "  'how does the paper solve this problem?': {'answer': 'The paper \"Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving\" addresses the problem by utilizing the Qwen 2.5 series models, particularly the Qwen-32B-Instruct model, to solve mathematical problems in Bengali. The approach involves several key techniques:\\n\\n1. **Translation**: Bengali mathematical questions are translated into English using the Qwen-32B-Instruct model to improve processing and understanding, leveraging the model\\'s richer pre-trained capabilities.\\n\\n2. **Prompt Engineering**: The paper employs prompt tuning to optimize model performance, experimenting with various reasoning techniques such as Chain of Thought (COT), Tool Integrated Reasoning (TIR), and self-consistent versions of these techniques. Prompts are designed to guide the model through step-by-step reasoning and to perform calculations using Python tools for complex problems.\\n\\n3. **Tool Integrated Reasoning (TIR)**: This involves adapting prompts to instruct the model to use external tools for calculations, enhancing its ability to handle complex mathematical problems.\\n\\n4. **Self-Consistency**: The model generates multiple reasoning paths and selects the most consistent solution, improving accuracy and robustness.\\n\\n5. **Model Quantization and Hyperparameter Tuning**: These techniques are used to optimize the model\\'s adaptability and response accuracy, with manual tuning of parameters like temperature and top-p.\\n\\n6. **Removal of RAG**: The paper found that removing Retrieval-Augmented Generation (RAG) improved scores, indicating that the inherent capabilities of the larger Qwen models were sufficient without additional retrieved context.\\n\\nThese methods demonstrate the efficacy of advanced NLP techniques in mathematical problem-solving, particularly for low-resource languages like Bengali, despite challenges such as translation dependencies and limited datasets.\\n\\n',\n",
       "   'sources': 'Paper_77_chunk_0, Paper_77_chunk_1, Paper_77_chunk_6, Paper_77_chunk_11, Paper_77_chunk_14'},\n",
       "  'what are the next steps or future work suggested in this paper?': {'answer': 'The next steps or future work suggested in the paper include increasing the availability and diversity of Bengali mathematical datasets, domain-specific fine-tuning of the Qwen 2.5-32B model on Bengali math problems, optimizing prompt engineering for mathematical reasoning tasks in low-resource languages, and exploring lightweight models tailored for Bengali to balance memory constraints and inference speed.\\n\\n',\n",
       "   'sources': 'Paper_77_chunk_15'}},\n",
       " '80': {'Provide a comprehensive summary of the paper from the info that you have': {'answer': 'The paper titled \"Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework\" discusses the development of an autonomous intelligent system using Large Language Models (LLMs) for diagnosing and troubleshooting AI clusters. The system leverages recent advancements in LLMs, Retrieval-Augmented Generation (RAG), and Diagram of Thought (DoT) technologies, integrating them with self-play methodologies to create an LLM-agent capable of autonomously diagnosing and resolving cluster issues. Key innovations include a knowledge base tailored for cluster diagnostics, enhanced LLM algorithms, and practical deployment strategies for agents. The paper also introduces a benchmark with 150 manually crafted questions to evaluate the LLM-agent\\'s capabilities, demonstrating its efficiency in identifying faults faster than human experts. The research aims to enhance cluster availability by detecting and rectifying performance issues more efficiently than traditional methods, although it acknowledges the need for further advancements in communication abilities and quantitative analysis.\\n\\n',\n",
       "   'sources': 'Paper_80_chunk_0, Paper_80_chunk_2, Paper_80_chunk_45, Paper_80_chunk_48'},\n",
       "  'what specific problem does this paper solve?': {'answer': 'The paper titled \"Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework\" addresses the problem of cluster troubleshooting by proposing a system that utilizes LLM-agents to autonomously and intelligently diagnose and resolve issues within AI clusters. This system enhances cluster resilience by preemptively addressing potential faults, thereby freeing up engineering resources to focus on more complex tasks. It is capable of detecting task failures, identifying points of failure, clarifying reasons for failure, and repairing corresponding faults, ensuring the overall availability of the AI cluster. The system demonstrates efficiency and accuracy in diagnosing and rectifying performance issues more effectively than traditional methods.\\n\\n',\n",
       "   'sources': 'Paper_80_chunk_4, Paper_80_chunk_45, Paper_80_chunk_0, Paper_80_chunk_1, Paper_80_chunk_48'},\n",
       "  'how does the paper solve this problem?': {'answer': 'The paper \"Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework\" addresses the problem by developing an LLM-agent system that autonomously diagnoses and resolves issues within AI clusters. This system leverages recent advancements in Large Language Models (LLMs) and related technologies such as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT). The LLM-agent can autonomously execute remediation measures, freeing up engineering resources for more complex tasks. It accurately identifies faults, such as a GPU with low core frequency, before they are detected by engineers, and automatically completes repairs, enhancing the overall availability of the cluster. The system includes a specialized knowledge base for cluster diagnostics, enhanced LLM algorithms, practical deployment strategies, and a benchmark for evaluating LLM capabilities in this domain. Through extensive experimentation, the system has demonstrated superiority in detecting and rectifying performance issues more efficiently and accurately than traditional methods.\\n\\n',\n",
       "   'sources': 'Paper_80_chunk_0, Paper_80_chunk_1, Paper_80_chunk_2, Paper_80_chunk_4, Paper_80_chunk_12, Paper_80_chunk_45, Paper_80_chunk_48'},\n",
       "  'what are the next steps or future work suggested in this paper?': {'answer': 'The paper suggests that future work will focus on continuously monitoring the development of smaller models and exploring the possibility of teaching the capabilities of the LLM-agent to these smaller models in the form of DoT when appropriate.\\n\\n',\n",
       "   'sources': 'Paper_80_chunk_51'}},\n",
       " '87': {'Provide a comprehensive summary of the paper from the info that you have': {'answer': 'The paper titled \"ML-Promise: A Multilingual Dataset for Corporate Promise Verification\" introduces the concept of Promise Verification, a novel task aimed at evaluating the credibility and fulfillment of promises made by corporations, particularly in the context of ESG (Environmental, Social, and Governance) reports. The authors propose the first multilingual dataset, ML-Promise, which includes languages such as English, French, Chinese, Japanese, and Korean, to facilitate in-depth verification of corporate promises. This dataset addresses challenges like greenwashing by evaluating corporate environmental and social promises.\\n\\nThe study highlights that around 35-40% of the evidence is unclear in supporting the associated promises in four out of five languages, emphasizing the necessity of the proposed task. Additionally, about 4% of instances contain potentially misleading evidence in the English and Japanese datasets. The authors explore textual and image-based baselines, with promising results from Retrieval-Augmented Generation (RAG) approaches, although RAG does not consistently outperform baseline models across all languages and tasks.\\n\\nThe paper also discusses the limitations of the dataset, noting its focus on specific regions and the need for future studies to expand its scope to include more regions and languages. The authors plan to release their annotations under the CCBY-NC-SA 4.0 license, hoping their work will contribute to greater accountability in corporate and public disclosures.\\n\\n',\n",
       "   'sources': 'Paper_87_chunk_0, Paper_87_chunk_1, Paper_87_chunk_3, Paper_87_chunk_5, Paper_87_chunk_8, Paper_87_chunk_10, Paper_87_chunk_18, Paper_87_chunk_19'},\n",
       "  'what specific problem does this paper solve?': {'answer': 'The paper \"ML-Promise: A Multilingual Dataset for Corporate Promise Verification\" addresses the problem of evaluating the credibility and fulfillment of promises made by corporations, particularly in the context of Environmental, Social, and Governance (ESG) reports. It introduces the concept of Promise Verification and proposes the first multilingual dataset, ML-Promise, to facilitate the verification of corporate promises and to address challenges such as greenwashing. The dataset includes languages such as English, French, Chinese, Japanese, and Korean, and focuses on ESG reports from corporations in the U.K., France, Taiwan, Japan, and Korea.\\n\\n',\n",
       "   'sources': 'Paper_87_chunk_0, Paper_87_chunk_18, Paper_87_chunk_1, Paper_87_chunk_5'},\n",
       "  'how does the paper solve this problem?': {'answer': 'The paper \"ML-Promise: A Multilingual Dataset for Corporate Promise Verification\" addresses the problem of verifying corporate promises, particularly in the context of Environmental, Social, and Governance (ESG) reports, by introducing a novel task called Promise Verification. This involves evaluating the credibility and fulfillment of promises made by corporations. The paper proposes the first multilingual dataset, ML-Promise, which includes languages such as English, French, Chinese, Japanese, and Korean, to facilitate in-depth verification of these promises. The dataset is designed to address challenges like greenwashing by evaluating corporate promises in ESG reports from companies in the UK, France, Taiwan, Japan, and Korea. The task involves several steps: Promise Identification, Supporting Evidence, and Clarity of the Promise-Evidence Pair. The study also explores textual and image-based baselines, with promising results from retrieval-augmented generation (RAG) approaches, although RAG does not consistently outperform baseline models across all languages and tasks, indicating a need for further optimization.\\n\\n',\n",
       "   'sources': 'Paper_87_chunk_0, Paper_87_chunk_1, Paper_87_chunk_8, Paper_87_chunk_18'},\n",
       "  'what are the next steps or future work suggested in this paper?': {'answer': 'Future studies can follow the design of the paper to expand the dataset to include more regions and languages, which could enhance the robustness and applicability of the proposed methods.\\n\\n',\n",
       "   'sources': 'Paper_87_chunk_19'}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from data/RAG/text/rag_papers.json\n"
     ]
    }
   ],
   "source": [
    "# Load fetched paper data from JSON \n",
    "\n",
    "rag_papers = load_data(file_path=\"data/RAG/text/rag_papers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_id': 1,\n",
       "  'title': 'Advanced System Integration: Analyzing OpenAPI Chunking for\\n  Retrieval-Augmented Generation',\n",
       "  'summary': 'Integrating multiple (sub-)systems is essential to create advanced\\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\\nenvironments across the IS lifecycle. A traditional approach is a registry that\\nprovides the API documentation of the systems\\' endpoints. Large Language Models\\n(LLMs) have shown to be capable of automatically creating system integrations\\n(e.g., as service composition) based on this documentation but require concise\\ninput due to input token limitations, especially regarding comprehensive API\\ndescriptions. Currently, it is unknown how best to preprocess these API\\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\\nof OpenAPIs to reduce the input token length while preserving the most relevant\\ninformation. To further reduce the input token length for the composition\\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\\nonly receives a summary of the most relevant endpoints and retrieves details on\\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\\nfirst, for the different chunking possibilities and parameters measuring the\\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\\nto successfully employ RAG for endpoint discovery to reduce the token count.\\nWhile revealing high values for recall, precision, and F1, further research is\\nnecessary to retrieve all requisite endpoints. Our experiments show that for\\npreprocessing, LLM-based and format-specific approaches outperform na\\\\\"ive\\nchunking methods. Relying on an agent further enhances these results as the\\nagent splits the tasks into multiple fine granular subtasks, improving the\\noverall RAG performance in the token count, precision, and F1 score.',\n",
       "  'published': '2024-11-29T16:09:43Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.19804v1',\n",
       "  'authors': ['Robin D. Pesl',\n",
       "   'Jerin G. Mathew',\n",
       "   'Massimo Mecella',\n",
       "   'Marco Aiello'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.19804v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0001_Advanced_System_Integration_Analyzing_OpenAPI_Chunking_for_Retrieval-Augmented_Generation.pdf'},\n",
       " {'paper_id': 2,\n",
       "  'title': 'CantorNet: A Sandbox for Testing Topological and Geometrical Measures',\n",
       "  'summary': \"Many natural phenomena are characterized by self-similarity, for example the\\nsymmetry of human faces, or a repetitive motif of a song. Studying of such\\nsymmetries will allow us to gain deeper insights into the underlying mechanisms\\nof complex systems. Recognizing the importance of understanding these patterns,\\nwe propose a geometrically inspired framework to study such phenomena in\\nartificial neural networks. To this end, we introduce \\\\emph{CantorNet},\\ninspired by the triadic construction of the Cantor set, which was introduced by\\nGeorg Cantor in the $19^\\\\text{th}$ century. In mathematics, the Cantor set is a\\nset of points lying on a single line that is self-similar and has a counter\\nintuitive property of being an uncountably infinite null set. Similarly, we\\nintroduce CantorNet as a sandbox for studying self-similarity by means of novel\\ntopological and geometrical complexity measures. CantorNet constitutes a family\\nof ReLU neural networks that spans the whole spectrum of possible Kolmogorov\\ncomplexities, including the two opposite descriptions (linear and exponential\\nas measured by the description length). CantorNet's decision boundaries can be\\narbitrarily ragged, yet are analytically known. Besides serving as a testing\\nground for complexity measures, our work may serve to illustrate potential\\npitfalls in geometry-ignorant data augmentation techniques and adversarial\\nattacks.\",\n",
       "  'published': '2024-11-29T14:01:34Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.19713v1',\n",
       "  'authors': ['Michal Lewandowski', 'Hamid Eghbalzadeh', 'Bernhard A. Moser'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.19713v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0002_CantorNet_A_Sandbox_for_Testing_Topological_and_Geometrical_Measures.pdf'},\n",
       " {'paper_id': 3,\n",
       "  'title': 'Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating\\n  RAG Systems',\n",
       "  'summary': \"Retrieval Augmented Generation (RAG) systems are a widespread application of\\nLarge Language Models (LLMs) in the industry. While many tools exist empowering\\ndevelopers to build their own systems, measuring their performance locally,\\nwith datasets reflective of the system's use cases, is a technological\\nchallenge. Solutions to this problem range from non-specific and cheap (most\\npublic datasets) to specific and costly (generating data from local documents).\\nIn this paper, we show that using public question and answer (Q&A) datasets to\\nassess retrieval performance can lead to non-optimal systems design, and that\\ncommon tools for RAG dataset generation can lead to unbalanced data. We propose\\nsolutions to these issues based on the characterization of RAG datasets through\\nlabels and through label-targeted data generation. Finally, we show that\\nfine-tuned small LLMs can efficiently generate Q&A datasets. We believe that\\nthese observations are invaluable to the know-your-data step of RAG systems\\ndevelopment.\",\n",
       "  'published': '2024-11-29T13:57:07Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.19710v1',\n",
       "  'authors': ['Rafael Teixeira de Lima',\n",
       "   'Shubham Gupta',\n",
       "   'Cesar Berrospi',\n",
       "   'Lokesh Mishra',\n",
       "   'Michele Dolfi',\n",
       "   'Peter Staar',\n",
       "   'Panagiotis Vagenas'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.19710v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0003_Know_Your_RAG_Dataset_Taxonomy_and_Generation_Strategies_for_Evaluating_RAG_Systems.pdf'},\n",
       " {'paper_id': 4,\n",
       "  'title': 'Unimib Assistant: designing a student-friendly RAG-based chatbot for all\\n  their needs',\n",
       "  'summary': \"Natural language processing skills of Large Language Models (LLMs) are\\nunprecedented, having wide diffusion and application in different tasks. This\\npilot study focuses on specializing ChatGPT behavior through a\\nRetrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs\\nfeature. The purpose of our chatbot, called Unimib Assistant, is to provide\\ninformation and solutions to the specific needs of University of Milano-Bicocca\\n(Unimib) students through a question-answering approach. We provided the system\\nwith a prompt highlighting its specific purpose and behavior, as well as\\nuniversity-related documents and links obtained from an initial need-finding\\nphase, interviewing six students. After a preliminary customization phase, a\\nqualitative usability test was conducted with six other students to identify\\nthe strengths and weaknesses of the chatbot, with the goal of improving it in a\\nsubsequent redesign phase. While the chatbot was appreciated for its\\nuser-friendly experience, perceived general reliability, well-structured\\nresponses, and conversational tone, several significant technical and\\nfunctional limitations emerged. In particular, the satisfaction and overall\\nexperience of the users was impaired by the system's inability to always\\nprovide fully accurate information. Moreover, it would often neglect to report\\nrelevant information even if present in the materials uploaded and prompt\\ngiven. Furthermore, it sometimes generated unclickable links, undermining its\\ntrustworthiness, since providing the source of information was an important\\naspect for our users. Further in-depth studies and feedback from other users as\\nwell as implementation iterations are planned to refine our Unimib Assistant.\",\n",
       "  'published': '2024-11-29T09:07:21Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.19554v1',\n",
       "  'authors': ['Chiara Antico',\n",
       "   'Stefano Giordano',\n",
       "   'Cansu Koyuturk',\n",
       "   'Dimitri Ognibene'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.19554v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0004_Unimib_Assistant_designing_a_student-friendly_RAG-based_chatbot_for_all_their_needs.pdf'},\n",
       " {'paper_id': 5,\n",
       "  'title': 'Knowledge Management for Automobile Failure Analysis Using Graph RAG',\n",
       "  'summary': 'This paper presents a knowledge management system for automobile failure\\nanalysis using retrieval-augmented generation (RAG) with large language models\\n(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a\\ngrowing demand for knowledge transfer of failure analysis from experienced\\nengineers to young engineers. However, failure events are phenomena that occur\\nin a chain reaction, making them difficult for beginners to analyze them. While\\nknowledge graphs, which can describe semantic relationships and structure\\ninformation is effective in representing failure events, due to their\\ncapability of representing the relationships between components, there is much\\ninformation in KGs, so it is challenging for young engineers to extract and\\nunderstand sub-graphs from the KG. On the other hand, there is increasing\\ninterest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for\\nknowledge management. However, when using the current Graph RAG framework with\\nan existing knowledge graph for automobile failures, several issues arise\\nbecause it is difficult to generate executable queries for a knowledge graph\\ndatabase which is not constructed by LLMs. To address this, we focused on\\noptimizing the Graph RAG pipeline for existing knowledge graphs. Using an\\noriginal Q&A dataset, the ROUGE F1 score of the sentences generated by the\\nproposed method showed an average improvement of 157.6% compared to the current\\nmethod. This highlights the effectiveness of the proposed method for automobile\\nfailure analysis.',\n",
       "  'published': '2024-11-29T08:34:07Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.19539v1',\n",
       "  'authors': ['Yuta Ojima',\n",
       "   'Hiroki Sakaji',\n",
       "   'Tadashi Nakamura',\n",
       "   'Hiroaki Sakata',\n",
       "   'Kazuya Seki',\n",
       "   'Yuu Teshigawara',\n",
       "   'Masami Yamashita',\n",
       "   'Kazuhiro Aoyama'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.19539v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0005_Knowledge_Management_for_Automobile_Failure_Analysis_Using_Graph_RAG.pdf'},\n",
       " {'paper_id': 6,\n",
       "  'title': 'RAGDiffusion: Faithful Cloth Generation via External Knowledge\\n  Assimilation',\n",
       "  'summary': 'Standard clothing asset generation involves creating forward-facing flat-lay\\ngarment images displayed on a clear background by extracting clothing\\ninformation from diverse real-world contexts, which presents significant\\nchallenges due to highly standardized sampling distributions and precise\\nstructural requirements in the generated images. Existing models have limited\\nspatial perception and often exhibit structural hallucinations in this\\nhigh-specification generative task. To address this issue, we propose a novel\\nRetrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance\\nstructure determinacy and mitigate hallucinations by assimilating external\\nknowledge from LLM and databases. RAGDiffusion consists of two core processes:\\n(1) Retrieval-based structure aggregation, which employs contrastive learning\\nand a Structure Locally Linear Embedding (SLLE) to derive global structure and\\nspatial landmarks, providing both soft and hard guidance to counteract\\nstructural ambiguities; and (2) Omni-level faithful garment generation, which\\nintroduces a three-level alignment that ensures fidelity in structural,\\npattern, and decoding components within the diffusing. Extensive experiments on\\nchallenging real-world datasets demonstrate that RAGDiffusion synthesizes\\nstructurally and detail-faithful clothing assets with significant performance\\nimprovements, representing a pioneering effort in high-specification faithful\\ngeneration with RAG to confront intrinsic hallucinations and enhance fidelity.',\n",
       "  'published': '2024-11-29T07:57:32Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.19528v1',\n",
       "  'authors': ['Xianfeng Tan',\n",
       "   'Yuhan Li',\n",
       "   'Wenxiang Shang',\n",
       "   'Yubo Wu',\n",
       "   'Jian Wang',\n",
       "   'Xuanhong Chen',\n",
       "   'Yi Zhang',\n",
       "   'Ran Lin',\n",
       "   'Bingbing Ni'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.19528v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0006_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation.pdf'},\n",
       " {'paper_id': 7,\n",
       "  'title': 'Towards Understanding Retrieval Accuracy and Prompt Quality in RAG\\n  Systems',\n",
       "  'summary': \"Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the\\ncapability of large language models (LLMs) and has demonstrated promising\\nefficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show\\nsuperior performance, they face unique challenges in stability and reliability.\\nTheir complexity hinders developers' efforts to design, maintain, and optimize\\neffective RAG systems. Therefore, it is crucial to understand how RAG's\\nperformance is impacted by its design. In this work, we conduct an early\\nexploratory study toward a better understanding of the mechanism of RAG\\nsystems, covering three code datasets, three QA datasets, and two LLMs. We\\nfocus on four design factors: retrieval document type, retrieval recall,\\ndocument selection, and prompt techniques. Our study uncovers how each factor\\nimpacts system correctness and confidence, providing valuable insights for\\ndeveloping an accurate and reliable RAG system. Based on these findings, we\\npresent nine actionable guidelines for detecting defects and optimizing the\\nperformance of RAG systems. We hope our early exploration can inspire further\\nadvancements in engineering, improving and maintaining LLM-driven intelligent\\nsoftware systems for greater efficiency and reliability.\",\n",
       "  'published': '2024-11-29T04:25:31Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.19463v1',\n",
       "  'authors': ['Shengming Zhao',\n",
       "   'Yuheng Huang',\n",
       "   'Jiayang Song',\n",
       "   'Zhijie Wang',\n",
       "   'Chengcheng Wan',\n",
       "   'Lei Ma'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.19463v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0007_Towards_Understanding_Retrieval_Accuracy_and_Prompt_Quality_in_RAG_Systems.pdf'},\n",
       " {'paper_id': 8,\n",
       "  'title': 'Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language\\n  Models',\n",
       "  'summary': \"Iterative retrieval refers to the process in which the model continuously\\nqueries the retriever during generation to enhance the relevance of the\\nretrieved knowledge, thereby improving the performance of Retrieval-Augmented\\nGeneration (RAG). Existing work typically employs few-shot prompting or\\nmanually constructed rules to implement iterative retrieval. This introduces\\nadditional inference overhead and overlooks the remarkable reasoning\\ncapabilities of Large Language Models (LLMs). In this paper, we introduce\\nAuto-RAG, an autonomous iterative retrieval model centered on the LLM's\\npowerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues\\nwith the retriever, systematically planning retrievals and refining queries to\\nacquire valuable knowledge. This process continues until sufficient external\\ninformation is gathered, at which point the results are presented to the user.\\nTo this end, we develop a method for autonomously synthesizing reasoning-based\\ndecision-making instructions in iterative retrieval and fine-tuned the latest\\nopen-source LLMs. The experimental results indicate that Auto-RAG is capable of\\nautonomous iterative interaction with the retriever, effectively leveraging the\\nremarkable reasoning and decision-making abilities of LLMs, which lead to\\noutstanding performance across six benchmarks. Further analysis reveals that\\nAuto-RAG can autonomously adjust the number of iterations based on the\\ndifficulty of the questions and the utility of the retrieved knowledge, without\\nrequiring any human intervention. Moreover, Auto-RAG expresses the iterative\\nretrieval process in natural language, enhancing interpretability while\\nproviding users with a more intuitive experience\\\\footnote{Code is available at\\n\\\\url{https://github.com/ictnlp/Auto-RAG}.\",\n",
       "  'published': '2024-11-29T03:01:05Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.19443v1',\n",
       "  'authors': ['Tian Yu', 'Shaolei Zhang', 'Yang Feng'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.19443v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0008_Auto-RAG_Autonomous_Retrieval-Augmented_Generation_for_Large_Language_Models.pdf'},\n",
       " {'paper_id': 9,\n",
       "  'title': 'Habit Coach: Customising RAG-based chatbots to support behavior change',\n",
       "  'summary': \"This paper presents the iterative development of Habit Coach, a GPT-based\\nchatbot designed to support users in habit change through personalized\\ninteraction. Employing a user-centered design approach, we developed the\\nchatbot using a Retrieval-Augmented Generation (RAG) system, which enables\\nbehavior personalization without retraining the underlying language model\\n(GPT-4). The system leverages document retrieval and specialized prompts to\\ntailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and\\nnarrative therapy techniques. A key challenge in the development process was\\nthe difficulty of translating declarative knowledge into effective interaction\\nbehaviors. In the initial phase, the chatbot was provided with declarative\\nknowledge about CBT via reference textbooks and high-level conversational\\ngoals. However, this approach resulted in imprecise and inefficient behavior,\\nas the GPT model struggled to convert static information into dynamic and\\ncontextually appropriate interactions. This highlighted the limitations of\\nrelying solely on declarative knowledge to guide chatbot behavior, particularly\\nin nuanced, therapeutic conversations. Over four iterations, we addressed this\\nissue by gradually transitioning towards procedural knowledge, refining the\\nchatbot's interaction strategies, and improving its overall effectiveness. In\\nthe final evaluation, 5 participants engaged with the chatbot over five\\nconsecutive days, receiving individualized CBT interventions. The Self-Report\\nHabit Index (SRHI) was used to measure habit strength before and after the\\nintervention, revealing a reduction in habit strength post-intervention. These\\nresults underscore the importance of procedural knowledge in driving effective,\\npersonalized behavior change support in RAG-based systems.\",\n",
       "  'published': '2024-11-28T15:53:27Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.19229v1',\n",
       "  'authors': ['Arian Fooroogh Mand Arabi',\n",
       "   'Cansu Koyuturk',\n",
       "   \"Michael O'Mahony\",\n",
       "   'Raffaella Calati',\n",
       "   'Dimitri Ognibene'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.19229v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0009_Habit_Coach_Customising_RAG-based_chatbots_to_support_behavior_change.pdf'},\n",
       " {'paper_id': 10,\n",
       "  'title': 'Knowledge Database or Poison Base? Detecting RAG Poisoning Attack\\n  through LLM Activations',\n",
       "  'summary': \"As Large Language Models (LLMs) are progressively deployed across diverse\\nfields and real-world applications, ensuring the security and robustness of\\nLLMs has become ever more critical. Retrieval-Augmented Generation (RAG) is a\\ncutting-edge approach designed to address the limitations of large language\\nmodels (LLMs). By retrieving information from the relevant knowledge database,\\nRAG enriches the input to LLMs, enabling them to produce responses that are\\nmore accurate and contextually appropriate. It is worth noting that the\\nknowledge database, being sourced from publicly available channels such as\\nWikipedia, inevitably introduces a new attack surface. RAG poisoning involves\\ninjecting malicious texts into the knowledge database, ultimately leading to\\nthe generation of the attacker's target response (also called poisoned\\nresponse). However, there are currently limited methods available for detecting\\nsuch poisoning attacks. We aim to bridge the gap in this work. Particularly, we\\nintroduce RevPRAG, a flexible and automated detection pipeline that leverages\\nthe activations of LLMs for poisoned response detection. Our investigation\\nuncovers distinct patterns in LLMs' activations when generating correct\\nresponses versus poisoned responses. Our results on multiple benchmark datasets\\nand RAG architectures show our approach could achieve 98% true positive rate,\\nwhile maintaining false positive rates close to 1%. We also evaluate recent\\nbackdoor detection methods specifically designed for LLMs and applicable for\\nidentifying poisoned responses in RAG. The results demonstrate that our\\napproach significantly surpasses them.\",\n",
       "  'published': '2024-11-28T06:29:46Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.18948v1',\n",
       "  'authors': ['Xue Tan',\n",
       "   'Hao Luan',\n",
       "   'Mingyu Luo',\n",
       "   'Xiaoyan Sun',\n",
       "   'Ping Chen',\n",
       "   'Jun Dai'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.18948v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0010_Knowledge_Database_or_Poison_Base_Detecting_RAG_Poisoning_Attack_through_LLM_Activations.pdf'},\n",
       " {'paper_id': 11,\n",
       "  'title': 'ICLERB: In-Context Learning Embedding and Reranker Benchmark',\n",
       "  'summary': \"In-Context Learning (ICL) enables Large Language Models (LLMs) to perform new\\ntasks by conditioning on prompts with relevant information. Retrieval-Augmented\\nGeneration (RAG) enhances ICL by incorporating retrieved documents into the\\nLLM's context at query time. However, traditional retrieval methods focus on\\nsemantic relevance, treating retrieval as a search problem. In this paper, we\\npropose reframing retrieval for ICL as a recommendation problem, aiming to\\nselect documents that maximize utility in ICL tasks. We introduce the\\nIn-Context Learning Embedding and Reranker Benchmark (ICLERB), a novel\\nevaluation framework that compares retrievers based on their ability to enhance\\nLLM accuracy in ICL settings. Additionally, we propose a novel Reinforcement\\nLearning-to-Rank from AI Feedback (RLRAIF) algorithm, designed to fine-tune\\nretrieval models using minimal feedback from the LLM. Our experimental results\\nreveal notable differences between ICLERB and existing benchmarks, and\\ndemonstrate that small models fine-tuned with our RLRAIF algorithm outperform\\nlarge state-of-the-art retrieval models. These findings highlight the\\nlimitations of existing evaluation methods and the need for specialized\\nbenchmarks and training strategies adapted to ICL.\",\n",
       "  'published': '2024-11-28T06:28:45Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.18947v1',\n",
       "  'authors': ['Marie Al Ghossein', 'Emile Contal', 'Alexandre Robicquet'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.18947v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0011_ICLERB_In-Context_Learning_Embedding_and_Reranker_Benchmark.pdf'},\n",
       " {'paper_id': 12,\n",
       "  'title': 'Automated Literature Review Using NLP Techniques and LLM-Based\\n  Retrieval-Augmented Generation',\n",
       "  'summary': 'This research presents and compares multiple approaches to automate the\\ngeneration of literature reviews using several Natural Language Processing\\n(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language\\nModel (LLM). The ever-increasing number of research articles provides a huge\\nchallenge for manual literature review. It has resulted in an increased demand\\nfor automation. Developing a system capable of automatically generating the\\nliterature reviews from only the PDF files as input is the primary objective of\\nthis research work. The effectiveness of several Natural Language Processing\\n(NLP) strategies, such as the frequency-based method (spaCy), the transformer\\nmodel (Simple T5), and retrieval-augmented generation (RAG) with Large Language\\nModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR\\ndataset is chosen for this research experiment and three distinct techniques\\nare utilized to implement three different systems for auto-generating the\\nliterature reviews. The ROUGE scores are used for the evaluation of all three\\nsystems. Based on the evaluation, the Large Language Model GPT-3.5-turbo\\nachieved the highest ROUGE-1 score, 0.364. The transformer model comes in\\nsecond place and spaCy is at the last position. Finally, a graphical user\\ninterface is created for the best system based on the large language model.',\n",
       "  'published': '2024-11-27T18:27:07Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.18583v1',\n",
       "  'authors': ['Nurshat Fateh Ali',\n",
       "   'Md. Mahdi Mohtasim',\n",
       "   'Shakil Mosharrof',\n",
       "   'T. Gopi Krishna'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.18583v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0012_Automated_Literature_Review_Using_NLP_Techniques_and_LLM-Based_Retrieval-Augmented_Generation.pdf'},\n",
       " {'paper_id': 13,\n",
       "  'title': 'Evaluating and Improving the Robustness of Security Attack Detectors\\n  Generated by LLMs',\n",
       "  'summary': 'Large Language Models (LLMs) are increasingly used in software development to\\ngenerate functions, such as attack detectors, that implement security\\nrequirements. However, LLMs struggle to generate accurate code, resulting,\\ne.g., in attack detectors that miss well-known attacks when used in practice.\\nThis is most likely due to the LLM lacking knowledge about some existing\\nattacks and to the generated code being not evaluated in real usage scenarios.\\nWe propose a novel approach integrating Retrieval Augmented Generation (RAG)\\nand Self-Ranking into the LLM pipeline. RAG enhances the robustness of the\\noutput by incorporating external knowledge sources, while the Self-Ranking\\ntechnique, inspired to the concept of Self-Consistency, generates multiple\\nreasoning paths and creates ranks to select the most robust detector. Our\\nextensive empirical study targets code generated by LLMs to detect two\\nprevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL\\ninjection (SQLi). Results show a significant improvement in detection\\nperformance compared to baselines, with an increase of up to 71%pt and 37%pt in\\nthe F2-Score for XSS and SQLi detection, respectively.',\n",
       "  'published': '2024-11-27T10:48:37Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.18216v1',\n",
       "  'authors': ['Samuele Pasini',\n",
       "   'Jinhan Kim',\n",
       "   'Tommaso Aiello',\n",
       "   'Rocio Cabrera Lozoya',\n",
       "   'Antonino Sabetta',\n",
       "   'Paolo Tonella'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.18216v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0013_Evaluating_and_Improving_the_Robustness_of_Security_Attack_Detectors_Generated_by_LLMs.pdf'},\n",
       " {'paper_id': 14,\n",
       "  'title': 'Path-RAG: Knowledge-Guided Key Region Retrieval for Open-ended Pathology\\n  Visual Question Answering',\n",
       "  'summary': 'Accurate diagnosis and prognosis assisted by pathology images are essential\\nfor cancer treatment selection and planning. Despite the recent trend of\\nadopting deep-learning approaches for analyzing complex pathology images, they\\nfall short as they often overlook the domain-expert understanding of tissue\\nstructure and cell composition. In this work, we focus on a challenging\\nOpen-ended Pathology VQA (PathVQA-Open) task and propose a novel framework\\nnamed Path-RAG, which leverages HistoCartography to retrieve relevant domain\\nknowledge from pathology images and significantly improves performance on\\nPathVQA-Open. Admitting the complexity of pathology image analysis, Path-RAG\\nadopts a human-centered AI approach by retrieving domain knowledge using\\nHistoCartography to select the relevant patches from pathology images. Our\\nexperiments suggest that domain guidance can significantly boost the accuracy\\nof LLaVA-Med from 38% to 47%, with a notable gain of 28% for H&E-stained\\npathology images in the PathVQA-Open dataset. For longer-form question and\\nanswer pairs, our model consistently achieves significant improvements of 32.5%\\nin ARCH-Open PubMed and 30.6% in ARCH-Open Books on H\\\\&E images. Our code and\\ndataset is available here (https://github.com/embedded-robotics/path-rag).',\n",
       "  'published': '2024-11-26T03:22:01Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.17073v1',\n",
       "  'authors': ['Awais Naeem',\n",
       "   'Tianhao Li',\n",
       "   'Huang-Ru Liao',\n",
       "   'Jiawei Xu',\n",
       "   'Aby M. Mathew',\n",
       "   'Zehao Zhu',\n",
       "   'Zhen Tan',\n",
       "   'Ajay Kumar Jaiswal',\n",
       "   'Raffi A. Salibian',\n",
       "   'Ziniu Hu',\n",
       "   'Tianlong Chen',\n",
       "   'Ying Ding'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.17073v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0014_Path-RAG_Knowledge-Guided_Key_Region_Retrieval_for_Open-ended_Pathology_Visual_Question_Answering.pdf'},\n",
       " {'paper_id': 15,\n",
       "  'title': 'LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology\\n  Report Generation',\n",
       "  'summary': 'In the current paradigm of image captioning, deep learning models are trained\\nto generate text from image embeddings of latent features. We challenge the\\nassumption that these latent features ought to be high-dimensional vectors\\nwhich require model fine tuning to handle. Here we propose Label Boosted\\nRetrieval Augmented Generation (LaB-RAG), a text-based approach to image\\ncaptioning that leverages image descriptors in the form of categorical labels\\nto boost standard retrieval augmented generation (RAG) with pretrained large\\nlanguage models (LLMs). We study our method in the context of radiology report\\ngeneration (RRG), where the task is to generate a clinician\\'s report detailing\\ntheir observations from a set of radiological images, such as X-rays. We argue\\nthat simple linear classifiers over extracted image embeddings can effectively\\ntransform X-rays into text-space as radiology-specific labels. In combination\\nwith standard RAG, we show that these derived text labels can be used with\\ngeneral-domain LLMs to generate radiology reports. Without ever training our\\ngenerative language model or image feature encoder models, and without ever\\ndirectly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves\\nbetter results across natural language and radiology language metrics compared\\nwith other retrieval-based RRG methods, while attaining competitive results\\ncompared to other fine-tuned vision-language RRG models. We further present\\nresults of our experiments with various components of LaB-RAG to better\\nunderstand our method. Finally, we critique the use of a popular RRG metric,\\narguing it is possible to artificially inflate its results without true\\ndata-leakage.',\n",
       "  'published': '2024-11-25T16:10:05Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.16523v1',\n",
       "  'authors': ['Steven Song',\n",
       "   'Anirudh Subramanyam',\n",
       "   'Irene Madejski',\n",
       "   'Robert L. Grossman'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.16523v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0015_LaB-RAG_Label_Boosted_Retrieval_Augmented_Generation_for_Radiology_Report_Generation.pdf'},\n",
       " {'paper_id': 16,\n",
       "  'title': 'AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\\n  Knowledge Reasoning',\n",
       "  'summary': \"Recent advancements in large language models (LLMs) have led to significant\\nimprovements in various natural language processing tasks, but it is still\\nchallenging for LLMs to perform knowledge-intensive complex question answering\\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\\nchain-like sub-questions and applies iterative RAG at each sub-question.\\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\\nto decompose complex questions into combinations of three atomic knowledge\\noperators, significantly enhancing the reasoning process at both the planning\\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\\nthat AtomR significantly outperforms state-of-the-art baselines across three\\nsingle-source and two multi-source reasoning benchmarks, with notable\\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.\",\n",
       "  'published': '2024-11-25T15:35:51Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.16495v1',\n",
       "  'authors': ['Amy Xin',\n",
       "   'Jinxin Liu',\n",
       "   'Zijun Yao',\n",
       "   'Zhicheng Li',\n",
       "   'Shulin Cao',\n",
       "   'Lei Hou',\n",
       "   'Juanzi Li'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.16495v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0016_AtomR_Atomic_Operator-Empowered_Large_Language_Models_for_Heterogeneous_Knowledge_Reasoning.pdf'},\n",
       " {'paper_id': 17,\n",
       "  'title': 'Human-Calibrated Automated Testing and Validation of Generative Language\\n  Models',\n",
       "  'summary': 'This paper introduces a comprehensive framework for the evaluation and\\nvalidation of generative language models (GLMs), with a focus on\\nRetrieval-Augmented Generation (RAG) systems deployed in high-stakes domains\\nsuch as banking. GLM evaluation is challenging due to open-ended outputs and\\nsubjective quality assessments. Leveraging the structured nature of RAG\\nsystems, where generated responses are grounded in a predefined document\\ncollection, we propose the Human-Calibrated Automated Testing (HCAT) framework.\\nHCAT integrates a) automated test generation using stratified sampling, b)\\nembedding-based metrics for explainable assessment of functionality, risk and\\nsafety attributes, and c) a two-stage calibration approach that aligns\\nmachine-generated evaluations with human judgments through probability\\ncalibration and conformal prediction.\\n  In addition, the framework includes robustness testing to evaluate model\\nperformance against adversarial, out-of-distribution, and varied input\\nconditions, as well as targeted weakness identification using marginal and\\nbivariate analysis to pinpoint specific areas for improvement. This\\nhuman-calibrated, multi-layered evaluation framework offers a scalable,\\ntransparent, and interpretable approach to GLM assessment, providing a\\npractical and reliable solution for deploying GLMs in applications where\\naccuracy, transparency, and regulatory compliance are paramount.',\n",
       "  'published': '2024-11-25T13:53:36Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.16391v1',\n",
       "  'authors': ['Agus Sudjianto',\n",
       "   'Aijun Zhang',\n",
       "   'Srinivas Neppalli',\n",
       "   'Tarun Joshi',\n",
       "   'Michal Malohlava'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.16391v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0017_Human-Calibrated_Automated_Testing_and_Validation_of_Generative_Language_Models.pdf'},\n",
       " {'paper_id': 18,\n",
       "  'title': 'Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark,\\n  Evaluate Metrics and Strong Baselines',\n",
       "  'summary': 'This paper investigates an intriguing task of Multi-modal Retrieval Augmented\\nMulti-modal Generation (M$^2$RAG). This task requires foundation models to\\nbrowse multi-modal web pages, with mixed text and images, and generate\\nmulti-modal responses for solving user queries, which exhibits better\\ninformation density and readability. Given the early researching stage of\\nM$^2$RAG task, there is a lack of systematic studies and analysis. To fill this\\ngap, we construct a benchmark for M$^2$RAG task, equipped with a suite of\\ntext-modal metrics and multi-modal metrics to analyze the capabilities of\\nexisting foundation models. Besides, we also propose several effective methods\\nfor foundation models to accomplish this task, based on the comprehensive\\nevaluation results on our benchmark. Extensive experimental results reveal\\nseveral intriguing phenomena worth further research.',\n",
       "  'published': '2024-11-25T13:20:19Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.16365v1',\n",
       "  'authors': ['Zi-Ao Ma',\n",
       "   'Tian Lan',\n",
       "   'Rong-Cheng Tu',\n",
       "   'Yong Hu',\n",
       "   'Heyan Huang',\n",
       "   'Xian-Ling Mao'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.16365v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0018_Multi-modal_Retrieval_Augmented_Multi-modal_Generation_A_Benchmark_Evaluate_Metrics_and_Strong_Baselines.pdf'},\n",
       " {'paper_id': 19,\n",
       "  'title': 'Context Awareness Gate For Retrieval Augmented Generation',\n",
       "  'summary': \"Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\\nto mitigate the limitations of large language models (LLMs) in answering\\ndomain-specific questions. Previous research has predominantly focused on\\nimproving the accuracy and quality of retrieved data chunks to enhance the\\noverall performance of the generation pipeline. However, despite ongoing\\nadvancements, the critical issue of retrieving irrelevant information -- which\\ncan impair the ability of the model to utilize its internal knowledge\\neffectively -- has received minimal attention. In this work, we investigate the\\nimpact of retrieving irrelevant information in open-domain question answering,\\nhighlighting its significant detrimental effect on the quality of LLM outputs.\\nTo address this challenge, we propose the Context Awareness Gate (CAG)\\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\\nbased on whether the user query necessitates external context retrieval.\\nAdditionally, we introduce the Vector Candidates method, a core mathematical\\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\\nfurther examine the distributions of relationships between contexts and\\nquestions, presenting a statistical analysis of these distributions. This\\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\\nAugmented Generation (RAG) systems.\",\n",
       "  'published': '2024-11-25T06:48:38Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.16133v1',\n",
       "  'authors': ['Mohammad Hassan Heydari',\n",
       "   'Arshia Hemmat',\n",
       "   'Erfan Naman',\n",
       "   'Afsaneh Fatemi'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.16133v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0019_Context_Awareness_Gate_For_Retrieval_Augmented_Generation.pdf'},\n",
       " {'paper_id': 20,\n",
       "  'title': 'RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large\\n  Language Models on Dietary Supplements',\n",
       "  'summary': \"\\\\textbf{Objective:} We aimed to develop an advanced multi-task large language\\nmodel (LLM) framework to extract multiple types of information about dietary\\nsupplements (DS) from clinical records.\\n  \\\\textbf{Methods:} We used four core DS information extraction tasks - namely,\\nnamed entity recognition (NER: 2,949 clinical sentences), relation extraction\\n(RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage\\nclassification (UC: 2,460 sentences) as our multitasks. We introduced a novel\\nRetrieval-Augmented Multi-task Information Extraction (RAMIE) Framework,\\nincluding: 1) employed instruction fine-tuning techniques with task-specific\\nprompts, 2) trained LLMs for multiple tasks with improved storage efficiency\\nand lower training costs, and 3) incorporated retrieval augmentation generation\\n(RAG) techniques by retrieving similar examples from the training set. We\\ncompared RAMIE's performance to LLMs with instruction fine-tuning alone and\\nconducted an ablation study to assess the contributions of multi-task learning\\nand RAG to improved multitasking performance.\\n  \\\\textbf{Results:} With the aid of the RAMIE framework, Llama2-13B achieved an\\nF1 score of 87.39 (3.51\\\\% improvement) on the NER task and demonstrated\\noutstanding performance on the RE task with an F1 score of 93.74 (1.15\\\\%\\nimprovement). For the TE task, Llama2-7B scored 79.45 (14.26\\\\% improvement),\\nand MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94\\\\% improvement) on\\nthe UC task. The ablation study revealed that while MTL increased efficiency\\nwith a slight trade-off in performance, RAG significantly boosted overall\\naccuracy.\\n  \\\\textbf{Conclusion:} This study presents a novel RAMIE framework that\\ndemonstrates substantial improvements in multi-task information extraction for\\nDS-related data from clinical records. Our framework can potentially be applied\\nto other domains.\",\n",
       "  'published': '2024-11-24T03:56:43Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.15700v1',\n",
       "  'authors': ['Zaifu Zhan', 'Shuang Zhou', 'Mingchen Li', 'Rui Zhang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.15700v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0020_RAMIE_Retrieval-Augmented_Multi-task_Information_Extraction_with_Large_Language_Models_on_Dietary_Supplements.pdf'},\n",
       " {'paper_id': 21,\n",
       "  'title': 'Document Haystacks: Vision-Language Reasoning Over Piles of 1000+\\n  Documents',\n",
       "  'summary': 'Large multimodal models (LMMs) have achieved impressive progress in\\nvision-language understanding, yet they face limitations in real-world\\napplications requiring complex reasoning over a large number of images.\\nExisting benchmarks for multi-image question-answering are limited in scope,\\neach question is paired with only up to 30 images, which does not fully capture\\nthe demands of large-scale retrieval tasks encountered in the real-world\\nusages. To reduce these gaps, we introduce two document haystack benchmarks,\\ndubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on\\nlarge-scale visual document retrieval and understanding. Additionally, we\\npropose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG)\\nframework that leverages a suite of multimodal vision encoders, each optimized\\nfor specific strengths, and a dedicated question-document relevance module.\\nV-RAG sets a new standard, with a 9% and 11% improvement in Recall@1 on the\\nchallenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively,\\ncompared to the previous best baseline models. Additionally, integrating V-RAG\\nwith LMMs enables them to efficiently operate across thousands of images,\\nyielding significant improvements on our DocHaystack and InfoHaystack\\nbenchmarks. Our code and datasets are available at\\nhttps://github.com/Vision-CAIR/dochaystacks',\n",
       "  'published': '2024-11-23T18:14:42Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.16740v1',\n",
       "  'authors': ['Jun Chen',\n",
       "   'Dannong Xu',\n",
       "   'Junjie Fei',\n",
       "   'Chun-Mei Feng',\n",
       "   'Mohamed Elhoseiny'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.16740v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0021_Document_Haystacks_Vision-Language_Reasoning_Over_Piles_of_1000_Documents.pdf'},\n",
       " {'paper_id': 22,\n",
       "  'title': 'From MTEB to MTOB: Retrieval-Augmented Classification for Descriptive\\n  Grammars',\n",
       "  'summary': \"Recent advances in language modeling have demonstrated significant\\nimprovements in zero-shot capabilities, including in-context learning,\\ninstruction following, and machine translation for extremely under-resourced\\nlanguages (Tanzer et al., 2024). However, many languages with limited written\\nresources rely primarily on formal descriptions of grammar and vocabulary.\\n  In this paper, we introduce a set of benchmarks to evaluate how well models\\ncan extract and classify information from the complex descriptions found in\\nlinguistic grammars. We present a Retrieval-Augmented Generation (RAG)-based\\napproach that leverages these descriptions for downstream tasks such as machine\\ntranslation. Our benchmarks encompass linguistic descriptions for 248 languages\\nacross 142 language families, focusing on typological features from WALS and\\nGrambank.\\n  This set of benchmarks offers the first comprehensive evaluation of language\\nmodels' in-context ability to accurately interpret and extract linguistic\\nfeatures, providing a critical resource for scaling NLP to low-resource\\nlanguages. The code and data are publicly available at\\n\\\\url{https://github.com/al-the-eigenvalue/RAG-on-grammars}.\",\n",
       "  'published': '2024-11-23T14:47:10Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.15577v1',\n",
       "  'authors': ['Albert Kornilov', 'Tatiana Shavrina'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.15577v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0022_From_MTEB_to_MTOB_Retrieval-Augmented_Classification_for_Descriptive_Grammars.pdf'},\n",
       " {'paper_id': 23,\n",
       "  'title': 'Multi-Reranker: Maximizing performance of retrieval-augmented generation\\n  in the FinanceRAG challenge',\n",
       "  'summary': \"As Large Language Models (LLMs) increasingly address domain-specific\\nproblems, their application in the financial sector has expanded rapidly. Tasks\\nthat are both highly valuable and time-consuming, such as analyzing financial\\nstatements, disclosures, and related documents, are now being effectively\\ntackled using LLMs. This paper details the development of a high-performance,\\nfinance-specific Retrieval-Augmented Generation (RAG) system for the ACM-ICAIF\\n'24 FinanceRAG competition. We optimized performance through ablation studies\\non query expansion and corpus refinement during the pre-retrieval phase. To\\nenhance retrieval accuracy, we employed multiple reranker models. Notably, we\\nintroduced an efficient method for managing long context sizes during the\\ngeneration phase, significantly improving response quality without sacrificing\\nperformance. We ultimately achieve 2nd place in the FinanceRAG Challenge. Our\\nkey contributions include: (1) pre-retrieval ablation analysis, (2) an enhanced\\nretrieval algorithm, and (3) a novel approach for long-context management. This\\nwork demonstrates the potential of LLMs in effectively processing and analyzing\\ncomplex financial data to generate accurate and valuable insights. The source\\ncode and further details are available at https://github.com/cv-lee/FinanceRAG.\",\n",
       "  'published': '2024-11-23T09:56:21Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.16732v1',\n",
       "  'authors': ['Joohyun Lee', 'Minji Roh'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.16732v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0023_Multi-Reranker_Maximizing_performance_of_retrieval-augmented_generation_in_the_FinanceRAG_challenge.pdf'},\n",
       " {'paper_id': 24,\n",
       "  'title': 'Traditional Chinese Medicine Case Analysis System for High-Level\\n  Semantic Abstraction: Optimized with Prompt and RAG',\n",
       "  'summary': 'This paper details a technical plan for building a clinical case database for\\nTraditional Chinese Medicine (TCM) using web scraping. Leveraging multiple\\nplatforms, including 360doc, we gathered over 5,000 TCM clinical cases,\\nperformed data cleaning, and structured the dataset with crucial fields such as\\npatient details, pathogenesis, syndromes, and annotations. Using the\\n$Baidu\\\\_ERNIE\\\\_Speed\\\\_128K$ API, we removed redundant information and generated\\nthe final answers through the $DeepSeekv2$ API, outputting results in standard\\nJSON format. We optimized data recall with RAG and rerank techniques during\\nretrieval and developed a hybrid matching scheme. By combining two-stage\\nretrieval method with keyword matching via Jieba, we significantly enhanced the\\naccuracy of model outputs.',\n",
       "  'published': '2024-11-23T08:24:15Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.15491v1',\n",
       "  'authors': ['Peng Xu',\n",
       "   'Hongjin Wu',\n",
       "   'Jinle Wang',\n",
       "   'Rongjia Lin',\n",
       "   'Liwei Tan'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.15491v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0024_Traditional_Chinese_Medicine_Case_Analysis_System_for_High-Level_Semantic_Abstraction_Optimized_with_Prompt_and_RAG.pdf'},\n",
       " {'paper_id': 25,\n",
       "  'title': 'Improving Factuality of 3D Brain MRI Report Generation with Paired\\n  Image-domain Retrieval and Text-domain Augmentation',\n",
       "  'summary': 'Acute ischemic stroke (AIS) requires time-critical management, with hours of\\ndelayed intervention leading to an irreversible disability of the patient.\\nSince diffusion weighted imaging (DWI) using the magnetic resonance image (MRI)\\nplays a crucial role in the detection of AIS, automated prediction of AIS from\\nDWI has been a research topic of clinical importance. While text radiology\\nreports contain the most relevant clinical information from the image findings,\\nthe difficulty of mapping across different modalities has limited the\\nfactuality of conventional direct DWI-to-report generation methods. Here, we\\npropose paired image-domain retrieval and text-domain augmentation (PIRTA), a\\ncross-modal retrieval-augmented generation (RAG) framework for providing\\nclinician-interpretative AIS radiology reports with improved factuality. PIRTA\\nmitigates the need for learning cross-modal mapping, which poses difficulty in\\nimage-to-text generation, by casting the cross-modal mapping problem as an\\nin-domain retrieval of similar DWI images that have paired ground-truth text\\nradiology reports. By exploiting the retrieved radiology reports to augment the\\nreport generation process of the query image, we show by experiments with\\nextensive in-house and public datasets that PIRTA can accurately retrieve\\nrelevant reports from 3D DWI images. This approach enables the generation of\\nradiology reports with significantly higher accuracy compared to direct\\nimage-to-text generation using state-of-the-art multimodal language models.',\n",
       "  'published': '2024-11-23T08:18:55Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.15490v1',\n",
       "  'authors': ['Junhyeok Lee',\n",
       "   'Yujin Oh',\n",
       "   'Dahyoun Lee',\n",
       "   'Hyon Keun Joh',\n",
       "   'Chul-Ho Sohn',\n",
       "   'Sung Hyun Baik',\n",
       "   'Cheol Kyu Jung',\n",
       "   'Jung Hyun Park',\n",
       "   'Kyu Sung Choi',\n",
       "   'Byung-Hoon Kim',\n",
       "   'Jong Chul Ye'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.15490v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0025_Improving_Factuality_of_3D_Brain_MRI_Report_Generation_with_Paired_Image-domain_Retrieval_and_Text-domain_Augmentation.pdf'},\n",
       " {'paper_id': 26,\n",
       "  'title': 'mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for\\n  Knowledge-Based VQA',\n",
       "  'summary': 'Advanced Multimodal Large Language Models (MLLMs) struggle with recent\\nKnowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their\\nlimited and frozen knowledge scope, often leading to ambiguous and inaccurate\\nresponses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally\\nintroduced to provide MLLMs with comprehensive and up-to-date knowledge,\\neffectively expanding the knowledge scope. However, current mRAG methods have\\ninherent drawbacks, including: 1) Performing retrieval even when external\\nknowledge is not needed. 2) Lacking of identification of evidence that supports\\nthe query. 3) Increasing model complexity due to additional information\\nfiltering modules or rules. To address these shortcomings, we propose a novel\\ngeneralized framework called \\\\textbf{m}ultimodal\\n\\\\textbf{R}etrieval-\\\\textbf{R}eflection-\\\\textbf{A}ugmented \\\\textbf{G}eneration\\n(mR$^2$AG), which achieves adaptive retrieval and useful information\\nlocalization to enable answers through two easy-to-implement reflection\\noperations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection\\nis designed to distinguish different user queries and avoids redundant\\nretrieval calls, and Relevance-Reflection is introduced to guide the MLLM in\\nlocating beneficial evidence of the retrieved content and generating answers\\naccordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM\\nwith efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset\\n(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,\\nGPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while\\nmaintaining the exceptional capabilities of base MLLMs across a wide range of\\nVisual-dependent tasks.',\n",
       "  'published': '2024-11-22T16:15:50Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.15041v1',\n",
       "  'authors': ['Tao Zhang',\n",
       "   'Ziqi Zhang',\n",
       "   'Zongyang Ma',\n",
       "   'Yuxin Chen',\n",
       "   'Zhongang Qi',\n",
       "   'Chunfeng Yuan',\n",
       "   'Bing Li',\n",
       "   'Junfu Pu',\n",
       "   'Yuxuan Zhao',\n",
       "   'Zehua Xie',\n",
       "   'Jin Ma',\n",
       "   'Ying Shan',\n",
       "   'Weiming Hu'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.15041v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0026_mR2AG_Multimodal_Retrieval-Reflection-Augmented_Generation_for_Knowledge-Based_VQA.pdf'},\n",
       " {'paper_id': 27,\n",
       "  'title': 'G-RAG: Knowledge Expansion in Material Science',\n",
       "  'summary': 'In the field of Material Science, effective information retrieval systems are\\nessential for facilitating research. Traditional Retrieval-Augmented Generation\\n(RAG) approaches in Large Language Models (LLMs) often encounter challenges\\nsuch as outdated information, hallucinations, limited interpretability due to\\ncontext constraints, and inaccurate retrieval. To address these issues, Graph\\nRAG integrates graph databases to enhance the retrieval process. Our proposed\\nmethod processes Material Science documents by extracting key entities\\n(referred to as MatIDs) from sentences, which are then utilized to query\\nexternal Wikipedia knowledge bases (KBs) for additional relevant information.\\nWe implement an agent-based parsing technique to achieve a more detailed\\nrepresentation of the documents. Our improved version of Graph RAG called G-RAG\\nfurther leverages a graph database to capture relationships between these\\nentities, improving both retrieval accuracy and contextual understanding. This\\nenhanced approach demonstrates significant improvements in performance for\\ndomains that require precise information retrieval, such as Material Science.',\n",
       "  'published': '2024-11-21T21:22:58Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.14592v1',\n",
       "  'authors': ['Radeen Mostafa',\n",
       "   'Mirza Nihal Baig',\n",
       "   'Mashaekh Tausif Ehsan',\n",
       "   'Jakir Hasan'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.14592v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0027_G-RAG_Knowledge_Expansion_in_Material_Science.pdf'},\n",
       " {'paper_id': 28,\n",
       "  'title': 'Towards Knowledge Checking in Retrieval-augmented Generation: A\\n  Representation Perspective',\n",
       "  'summary': \"Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing\\nthe performance of Large Language Models (LLMs). However, these systems face\\nchallenges in effectively integrating external knowledge with the LLM's\\ninternal knowledge, often leading to issues with misleading or unhelpful\\ninformation. This work aims to provide a systematic study on knowledge checking\\nin RAG systems. We conduct a comprehensive analysis of LLM representation\\nbehaviors and demonstrate the significance of using representations in\\nknowledge checking. Motivated by the findings, we further develop\\nrepresentation-based classifiers for knowledge filtering. We show substantial\\nimprovements in RAG performance, even when dealing with noisy knowledge\\ndatabases. Our study provides new insights into leveraging LLM representations\\nfor enhancing the reliability and effectiveness of RAG systems.\",\n",
       "  'published': '2024-11-21T20:39:13Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.14572v1',\n",
       "  'authors': ['Shenglai Zeng',\n",
       "   'Jiankun Zhang',\n",
       "   'Bingheng Li',\n",
       "   'Yuping Lin',\n",
       "   'Tianqi Zheng',\n",
       "   'Dante Everaert',\n",
       "   'Hanqing Lu',\n",
       "   'Hui Liu',\n",
       "   'Hui Liu',\n",
       "   'Yue Xing',\n",
       "   'Monica Xiao Cheng',\n",
       "   'Jiliang Tang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.14572v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0028_Towards_Knowledge_Checking_in_Retrieval-augmented_Generation_A_Representation_Perspective.pdf'},\n",
       " {'paper_id': 29,\n",
       "  'title': 'Enhancing LLMs for Power System Simulations: A Feedback-driven\\n  Multi-agent Framework',\n",
       "  'summary': 'The integration of experimental technologies with large language models\\n(LLMs) is transforming scientific research, positioning AI as a versatile\\nresearch assistant rather than a mere problem-solving tool. In the field of\\npower systems, however, managing simulations -- one of the essential\\nexperimental technologies -- remains a challenge for LLMs due to their limited\\ndomain-specific knowledge, restricted reasoning capabilities, and imprecise\\nhandling of simulation parameters. To address these limitations, we propose a\\nfeedback-driven, multi-agent framework that incorporates three proposed\\nmodules: an enhanced retrieval-augmented generation (RAG) module, an improved\\nreasoning module, and a dynamic environmental acting module with an\\nerror-feedback mechanism. Validated on 69 diverse tasks from Daline and\\nMATPOWER, this framework achieves success rates of 93.13% and 96.85%,\\nrespectively, significantly outperforming the latest LLMs (ChatGPT 4o and\\no1-preview), which achieved a 27.77% success rate on standard simulation tasks\\nand 0% on complex tasks. Additionally, our framework also supports rapid,\\ncost-effective task execution, completing each simulation in approximately 30\\nseconds at an average cost of 0.014 USD for tokens. Overall, this adaptable\\nframework lays a foundation for developing intelligent LLM-based assistants for\\nhuman researchers, facilitating power system research and beyond.',\n",
       "  'published': '2024-11-21T19:01:07Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.16707v1',\n",
       "  'authors': ['Mengshuo Jia', 'Zeyu Cui', 'Gabriela Hug'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.16707v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0029_Enhancing_LLMs_for_Power_System_Simulations_A_Feedback-driven_Multi-agent_Framework.pdf'},\n",
       " {'paper_id': 30,\n",
       "  'title': 'Efficient Aspect-Based Summarization of Climate Change Reports with\\n  Small Language Models',\n",
       "  'summary': 'The use of Natural Language Processing (NLP) for helping decision-makers with\\nClimate Change action has recently been highlighted as a use case aligning with\\na broader drive towards NLP technologies for social good. In this context,\\nAspect-Based Summarization (ABS) systems that extract and summarize relevant\\ninformation are particularly useful as they provide stakeholders with a\\nconvenient way of finding relevant information in expert-curated reports. In\\nthis work, we release a new dataset for ABS of Climate Change reports and we\\nemploy different Large Language Models (LLMs) and so-called Small Language\\nModels (SLMs) to tackle this problem in an unsupervised way. Considering the\\nproblem at hand, we also show how SLMs are not significantly worse for the\\nproblem while leading to reduced carbon footprint; we do so by applying for the\\nfirst time an existing framework considering both energy efficiency and task\\nperformance to the evaluation of zero-shot generative models for ABS. Overall,\\nour results show that modern language models, both big and small, can\\neffectively tackle ABS for Climate Change reports but more research is needed\\nwhen we frame the problem as a Retrieval Augmented Generation (RAG) problem and\\nour work and dataset will help foster efforts in this direction.',\n",
       "  'published': '2024-11-21T16:28:32Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.14272v1',\n",
       "  'authors': ['Iacopo Ghinassi', 'Leonardo Catalano', 'Tommaso Colella'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.14272v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0030_Efficient_Aspect-Based_Summarization_of_Climate_Change_Reports_with_Small_Language_Models.pdf'},\n",
       " {'paper_id': 31,\n",
       "  'title': 'Towards Context-Rich Automated Biodiversity Assessments: Deriving\\n  AI-Powered Insights from Camera Trap Data',\n",
       "  'summary': \"Camera traps offer enormous new opportunities in ecological studies, but\\ncurrent automated image analysis methods often lack the contextual richness\\nneeded to support impactful conservation outcomes. Here we present an\\nintegrated approach that combines deep learning-based vision and language\\nmodels to improve ecological reporting using data from camera traps. We\\nintroduce a two-stage system: YOLOv10-X to localise and classify species\\n(mammals and birds) within images, and a Phi-3.5-vision-instruct model to read\\nYOLOv10-X binding box labels to identify species, overcoming its limitation\\nwith hard to classify objects in images. Additionally, Phi-3.5 detects broader\\nvariables, such as vegetation type, and time of day, providing rich ecological\\nand environmental context to YOLO's species detection output. When combined,\\nthis output is processed by the model's natural language system to answer\\ncomplex queries, and retrieval-augmented generation (RAG) is employed to enrich\\nresponses with external information, like species weight and IUCN status\\n(information that cannot be obtained through direct visual analysis). This\\ninformation is used to automatically generate structured reports, providing\\nbiodiversity stakeholders with deeper insights into, for example, species\\nabundance, distribution, animal behaviour, and habitat selection. Our approach\\ndelivers contextually rich narratives that aid in wildlife management\\ndecisions. By providing contextually rich insights, our approach not only\\nreduces manual effort but also supports timely decision-making in conservation,\\npotentially shifting efforts from reactive to proactive management.\",\n",
       "  'published': '2024-11-21T15:28:52Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.14219v1',\n",
       "  'authors': ['Paul Fergus',\n",
       "   'Carl Chalmers',\n",
       "   'Naomi Matthews',\n",
       "   'Stuart Nixon',\n",
       "   'Andre Burger',\n",
       "   'Oliver Hartley',\n",
       "   'Chris Sutherland',\n",
       "   'Xavier Lambin',\n",
       "   'Steven Longmore',\n",
       "   'Serge Wich'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.14219v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0031_Towards_Context-Rich_Automated_Biodiversity_Assessments_Deriving_AI-Powered_Insights_from_Camera_Trap_Data.pdf'},\n",
       " {'paper_id': 32,\n",
       "  'title': 'RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented\\n  Generation Applications with Agent-based Attacks',\n",
       "  'summary': \"While large language models (LLMs) have achieved notable success in\\ngenerative tasks, they still face limitations, such as lacking up-to-date\\nknowledge and producing hallucinations. Retrieval-Augmented Generation (RAG)\\nenhances LLM performance by integrating external knowledge bases, providing\\nadditional context which significantly improves accuracy and knowledge\\ncoverage. However, building these external knowledge bases often requires\\nsubstantial resources and may involve sensitive information. In this paper, we\\npropose an agent-based automated privacy attack called RAG-Thief, which can\\nextract a scalable amount of private data from the private database used in RAG\\napplications. We conduct a systematic study on the privacy risks associated\\nwith RAG applications, revealing that the vulnerability of LLMs makes the\\nprivate knowledge bases suffer significant privacy risks. Unlike previous\\nmanual attacks which rely on traditional prompt injection techniques, RAG-Thief\\nstarts with an initial adversarial query and learns from model responses,\\nprogressively generating new queries to extract as many chunks from the\\nknowledge base as possible. Experimental results show that our RAG-Thief can\\nextract over 70% information from the private knowledge bases within customized\\nRAG applications deployed on local machines and real-world platforms, including\\nOpenAI's GPTs and ByteDance's Coze. Our findings highlight the privacy\\nvulnerabilities in current RAG applications and underscore the pressing need\\nfor stronger safeguards.\",\n",
       "  'published': '2024-11-21T13:18:03Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.14110v1',\n",
       "  'authors': ['Changyue Jiang',\n",
       "   'Xudong Pan',\n",
       "   'Geng Hong',\n",
       "   'Chenfu Bao',\n",
       "   'Min Yang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.14110v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0032_RAG-Thief_Scalable_Extraction_of_Private_Data_from_Retrieval-Augmented_Generation_Applications_with_Agent-based_Attacks.pdf'},\n",
       " {'paper_id': 33,\n",
       "  'title': 'FastRAG: Retrieval Augmented Generation for Semi-structured Data',\n",
       "  'summary': 'Efficiently processing and interpreting network data is critical for the\\noperation of increasingly complex networks. Recent advances in Large Language\\nModels (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved\\ndata processing in network management. However, existing RAG methods like\\nVectorRAG and GraphRAG struggle with the complexity and implicit nature of\\nsemi-structured technical data, leading to inefficiencies in time, cost, and\\nretrieval. This paper introduces FastRAG, a novel RAG approach designed for\\nsemi-structured data. FastRAG employs schema learning and script learning to\\nextract and structure data without needing to submit entire data sources to an\\nLLM. It integrates text search with knowledge graph (KG) querying to improve\\naccuracy in retrieving context-rich information. Evaluation results demonstrate\\nthat FastRAG provides accurate question answering, while improving up to 90% in\\ntime and 85% in cost compared to GraphRAG.',\n",
       "  'published': '2024-11-21T01:00:25Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.13773v1',\n",
       "  'authors': ['Amar Abane', 'Anis Bekri', 'Abdella Battou'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.13773v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0033_FastRAG_Retrieval_Augmented_Generation_for_Semi-structured_Data.pdf'},\n",
       " {'paper_id': 34,\n",
       "  'title': 'Retrieval-Augmented Generation for Domain-Specific Question Answering: A\\n  Case Study on Pittsburgh and CMU',\n",
       "  'summary': 'We designed a Retrieval-Augmented Generation (RAG) system to provide large\\nlanguage models with relevant documents for answering domain-specific questions\\nabout Pittsburgh and Carnegie Mellon University (CMU). We extracted over 1,800\\nsubpages using a greedy scraping strategy and employed a hybrid annotation\\nprocess, combining manual and Mistral-generated question-answer pairs,\\nachieving an inter-annotator agreement (IAA) score of 0.7625. Our RAG framework\\nintegrates BM25 and FAISS retrievers, enhanced with a reranker for improved\\ndocument retrieval accuracy. Experimental results show that the RAG system\\nsignificantly outperforms a non-RAG baseline, particularly in time-sensitive\\nand complex queries, with an F1 score improvement from 5.45% to 42.21% and\\nrecall of 56.18%. This study demonstrates the potential of RAG systems in\\nenhancing answer precision and relevance, while identifying areas for further\\noptimization in document retrieval and model training.',\n",
       "  'published': '2024-11-20T20:10:43Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.13691v1',\n",
       "  'authors': ['Haojia Sun', 'Yaqi Wang', 'Shuting Zhang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.13691v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0034_Retrieval-Augmented_Generation_for_Domain-Specific_Question_Answering_A_Case_Study_on_Pittsburgh_and_CMU.pdf'},\n",
       " {'paper_id': 35,\n",
       "  'title': 'On the Way to LLM Personalization: Learning to Remember User\\n  Conversations',\n",
       "  'summary': 'Large Language Models (LLMs) have quickly become an invaluable assistant for\\na variety of tasks. However, their effectiveness is constrained by their\\nability to tailor responses to human preferences and behaviors via\\npersonalization. Prior work in LLM personalization has largely focused on style\\ntransfer or incorporating small factoids about the user, as knowledge injection\\nremains an open challenge. In this paper, we explore injecting knowledge of\\nprior conversations into LLMs to enable future work on less redundant,\\npersonalized conversations. We identify two real-world constraints: (1)\\nconversations are sequential in time and must be treated as such during\\ntraining, and (2) per-user personalization is only viable in\\nparameter-efficient settings. To this aim, we propose PLUM, a pipeline\\nperforming data augmentation for up-sampling conversations as question-answer\\npairs, that are then used to finetune a low-rank adaptation adapter with a\\nweighted cross entropy loss. Even in this first exploration of the problem, we\\nperform competitively with baselines such as RAG, attaining an accuracy of\\n81.5% across 100 conversations.',\n",
       "  'published': '2024-11-20T15:45:08Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.13405v1',\n",
       "  'authors': ['Lucie Charlotte Magister',\n",
       "   'Katherine Metcalf',\n",
       "   'Yizhe Zhang',\n",
       "   'Maartje ter Hoeve'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.13405v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0035_On_the_Way_to_LLM_Personalization_Learning_to_Remember_User_Conversations.pdf'},\n",
       " {'paper_id': 36,\n",
       "  'title': 'AIDBench: A benchmark for evaluating the authorship identification\\n  capability of large language models',\n",
       "  'summary': \"As large language models (LLMs) rapidly advance and integrate into daily\\nlife, the privacy risks they pose are attracting increasing attention. We focus\\non a specific privacy risk where LLMs may help identify the authorship of\\nanonymous texts, which challenges the effectiveness of anonymity in real-world\\nsystems such as anonymous peer review systems. To investigate these risks, we\\npresent AIDBench, a new benchmark that incorporates several author\\nidentification datasets, including emails, blogs, reviews, articles, and\\nresearch papers. AIDBench utilizes two evaluation methods: one-to-one\\nauthorship identification, which determines whether two texts are from the same\\nauthor; and one-to-many authorship identification, which, given a query text\\nand a list of candidate texts, identifies the candidate most likely written by\\nthe same author as the query text. We also introduce a Retrieval-Augmented\\nGeneration (RAG)-based method to enhance the large-scale authorship\\nidentification capabilities of LLMs, particularly when input lengths exceed the\\nmodels' context windows, thereby establishing a new baseline for authorship\\nidentification using LLMs. Our experiments with AIDBench demonstrate that LLMs\\ncan correctly guess authorship at rates well above random chance, revealing new\\nprivacy risks posed by these powerful models. The source code and data will be\\nmade publicly available after acceptance.\",\n",
       "  'published': '2024-11-20T11:41:08Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.13226v1',\n",
       "  'authors': ['Zichen Wen', 'Dadi Guo', 'Huishuai Zhang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.13226v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0036_AIDBench_A_benchmark_for_evaluating_the_authorship_identification_capability_of_large_language_models.pdf'},\n",
       " {'paper_id': 37,\n",
       "  'title': 'Writing Style Matters: An Examination of Bias and Fairness in\\n  Information Retrieval Systems',\n",
       "  'summary': \"The rapid advancement of Language Model technologies has opened new\\nopportunities, but also introduced new challenges related to bias and fairness.\\nThis paper explores the uncharted territory of potential biases in\\nstate-of-the-art universal text embedding models towards specific document and\\nquery writing styles within Information Retrieval (IR) systems. Our\\ninvestigation reveals that different embedding models exhibit different\\npreferences of document writing style, while more informal and emotive styles\\nare less favored by most embedding models. In terms of query writing styles,\\nmany embedding models tend to match the style of the query with the style of\\nthe retrieved documents, but some show a consistent preference for specific\\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\\ndisplay a consistent preference for certain style of generated data. These\\nbiases in text embedding based IR systems can inadvertently silence or\\nmarginalize certain communication styles, thereby posing a significant threat\\nto fairness in information retrieval. Finally, we also compare the answer\\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\\nand find out that most text embedding models are biased towards LLM's answer\\nstyles when used as evaluation metrics for answer correctness. This study sheds\\nlight on the critical issue of writing style based bias in IR systems, offering\\nvaluable insights for the development of more fair and robust models.\",\n",
       "  'published': '2024-11-20T10:17:09Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.13173v1',\n",
       "  'authors': ['Hongliu Cao'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.13173v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0037_Writing_Style_Matters_An_Examination_of_Bias_and_Fairness_in_Information_Retrieval_Systems.pdf'},\n",
       " {'paper_id': 38,\n",
       "  'title': 'Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\\n  Large Language Model System for Medical Coding',\n",
       "  'summary': \"The reuse of historical clinical trial data has significant potential to\\naccelerate medical research and drug development. However, interoperability\\nchallenges, particularly with missing medical codes, hinders effective data\\nintegration across studies. While Large Language Models (LLMs) offer a\\npromising solution for automated coding without labeled data, current\\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\\nfollows a three-step process: (1) diverse candidate code generation; (2)\\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\\nwhile also providing capabilities for trustworthy deployment. For MedDRA\\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\\nenhancing performance on uncommon medications. ALIGN achieves this\\ncost-efficiently at \\\\$0.0007 and \\\\$0.02 per code for GPT-4o-mini and GPT-4o,\\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\\nfor clinical trial data, contributing to enhanced data interoperability and\\nreusability, positioning it as a promising tool to improve clinical research\\nand accelerate drug development.\",\n",
       "  'published': '2024-11-20T09:59:12Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.13163v1',\n",
       "  'authors': ['Nabeel Seedat',\n",
       "   'Caterina Tozzi',\n",
       "   'Andrea Hita Ardiaca',\n",
       "   'Mihaela van der Schaar',\n",
       "   'James Weatherall',\n",
       "   'Adam Taylor'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.13163v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0038_Unlocking_Historical_Clinical_Trial_Data_with_ALIGN_A_Compositional_Large_Language_Model_System_for_Medical_Coding.pdf'},\n",
       " {'paper_id': 39,\n",
       "  'title': 'DMQR-RAG: Diverse Multi-Query Rewriting for RAG',\n",
       "  'summary': 'Large language models often encounter challenges with static knowledge and\\nhallucinations, which undermine their reliability. Retrieval-augmented\\ngeneration (RAG) mitigates these issues by incorporating external information.\\nHowever, user queries frequently contain noise and intent deviations,\\nnecessitating query rewriting to improve the relevance of retrieved documents.\\nIn this paper, we introduce DMQR-RAG, a Diverse Multi-Query Rewriting framework\\ndesigned to improve the performance of both document retrieval and final\\nresponses in RAG. Specifically, we investigate how queries with varying\\ninformation quantities can retrieve a diverse array of documents, presenting\\nfour rewriting strategies that operate at different levels of information to\\nenhance the performance of baseline approaches. Additionally, we propose an\\nadaptive strategy selection method that minimizes the number of rewrites while\\noptimizing overall performance. Our methods have been rigorously validated\\nthrough extensive experiments conducted in both academic and industry settings.',\n",
       "  'published': '2024-11-20T09:43:30Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.13154v1',\n",
       "  'authors': ['Zhicong Li',\n",
       "   'Jiahao Wang',\n",
       "   'Zhishu Jiang',\n",
       "   'Hangyu Mao',\n",
       "   'Zhongxia Chen',\n",
       "   'Jiazhen Du',\n",
       "   'Yuanxing Zhang',\n",
       "   'Fuzheng Zhang',\n",
       "   'Di Zhang',\n",
       "   'Yong Liu'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.13154v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0039_DMQR-RAG_Diverse_Multi-Query_Rewriting_for_RAG.pdf'},\n",
       " {'paper_id': 40,\n",
       "  'title': 'Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension',\n",
       "  'summary': 'Existing large video-language models (LVLMs) struggle to comprehend long\\nvideos correctly due to limited context. To address this problem, fine-tuning\\nlong-context LVLMs and employing GPT-based agents have emerged as promising\\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\\nand substantial GPU resources, while GPT-based agents would rely on proprietary\\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\\nalignment while providing additional information beyond the visual content.\\nSpecifically, we leverage open-source external tools to extract\\nvisually-aligned information from pure video data (e.g., audio, optical\\ncharacter, and object detection), and incorporate the extracted information\\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\\nimplementation and compatibility with any LVLM; and (iii) significant,\\nconsistent performance gains across long video understanding benchmarks,\\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\\nwhen utilized with a 72B model.',\n",
       "  'published': '2024-11-20T07:44:34Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.13093v1',\n",
       "  'authors': ['Yongdong Luo',\n",
       "   'Xiawu Zheng',\n",
       "   'Xiao Yang',\n",
       "   'Guilin Li',\n",
       "   'Haojia Lin',\n",
       "   'Jinfa Huang',\n",
       "   'Jiayi Ji',\n",
       "   'Fei Chao',\n",
       "   'Jiebo Luo',\n",
       "   'Rongrong Ji'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.13093v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0040_Video-RAG_Visually-aligned_Retrieval-Augmented_Long_Video_Comprehension.pdf'},\n",
       " {'paper_id': 41,\n",
       "  'title': 'Multimodal large language model for wheat breeding: a new exploration of\\n  smart breeding',\n",
       "  'summary': 'UAV remote sensing technology has become a key technology in crop breeding,\\nwhich can achieve high-throughput and non-destructive collection of crop\\nphenotyping data. However, the multidisciplinary nature of breeding has brought\\ntechnical barriers and efficiency challenges to knowledge mining. Therefore, it\\nis important to develop a smart breeding goal tool to mine cross-domain\\nmultimodal data. Based on different pre-trained open-source multimodal large\\nlanguage models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used\\nsupervised fine-tuning (SFT), retrieval-augmented generation (RAG), and\\nreinforcement learning from human feedback (RLHF) technologies to inject\\ncross-domain knowledge into MLLMs, thereby constructing multiple multimodal\\nlarge language models for wheat breeding (WBLMs). The above WBLMs were\\nevaluated using the newly created evaluation benchmark in this study. The\\nresults showed that the WBLM constructed using SFT, RAG and RLHF technologies\\nand InternVL2-8B has leading performance. Then, subsequent experiments were\\nconducted using the WBLM. Ablation experiments indicated that the combination\\nof SFT, RAG, and RLHF technologies can improve the overall generation\\nperformance, enhance the generated quality, balance the timeliness and\\nadaptability of the generated answer, and reduce hallucinations and biases. The\\nWBLM performed best in wheat yield prediction using cross-domain data (remote\\nsensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of\\n0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate\\nprofessional decision support answers for phenotyping estimation, environmental\\nstress assessment, target germplasm screening, cultivation technique\\nrecommendation, and seed price query tasks.',\n",
       "  'published': '2024-11-20T04:47:42Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.15203v1',\n",
       "  'authors': ['Guofeng Yang',\n",
       "   'Yu Li',\n",
       "   'Yong He',\n",
       "   'Zhenjiang Zhou',\n",
       "   'Lingzhen Ye',\n",
       "   'Hui Fang',\n",
       "   'Yiqi Luo',\n",
       "   'Xuping Feng'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.15203v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0041_Multimodal_large_language_model_for_wheat_breeding_a_new_exploration_of_smart_breeding.pdf'},\n",
       " {'paper_id': 42,\n",
       "  'title': 'CodeXEmbed: A Generalist Embedding Model Family for Multiligual and\\n  Multi-task Code Retrieval',\n",
       "  'summary': 'Despite the success of text retrieval in many NLP tasks, code retrieval\\nremains a largely underexplored area. Most text retrieval systems are tailored\\nfor natural language queries, often neglecting the specific challenges of\\nretrieving code. This gap leaves existing models unable to effectively capture\\nthe diversity of programming languages and tasks across different domains,\\nhighlighting the need for more focused research in code retrieval. To address\\nthis, we introduce CodeXEmbed, a family of large-scale code embedding models\\nranging from 400M to 7B parameters. Our novel training pipeline unifies\\nmultiple programming languages and transforms various code-related tasks into a\\ncommon retrieval framework, enhancing model generalizability and retrieval\\nperformance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval,\\noutperforming the previous leading model, Voyage-Code, by over 20% on CoIR\\nbenchmark. In addition to excelling in code retrieval, our models demonstrate\\ncompetitive performance on the widely adopted BeIR text retrieval benchmark,\\noffering versatility across domains. Experimental results demonstrate that\\nimproving retrieval performance significantly enhances end-to-end\\nRetrieval-Augmented Generation (RAG) performance for code-related tasks.',\n",
       "  'published': '2024-11-19T16:54:45Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.12644v2',\n",
       "  'authors': ['Ye Liu',\n",
       "   'Rui Meng',\n",
       "   'Shafiq Joty',\n",
       "   'Silvio Savarese',\n",
       "   'Caiming Xiong',\n",
       "   'Yingbo Zhou',\n",
       "   'Semih Yavuz'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.12644v2.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0042_CodeXEmbed_A_Generalist_Embedding_Model_Family_for_Multiligual_and_Multi-task_Code_Retrieval.pdf'},\n",
       " {'paper_id': 43,\n",
       "  'title': 'CUE-M: Contextual Understanding and Enhanced Search with Multimodal\\n  Large Language Model',\n",
       "  'summary': 'The integration of Retrieval-Augmented Generation (RAG) with Multimodal Large\\nLanguage Models (MLLMs) has expanded the scope of multimodal query resolution.\\nHowever, current systems struggle with intent understanding, information\\nretrieval, and safety filtering, limiting their effectiveness. This paper\\nintroduces Contextual Understanding and Enhanced Search with MLLM (CUE-M), a\\nnovel multimodal search pipeline that addresses these challenges through a\\nmulti-stage framework comprising image context enrichment, intent refinement,\\ncontextual query generation, external API integration, and relevance-based\\nfiltering. CUE-M incorporates a robust safety framework combining image-based,\\ntext-based, and multimodal classifiers, dynamically adapting to instance- and\\ncategory-specific risks. Evaluations on a multimodal Q&A dataset and a public\\nsafety benchmark demonstrate that CUE-M outperforms baselines in accuracy,\\nknowledge integration, and safety, advancing the capabilities of multimodal\\nretrieval systems.',\n",
       "  'published': '2024-11-19T07:16:48Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.12287v1',\n",
       "  'authors': ['Dongyoung Go',\n",
       "   'Taesun Whang',\n",
       "   'Chanhee Lee',\n",
       "   'Hwayeon Kim',\n",
       "   'Sunghoon Park',\n",
       "   'Seunghwan Ji',\n",
       "   'Dongchan Kim',\n",
       "   'Young-Bum Kim'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.12287v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0043_CUE-M_Contextual_Understanding_and_Enhanced_Search_with_Multimodal_Large_Language_Model.pdf'},\n",
       " {'paper_id': 44,\n",
       "  'title': 'Large Language Models for Material Property Predictions: elastic\\n  constant tensor prediction and materials design',\n",
       "  'summary': 'Efficient and accurate prediction of material properties is critical for\\nadvancing materials design and applications. The rapid-evolution of large\\nlanguage models (LLMs) presents a new opportunity for material property\\npredictions, complementing experimental measurements and multi-scale\\ncomputational methods. We focus on predicting the elastic constant tensor, as a\\ncase study, and develop domain-specific LLMs for predicting elastic constants\\nand for materials discovery. The proposed ElaTBot LLM enables simultaneous\\nprediction of elastic constant tensors, bulk modulus at finite temperatures,\\nand the generation of new materials with targeted properties. Moreover, the\\ncapabilities of ElaTBot are further enhanced by integrating with general LLMs\\n(GPT-4o) and Retrieval-Augmented Generation (RAG) for prediction. A specialized\\nvariant, ElaTBot-DFT, designed for 0 K elastic constant tensor prediction,\\nreduces the prediction errors by 33.1% compared with domain-specific, material\\nscience LLMs (Darwin) trained on the same dataset. This natural language-based\\napproach lowers the barriers to computational materials science and highlights\\nthe broader potential of LLMs for material property predictions and inverse\\ndesign.',\n",
       "  'published': '2024-11-19T07:03:19Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.12280v1',\n",
       "  'authors': ['Siyu Liu',\n",
       "   'Tongqi Wen',\n",
       "   'Beilin Ye',\n",
       "   'Zhuoyuan Li',\n",
       "   'David J. Srolovitz'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.12280v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0044_Large_Language_Models_for_Material_Property_Predictions_elastic_constant_tensor_prediction_and_materials_design.pdf'},\n",
       " {'paper_id': 45,\n",
       "  'title': 'SayComply: Grounding Field Robotic Tasks in Operational Compliance\\n  through Retrieval-Based Language Models',\n",
       "  'summary': 'This paper addresses the problem of task planning for robots that must comply\\nwith operational manuals in real-world settings. Task planning under these\\nconstraints is essential for enabling autonomous robot operation in domains\\nthat require adherence to domain-specific knowledge. Current methods for\\ngenerating robot goals and plans rely on common sense knowledge encoded in\\nlarge language models. However, these models lack grounding of robot plans to\\ndomain-specific knowledge and are not easily transferable between multiple\\nsites or customers with different compliance needs. In this work, we present\\nSayComply, which enables grounding robotic task planning with operational\\ncompliance using retrieval-based language models. We design a hierarchical\\ndatabase of operational, environment, and robot embodiment manuals and\\nprocedures to enable efficient retrieval of the relevant context under the\\nlimited context length of the LLMs. We then design a task planner using a\\ntree-based retrieval augmented generation (RAG) technique to generate robot\\ntasks that follow user instructions while simultaneously complying with the\\ndomain knowledge in the database. We demonstrate the benefits of our approach\\nthrough simulations and hardware experiments in real-world scenarios that\\nrequire precise context retrieval across various types of context,\\noutperforming the standard RAG method. Our approach bridges the gap in\\ndeploying robots that consistently adhere to operational protocols, offering a\\nscalable and edge-deployable solution for ensuring compliance across varied and\\ncomplex real-world environments. Project website: saycomply.github.io.',\n",
       "  'published': '2024-11-18T06:33:05Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.11323v1',\n",
       "  'authors': ['Muhammad Fadhil Ginting',\n",
       "   'Dong-Ki Kim',\n",
       "   'Sung-Kyun Kim',\n",
       "   'Bandi Jai Krishna',\n",
       "   'Mykel J. Kochenderfer',\n",
       "   'Shayegan Omidshafiei',\n",
       "   'Ali-akbar Agha-mohammadi'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.11323v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0045_SayComply_Grounding_Field_Robotic_Tasks_in_Operational_Compliance_through_Retrieval-Based_Language_Models.pdf'},\n",
       " {'paper_id': 46,\n",
       "  'title': 'On-Board Vision-Language Models for Personalized Autonomous Vehicle\\n  Motion Control: System Design and Real-World Validation',\n",
       "  'summary': \"Personalized driving refers to an autonomous vehicle's ability to adapt its\\ndriving behavior or control strategies to match individual users' preferences\\nand driving styles while maintaining safety and comfort standards. However,\\nexisting works either fail to capture every individual preference precisely or\\nbecome computationally inefficient as the user base expands. Vision-Language\\nModels (VLMs) offer promising solutions to this front through their natural\\nlanguage understanding and scene reasoning capabilities. In this work, we\\npropose a lightweight yet effective on-board VLM framework that provides\\nlow-latency personalized driving performance while maintaining strong reasoning\\ncapabilities. Our solution incorporates a Retrieval-Augmented Generation\\n(RAG)-based memory module that enables continuous learning of individual\\ndriving preferences through human feedback. Through comprehensive real-world\\nvehicle deployment and experiments, our system has demonstrated the ability to\\nprovide safe, comfortable, and personalized driving experiences across various\\nscenarios and significantly reduce takeover rates by up to 76.9%. To the best\\nof our knowledge, this work represents the first end-to-end VLM-based motion\\ncontrol system in real-world autonomous vehicles.\",\n",
       "  'published': '2024-11-17T23:20:37Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.11913v1',\n",
       "  'authors': ['Can Cui',\n",
       "   'Zichong Yang',\n",
       "   'Yupeng Zhou',\n",
       "   'Juntong Peng',\n",
       "   'Sung-Yeon Park',\n",
       "   'Cong Zhang',\n",
       "   'Yunsheng Ma',\n",
       "   'Xu Cao',\n",
       "   'Wenqian Ye',\n",
       "   'Yiheng Feng',\n",
       "   'Jitesh Panchal',\n",
       "   'Lingxi Li',\n",
       "   'Yaobin Chen',\n",
       "   'Ziran Wang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.11913v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0046_On-Board_Vision-Language_Models_for_Personalized_Autonomous_Vehicle_Motion_Control_System_Design_and_Real-World_Validation.pdf'},\n",
       " {'paper_id': 47,\n",
       "  'title': 'LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems\\n  Anomaly Detection',\n",
       "  'summary': 'Modern industrial infrastructures rely heavily on Cyber-Physical Systems\\n(CPS), but these are vulnerable to cyber-attacks with potentially catastrophic\\neffects. To reduce these risks, anomaly detection methods based on physical\\ninvariants have been developed. However, these methods often require\\ndomain-specific expertise to manually define invariants, making them costly and\\ndifficult to scale. To address this limitation, we propose a novel approach to\\nextract physical invariants from CPS testbeds for anomaly detection. Our\\ninsight is that CPS design documentation often contains semantically rich\\ndescriptions of physical procedures, which can profile inter-correlated\\ndynamics among system components. Leveraging the built-in physics and\\nengineering knowledge of recent generative AI models, we aim to automate this\\ntraditionally manual process, improving scalability and reducing costs. This\\nwork focuses on designing and optimizing a Retrieval-Augmented-Generation (RAG)\\nworkflow with a customized prompting system tailored for CPS documentation,\\nenabling accurate extraction of semantic information and inference of physical\\ninvariants from complex, multimodal content. Then, rather than directly\\napplying the inferred invariants for anomaly detection, we introduce an\\ninnovative statistics-based learning approach that integrates these invariants\\ninto the training dataset. This method addresses limitations such as\\nhallucination and concept drift, enhancing the reliability of the model. We\\nevaluate our approach on real-world public CPS security dataset which contains\\n86 data points and 58 attacking cases. The results show that our approach\\nachieves a high precision of 0.923, accurately detecting anomalies while\\nminimizing false alarms.',\n",
       "  'published': '2024-11-17T00:09:04Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.10918v1',\n",
       "  'authors': ['Danial Abshari', 'Chenglong Fu', 'Meera Sridhar'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.10918v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0047_LLM-assisted_Physical_Invariant_Extraction_for_Cyber-Physical_Systems_Anomaly_Detection.pdf'},\n",
       " {'paper_id': 48,\n",
       "  'title': 'Empowering Meta-Analysis: Leveraging Large Language Models for\\n  Scientific Synthesis',\n",
       "  'summary': \"This study investigates the automation of meta-analysis in scientific\\ndocuments using large language models (LLMs). Meta-analysis is a robust\\nstatistical method that synthesizes the findings of multiple studies support\\narticles to provide a comprehensive understanding. We know that a meta-article\\nprovides a structured analysis of several articles. However, conducting\\nmeta-analysis by hand is labor-intensive, time-consuming, and susceptible to\\nhuman error, highlighting the need for automated pipelines to streamline the\\nprocess. Our research introduces a novel approach that fine-tunes the LLM on\\nextensive scientific datasets to address challenges in big data handling and\\nstructured data extraction. We automate and optimize the meta-analysis process\\nby integrating Retrieval Augmented Generation (RAG). Tailored through prompt\\nengineering and a new loss metric, Inverse Cosine Distance (ICD), designed for\\nfine-tuning on large contextual datasets, LLMs efficiently generate structured\\nmeta-analysis content. Human evaluation then assesses relevance and provides\\ninformation on model performance in key metrics. This research demonstrates\\nthat fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs\\ngenerating 87.6% relevant meta-analysis abstracts. The relevance of the\\ncontext, based on human evaluation, shows a reduction in irrelevancy from 4.56%\\nto 1.9%. These experiments were conducted in a low-resource environment,\\nhighlighting the study's contribution to enhancing the efficiency and\\nreliability of meta-analysis automation.\",\n",
       "  'published': '2024-11-16T20:18:57Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.10878v1',\n",
       "  'authors': ['Jawad Ibn Ahad',\n",
       "   'Rafeed Mohammad Sultan',\n",
       "   'Abraham Kaikobad',\n",
       "   'Fuad Rahman',\n",
       "   'Mohammad Ruhul Amin',\n",
       "   'Nabeel Mohammed',\n",
       "   'Shafin Rahman'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.10878v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0048_Empowering_Meta-Analysis_Leveraging_Large_Language_Models_for_Scientific_Synthesis.pdf'},\n",
       " {'paper_id': 49,\n",
       "  'title': 'A Novel Approach to Eliminating Hallucinations in Large Language\\n  Model-Assisted Causal Discovery',\n",
       "  'summary': 'The increasing use of large language models (LLMs) in causal discovery as a\\nsubstitute for human domain experts highlights the need for optimal model\\nselection. This paper presents the first hallucination survey of popular LLMs\\nfor causal discovery. We show that hallucinations exist when using LLMs in\\ncausal discovery so the choice of LLM is important. We propose using Retrieval\\nAugmented Generation (RAG) to reduce hallucinations when quality data is\\navailable. Additionally, we introduce a novel method employing multiple LLMs\\nwith an arbiter in a debate to audit edges in causal graphs, achieving a\\ncomparable reduction in hallucinations to RAG.',\n",
       "  'published': '2024-11-16T03:06:39Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.12759v1',\n",
       "  'authors': ['Grace Sng', 'Yanming Zhang', 'Klaus Mueller'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.12759v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0049_A_Novel_Approach_to_Eliminating_Hallucinations_in_Large_Language_Model-Assisted_Causal_Discovery.pdf'},\n",
       " {'paper_id': 50,\n",
       "  'title': 'Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the\\n  AutoNuggetizer Framework',\n",
       "  'summary': 'This report provides an initial look at partial results from the TREC 2024\\nRetrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation\\nas a barrier to continued progress in information access (and more broadly,\\nnatural language processing and artificial intelligence), and it is our hope\\nthat we can contribute to tackling the many challenges in this space. The\\ncentral hypothesis we explore in this work is that the nugget evaluation\\nmethodology, originally developed for the TREC Question Answering Track in\\n2003, provides a solid foundation for evaluating RAG systems. As such, our\\nefforts have focused on \"refactoring\" this methodology, specifically applying\\nlarge language models to both automatically create nuggets and to automatically\\nassign nuggets to system answers. We call this the AutoNuggetizer framework.\\nWithin the TREC setup, we are able to calibrate our fully automatic process\\nagainst a manual process whereby nuggets are created by human assessors\\nsemi-manually and then assigned manually to system answers. Based on initial\\nresults across 21 topics from 45 runs, we observe a strong correlation between\\nscores derived from a fully automatic nugget evaluation and a (mostly) manual\\nnugget evaluation by human assessors. This suggests that our fully automatic\\nevaluation process can be used to guide future iterations of RAG systems.',\n",
       "  'published': '2024-11-14T17:25:43Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.09607v1',\n",
       "  'authors': ['Ronak Pradeep',\n",
       "   'Nandan Thakur',\n",
       "   'Shivani Upadhyay',\n",
       "   'Daniel Campos',\n",
       "   'Nick Craswell',\n",
       "   'Jimmy Lin'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.09607v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0050_Initial_Nugget_Evaluation_Results_for_the_TREC_2024_RAG_Track_with_the_AutoNuggetizer_Framework.pdf'},\n",
       " {'paper_id': 51,\n",
       "  'title': 'Adopting RAG for LLM-Aided Future Vehicle Design',\n",
       "  'summary': 'In this paper, we explore the integration of Large Language Models (LLMs)\\nwith Retrieval-Augmented Generation (RAG) to enhance automated design and\\nsoftware development in the automotive industry. We present two case studies: a\\nstandardization compliance chatbot and a design copilot, both utilizing RAG to\\nprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,\\nLLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and\\nexecution time. Our results demonstrate that while GPT-4 offers superior\\nperformance, LLAMA3 and Mistral also show promising capabilities for local\\ndeployment, addressing data privacy concerns in automotive applications. This\\nstudy highlights the potential of RAG-augmented LLMs in improving design\\nworkflows and compliance in automotive engineering.',\n",
       "  'published': '2024-11-14T17:01:24Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.09590v1',\n",
       "  'authors': ['Vahid Zolfaghari',\n",
       "   'Nenad Petrovic',\n",
       "   'Fengjunjie Pan',\n",
       "   'Krzysztof Lebioda',\n",
       "   'Alois Knoll'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.09590v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0051_Adopting_RAG_for_LLM-Aided_Future_Vehicle_Design.pdf'},\n",
       " {'paper_id': 52,\n",
       "  'title': 'Harnessing multiple LLMs for Information Retrieval: A case study on Deep\\n  Learning methodologies in Biodiversity publications',\n",
       "  'summary': 'Deep Learning (DL) techniques are increasingly applied in scientific studies\\nacross various domains to address complex research questions. However, the\\nmethodological details of these DL models are often hidden in the unstructured\\ntext. As a result, critical information about how these models are designed,\\ntrained, and evaluated is challenging to access and comprehend. To address this\\nissue, in this work, we use five different open-source Large Language Models\\n(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,\\nand Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)\\napproach to extract and process DL methodological details from scientific\\npublications automatically. We built a voting classifier from the outputs of\\nfive LLMs to accurately report DL methodological information. We tested our\\napproach using biodiversity publications, building upon our previous research.\\nTo validate our pipeline, we employed two datasets of DL-related biodiversity\\npublications: a curated set of 100 publications from our prior work and a set\\nof 364 publications from the Ecological Informatics journal. Our results\\ndemonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of\\nDL methodological information, achieving an accuracy of 69.5% (417 out of 600\\ncomparisons) based solely on textual content from publications. This\\nperformance was assessed against human annotators who had access to code,\\nfigures, tables, and other supplementary information. Although demonstrated in\\nbiodiversity, our methodology is not limited to this field; it can be applied\\nacross other scientific domains where detailed methodological reporting is\\nessential for advancing knowledge and ensuring reproducibility. This study\\npresents a scalable and reliable approach for automating information\\nextraction, facilitating better reproducibility and knowledge transfer across\\nstudies.',\n",
       "  'published': '2024-11-14T08:12:36Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.09269v1',\n",
       "  'authors': ['Vamsi Krishna Kommineni',\n",
       "   'Birgitta Knig-Ries',\n",
       "   'Sheeba Samuel'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.09269v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0052_Harnessing_multiple_LLMs_for_Information_Retrieval_A_case_study_on_Deep_Learning_methodologies_in_Biodiversity_publications.pdf'},\n",
       " {'paper_id': 53,\n",
       "  'title': 'Comprehensive and Practical Evaluation of Retrieval-Augmented Generation\\n  Systems for Medical Question Answering',\n",
       "  'summary': \"Retrieval-augmented generation (RAG) has emerged as a promising approach to\\nenhance the performance of large language models (LLMs) in knowledge-intensive\\ntasks such as those from medical domain. However, the sensitive nature of the\\nmedical domain necessitates a completely accurate and trustworthy system. While\\nexisting RAG benchmarks primarily focus on the standard retrieve-answer\\nsetting, they overlook many practical scenarios that measure crucial aspects of\\na reliable medical system. This paper addresses this gap by providing a\\ncomprehensive evaluation framework for medical question-answering (QA) systems\\nin a RAG setting for these situations, including sufficiency, integration, and\\nrobustness. We introduce Medical Retrieval-Augmented Generation Benchmark\\n(MedRGB) that provides various supplementary elements to four medical QA\\ndatasets for testing LLMs' ability to handle these specific scenarios.\\nUtilizing MedRGB, we conduct extensive evaluations of both state-of-the-art\\ncommercial LLMs and open-source models across multiple retrieval conditions.\\nOur experimental results reveals current models' limited ability to handle\\nnoise and misinformation in the retrieved documents. We further analyze the\\nLLMs' reasoning processes to provides valuable insights and future directions\\nfor developing RAG systems in this critical medical domain.\",\n",
       "  'published': '2024-11-14T06:19:18Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.09213v1',\n",
       "  'authors': ['Nghia Trung Ngo',\n",
       "   'Chien Van Nguyen',\n",
       "   'Franck Dernoncourt',\n",
       "   'Thien Huu Nguyen'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.09213v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0053_Comprehensive_and_Practical_Evaluation_of_Retrieval-Augmented_Generation_Systems_for_Medical_Question_Answering.pdf'},\n",
       " {'paper_id': 54,\n",
       "  'title': \"Practitioners' Discussions on Building LLM-based Applications for\\n  Production\",\n",
       "  'summary': '\\\\textit{Background}: Large language models (LLMs) have become a paramount\\ninterest of researchers and practitioners alike, yet a comprehensive overview\\nof key considerations for those developing LLM-based systems is lacking. This\\nstudy addresses this gap by collecting and mapping the topics practitioners\\ndiscuss online, offering practical insights into where priorities lie in\\ndeveloping LLM-based applications. \\\\textit{Method}: We collected 189 videos\\nfrom 2022 to 2024 from practitioners actively developing such systems and\\ndiscussing various aspects they encounter during development and deployment of\\nLLMs in production. We analyzed the transcripts using BERTopic, then manually\\nsorted and merged the generated topics into themes, leading to a total of 20\\ntopics in 8 themes. \\\\textit{Results}: The most prevalent topics fall within the\\ntheme Design \\\\& Architecture, with a strong focus on retrieval-augmented\\ngeneration (RAG) systems. Other frequently discussed topics include model\\ncapabilities and enhancement techniques (e.g., fine-tuning, prompt\\nengineering), infrastructure and tooling, and risks and ethical challenges.\\n\\\\textit{Implications}: Our results highlight current discussions and challenges\\nin deploying LLMs in production. This way, we provide a systematic overview of\\nkey aspects practitioners should be aware of when developing LLM-based\\napplications. We further pale off topics of interest for academics where\\nfurther research is needed.',\n",
       "  'published': '2024-11-13T12:44:41Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.08574v1',\n",
       "  'authors': ['Alina Mailach',\n",
       "   'Sebastian Simon',\n",
       "   'Johannes Dorn',\n",
       "   'Norbert Siegmund'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.08574v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0054_Practitioners_Discussions_on_Building_LLM-based_Applications_for_Production.pdf'},\n",
       " {'paper_id': 55,\n",
       "  'title': 'Building Trustworthy AI: Transparent AI Systems via Large Language\\n  Models, Ontologies, and Logical Reasoning (TranspNet)',\n",
       "  'summary': 'Growing concerns over the lack of transparency in AI, particularly in\\nhigh-stakes fields like healthcare and finance, drive the need for explainable\\nand trustworthy systems. While Large Language Models (LLMs) perform\\nexceptionally well in generating accurate outputs, their \"black box\" nature\\nposes significant challenges to transparency and trust. To address this, the\\npaper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.\\nBy leveraging domain expert knowledge, retrieval-augmented generation (RAG),\\nand formal reasoning frameworks like Answer Set Programming (ASP), TranspNet\\nenhances LLM outputs with structured reasoning and verification. This approach\\nensures that AI systems deliver not only accurate but also explainable and\\ntrustworthy results, meeting regulatory demands for transparency and\\naccountability. TranspNet provides a comprehensive solution for developing AI\\nsystems that are reliable and interpretable, making it suitable for real-world\\napplications where trust is critical.',\n",
       "  'published': '2024-11-13T09:40:37Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.08469v1',\n",
       "  'authors': ['Fadi Al Machot', 'Martin Thomas Horsch', 'Habib Ullah'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.08469v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0055_Building_Trustworthy_AI_Transparent_AI_Systems_via_Large_Language_Models_Ontologies_and_Logical_Reasoning_TranspNet.pdf'},\n",
       " {'paper_id': 56,\n",
       "  'title': 'Towards Evaluating Large Language Models for Graph Query Generation',\n",
       "  'summary': 'Large Language Models (LLMs) are revolutionizing the landscape of Generative\\nArtificial Intelligence (GenAI), with innovative LLM-backed solutions emerging\\nrapidly. However, when applied to database technologies, specifically query\\ngeneration for graph databases and Knowledge Graphs (KGs), LLMs still face\\nsignificant challenges. While research on LLM-driven query generation for\\nStructured Query Language (SQL) exists, similar systems for graph databases\\nremain underdeveloped. This paper presents a comparative study addressing the\\nchallenge of generating Cypher queries a powerful language for interacting with\\ngraph databases using open-access LLMs. We rigorously evaluate several LLM\\nagents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a\\nlocally deployed Llama 3.1 8B) using a designed few-shot learning prompt and\\nRetrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)\\nreasoning. Our empirical analysis of query generation accuracy reveals that\\nClaude Sonnet 3.5 outperforms its counterparts in this specific domain.\\nFurther, we highlight promising future research directions to address the\\nidentified limitations and advance LLM-driven query generation for graph\\ndatabases.',\n",
       "  'published': '2024-11-13T09:11:56Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.08449v2',\n",
       "  'authors': ['Siraj Munir', 'Alessandro Aldini'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.08449v2.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0056_Towards_Evaluating_Large_Language_Models_for_Graph_Query_Generation.pdf'},\n",
       " {'paper_id': 57,\n",
       "  'title': 'Towards Optimizing a Retrieval Augmented Generation using Large Language\\n  Model on Academic Data',\n",
       "  'summary': 'Given the growing trend of many organizations integrating Retrieval Augmented\\nGeneration (RAG) into their operations, we assess RAG on domain-specific data\\nand test state-of-the-art models across various optimization techniques. We\\nincorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble\\nRetriever, and In-Context-Learning, to enhance the functionality and\\nperformance in the academic domain. We focus on data retrieval, specifically\\ntargeting various study programs at a large technical university. We\\nadditionally introduce a novel evaluation approach, the RAG Confusion Matrix\\ndesigned to assess the effectiveness of various configurations within the RAG\\nframework. By exploring the integration of both open-source (e.g., Llama2,\\nMistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer\\nvaluable insights into the application and optimization of RAG frameworks in\\ndomain-specific contexts. Our experiments show a significant performance\\nincrease when including multi-query in the retrieval phase.',\n",
       "  'published': '2024-11-13T08:43:37Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.08438v1',\n",
       "  'authors': ['Anum Afzal',\n",
       "   'Juraj Vladika',\n",
       "   'Gentrit Fazlija',\n",
       "   'Andrei Staradubets',\n",
       "   'Florian Matthes'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.08438v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0057_Towards_Optimizing_a_Retrieval_Augmented_Generation_using_Large_Language_Model_on_Academic_Data.pdf'},\n",
       " {'paper_id': 58,\n",
       "  'title': 'Refining Translations with LLMs: A Constraint-Aware Iterative Prompting\\n  Approach',\n",
       "  'summary': \"Large language models (LLMs) have demonstrated remarkable proficiency in\\nmachine translation (MT), even without specific training on the languages in\\nquestion. However, translating rare words in low-resource or domain-specific\\ncontexts remains challenging for LLMs. To address this issue, we propose a\\nmulti-step prompt chain that enhances translation faithfulness by prioritizing\\nkey terms crucial for semantic accuracy. Our method first identifies these\\nkeywords and retrieves their translations from a bilingual dictionary,\\nintegrating them into the LLM's context using Retrieval-Augmented Generation\\n(RAG). We further mitigate potential output hallucinations caused by long\\nprompts through an iterative self-checking mechanism, where the LLM refines its\\ntranslations based on lexical and semantic constraints. Experiments using Llama\\nand Qwen as base models on the FLORES-200 and WMT datasets demonstrate\\nsignificant improvements over baselines, highlighting the effectiveness of our\\napproach in enhancing translation faithfulness and robustness, particularly in\\nlow-resource scenarios.\",\n",
       "  'published': '2024-11-13T05:40:24Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.08348v1',\n",
       "  'authors': ['Shangfeng Chen',\n",
       "   'Xiayang Shi',\n",
       "   'Pu Li',\n",
       "   'Yinlin Li',\n",
       "   'Jingjing Liu'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.08348v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0058_Refining_Translations_with_LLMs_A_Constraint-Aware_Iterative_Prompting_Approach.pdf'},\n",
       " {'paper_id': 59,\n",
       "  'title': 'Are LLMs Prescient? A Continuous Evaluation using Daily News as the\\n  Oracle',\n",
       "  'summary': 'Many existing evaluation benchmarks for Large Language Models (LLMs) quickly\\nbecome outdated due to the emergence of new models and training data. These\\nbenchmarks also fall short in assessing how LLM performance changes over time,\\nas they consist of static questions without a temporal dimension. To address\\nthese limitations, we propose using future event prediction as a continuous\\nevaluation method to assess LLMs\\' temporal generalization and forecasting\\nabilities. Our benchmark, Daily Oracle, automatically generates question-answer\\n(QA) pairs from daily news, challenging LLMs to predict \"future\" event\\noutcomes. Our findings reveal that as pre-training data becomes outdated, LLM\\nperformance degrades over time. While Retrieval Augmented Generation (RAG) has\\nthe potential to enhance prediction accuracy, the performance degradation\\npattern persists, highlighting the need for continuous model updates.',\n",
       "  'published': '2024-11-13T04:20:20Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.08324v1',\n",
       "  'authors': ['Hui Dai', 'Ryan Teehan', 'Mengye Ren'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.08324v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0059_Are_LLMs_Prescient_A_Continuous_Evaluation_using_Daily_News_as_the_Oracle.pdf'},\n",
       " {'paper_id': 60,\n",
       "  'title': 'A Large-Scale Study of Relevance Assessments with Large Language Models:\\n  An Initial Look',\n",
       "  'summary': 'The application of large language models to provide relevance assessments\\npresents exciting opportunities to advance information retrieval, natural\\nlanguage processing, and beyond, but to date many unknowns remain. This paper\\nreports on the results of a large-scale evaluation (the TREC 2024 RAG Track)\\nwhere four different relevance assessment approaches were deployed in situ: the\\n\"standard\" fully manual process that NIST has implemented for decades and three\\ndifferent alternatives that take advantage of LLMs to different extents using\\nthe open-source UMBRELA tool. This setup allows us to correlate system rankings\\ninduced by the different approaches to characterize tradeoffs between cost and\\nquality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system\\nrankings induced by automatically generated relevance assessments from UMBRELA\\ncorrelate highly with those induced by fully manual assessments across a\\ndiverse set of 77 runs from 19 teams. Our results suggest that automatically\\ngenerated UMBRELA judgments can replace fully manual judgments to accurately\\ncapture run-level effectiveness. Surprisingly, we find that LLM assistance does\\nnot appear to increase correlation with fully manual assessments, suggesting\\nthat costs associated with human-in-the-loop processes do not bring obvious\\ntangible benefits. Overall, human assessors appear to be stricter than UMBRELA\\nin applying relevance criteria. Our work validates the use of LLMs in academic\\nTREC-style evaluations and provides the foundation for future studies.',\n",
       "  'published': '2024-11-13T01:12:35Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.08275v1',\n",
       "  'authors': ['Shivani Upadhyay',\n",
       "   'Ronak Pradeep',\n",
       "   'Nandan Thakur',\n",
       "   'Daniel Campos',\n",
       "   'Nick Craswell',\n",
       "   'Ian Soboroff',\n",
       "   'Hoa Trang Dang',\n",
       "   'Jimmy Lin'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.08275v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0060_A_Large-Scale_Study_of_Relevance_Assessments_with_Large_Language_Models_An_Initial_Look.pdf'},\n",
       " {'paper_id': 61,\n",
       "  'title': 'Retrieval Augmented Time Series Forecasting',\n",
       "  'summary': 'Retrieval-augmented generation (RAG) is a central component of modern LLM\\nsystems, particularly in scenarios where up-to-date information is crucial for\\naccurately responding to user queries or when queries exceed the scope of the\\ntraining data. The advent of time-series foundation models (TSFM), such as\\nChronos, and the need for effective zero-shot forecasting performance across\\nvarious time-series domains motivates the question: Do benefits of RAG\\nsimilarly carry over to time series forecasting? In this paper, we advocate\\nthat the dynamic and event-driven nature of time-series data makes RAG a\\ncrucial component of TSFMs and introduce a principled RAG framework for\\ntime-series forecasting, called Retrieval Augmented Forecasting (RAF). Within\\nRAF, we develop efficient strategies for retrieving related time-series\\nexamples and incorporating them into forecast. Through experiments and\\nmechanistic studies, we demonstrate that RAF indeed improves the forecasting\\naccuracy across diverse time series domains and the improvement is more\\nsignificant for larger TSFM sizes.',\n",
       "  'published': '2024-11-12T23:55:11Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.08249v1',\n",
       "  'authors': ['Kutay Tire',\n",
       "   'Ege Onur Taga',\n",
       "   'Muhammed Emrullah Ildiz',\n",
       "   'Samet Oymak'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.08249v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0061_Retrieval_Augmented_Time_Series_Forecasting.pdf'},\n",
       " {'paper_id': 62,\n",
       "  'title': 'Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\\n  Bases and Dual Decoders',\n",
       "  'summary': 'Although people are impressed by the content generation skills of large\\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\\ngrounding of the content. The correctness and groundedness of the generated\\ncontent need to be based on a verified context, such as results from\\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\\na customized domain is that the generated responses are often incomplete, or\\nthe additions are not verified and may even be hallucinated. Prior studies on\\nhallucination detection have focused on evaluation metrics, which are not\\neasily adaptable to dynamic domains and can be vulnerable to attacks like\\njail-breaking. In this work, we propose 1) a post-processing algorithm that\\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\\ndual-decoder model that fuses RAG context to guide the generation process.',\n",
       "  'published': '2024-11-12T15:26:17Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.07870v2',\n",
       "  'authors': ['Xiaofeng Zhu', 'Jaya Krishna Mandivarapu'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.07870v2.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0062_Trustful_LLMs_Customizing_and_Grounding_Text_Generation_with_Knowledge_Bases_and_Dual_Decoders.pdf'},\n",
       " {'paper_id': 63,\n",
       "  'title': 'Query Optimization for Parametric Knowledge Refinement in\\n  Retrieval-Augmented Large Language Models',\n",
       "  'summary': 'We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel\\napproach designed to bridge the pre-retrieval information gap in\\nRetrieval-Augmented Generation (RAG) systems through query optimization\\ntailored to meet the specific knowledge requirements of Large Language Models\\n(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR\\nframework begins by extracting parametric knowledge from LLMs, followed by\\nusing a specialized query optimizer for refining these queries. This process\\nensures the retrieval of only the most pertinent information essential for\\ngenerating accurate responses. Moreover, to enhance flexibility and reduce\\ncomputational costs, we propose a trainable scheme for our pipeline that\\nutilizes a smaller, tunable model as the query optimizer, which is refined\\nthrough knowledge distillation from a larger teacher model. Our evaluations on\\nvarious question-answering (QA) datasets and with different retrieval systems\\nshow that ERRR consistently outperforms existing baselines, proving to be a\\nversatile and cost-effective module for improving the utility and accuracy of\\nRAG systems.',\n",
       "  'published': '2024-11-12T14:12:45Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.07820v2',\n",
       "  'authors': ['Youan Cong',\n",
       "   'Cheng Wang',\n",
       "   'Pritom Saha Akash',\n",
       "   'Kevin Chen-Chuan Chang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.07820v2.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0063_Query_Optimization_for_Parametric_Knowledge_Refinement_in_Retrieval-Augmented_Large_Language_Models.pdf'},\n",
       " {'paper_id': 64,\n",
       "  'title': 'Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with\\n  ImageRAG',\n",
       "  'summary': \"Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000\\n$\\\\times$ 100,000 pixels or more) poses a significant challenge for current\\nRemote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize\\nthe UHR image to standard input image size, the extensive spatial and\\ncontextual information that UHR images contain will be neglected. Otherwise,\\nthe original size of these images often exceeds the token limits of standard\\nRSMLLMs, making it difficult to process the entire image and capture long-range\\ndependencies to answer the query based on the abundant visual context. In this\\npaper, we introduce ImageRAG for RS, a training-free framework to address the\\ncomplexities of analyzing UHR remote sensing imagery. By transforming UHR\\nremote sensing image analysis task to image's long context selection task, we\\ndesign an innovative image contextual retrieval mechanism based on the\\nRetrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's\\ncore innovation lies in its ability to selectively retrieve and focus on the\\nmost relevant portions of the UHR image as visual contexts that pertain to a\\ngiven query. Fast path and slow path are proposed in this framework to handle\\nthis task efficiently and effectively. ImageRAG allows RSMLLMs to manage\\nextensive context and spatial information from UHR RSI, ensuring the analysis\\nis both accurate and efficient.\",\n",
       "  'published': '2024-11-12T10:12:12Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.07688v1',\n",
       "  'authors': ['Zilun Zhang',\n",
       "   'Haozhan Shen',\n",
       "   'Tiancheng Zhao',\n",
       "   'Yuhao Wang',\n",
       "   'Bin Chen',\n",
       "   'Yuxiang Cai',\n",
       "   'Yongheng Shang',\n",
       "   'Jianwei Yin'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.07688v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0064_Enhancing_Ultra_High_Resolution_Remote_Sensing_Imagery_Analysis_with_ImageRAG.pdf'},\n",
       " {'paper_id': 65,\n",
       "  'title': 'Toward Optimal Search and Retrieval for RAG',\n",
       "  'summary': 'Retrieval-augmented generation (RAG) is a promising method for addressing\\nsome of the memory-related challenges associated with Large Language Models\\n(LLMs). Two separate systems form the RAG pipeline, the retriever and the\\nreader, and the impact of each on downstream task performance is not\\nwell-understood. Here, we work towards the goal of understanding how retrievers\\ncan be optimized for RAG pipelines for common tasks such as Question Answering\\n(QA). We conduct experiments focused on the relationship between retrieval and\\nRAG performance on QA and attributed QA and unveil a number of insights useful\\nto practitioners developing high-performance RAG pipelines. For example,\\nlowering search accuracy has minor implications for RAG performance while\\npotentially increasing retrieval speed and memory efficiency.',\n",
       "  'published': '2024-11-11T22:06:51Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.07396v1',\n",
       "  'authors': ['Alexandria Leto',\n",
       "   'Cecilia Aguerrebere',\n",
       "   'Ishwar Bhati',\n",
       "   'Ted Willke',\n",
       "   'Mariano Tepper',\n",
       "   'Vy Ai Vo'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.07396v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0065_Toward_Optimal_Search_and_Retrieval_for_RAG.pdf'},\n",
       " {'paper_id': 66,\n",
       "  'title': 'ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are\\n  We There Yet?',\n",
       "  'summary': 'Hallucinations, the tendency to produce irrelevant/incorrect responses, are\\nprevalent concerns in generative AI-based tools like ChatGPT. Although\\nhallucinations in ChatGPT are studied for textual responses, it is unknown how\\nChatGPT hallucinates for technical texts that contain both textual and\\ntechnical terms. We surveyed 47 software engineers and produced a benchmark of\\n412 Q&A pairs from the bug reports of two OSS projects. We find that a\\nRAG-based ChatGPT (i.e., ChatGPT tuned with the benchmark issue reports) is\\n36.4% correct when producing answers to the questions, due to two reasons 1)\\nlimitations to understand complex technical contents in code snippets like\\nstack traces, and 2) limitations to integrate contexts denoted in the technical\\nterms and texts. We present CHIME (ChatGPT Inaccuracy Mitigation Engine) whose\\nunderlying principle is that if we can preprocess the technical reports better\\nand guide the query validation process in ChatGPT, we can address the observed\\nlimitations. CHIME uses context-free grammar (CFG) to parse stack traces in\\ntechnical reports. CHIME then verifies and fixes ChatGPT responses by applying\\nmetamorphic testing and query transformation. In our benchmark, CHIME shows\\n30.3% more correction over ChatGPT responses. In a user study, we find that the\\nimproved responses with CHIME are considered more useful than those generated\\nfrom ChatGPT without CHIME.',\n",
       "  'published': '2024-11-11T20:54:54Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.07360v1',\n",
       "  'authors': ['Salma Begum Tamanna',\n",
       "   'Gias Uddin',\n",
       "   'Song Wang',\n",
       "   'Lan Xia',\n",
       "   'Longyu Zhang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.07360v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0066_ChatGPT_Inaccuracy_Mitigation_during_Technical_Report_Understanding_Are_We_There_Yet.pdf'},\n",
       " {'paper_id': 67,\n",
       "  'title': 'OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model',\n",
       "  'summary': \"OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,\\nfinetuned on over 2,000,000 Thai instruction pairs. This report provides an\\nengineering perspective on the model's development, capabilities, and\\nperformance. We discuss the model's architecture, training process, and key\\nfeatures, including multi-turn conversation support, Retrieval Augmented\\nGeneration (RAG) compatibility, and tool-calling functionality. Benchmark\\nresults demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various\\nThai language tasks, outperforming other open-source Thai language models. We\\nalso address practical considerations such as GPU memory requirements and\\ndeployment strategies.\",\n",
       "  'published': '2024-11-11T18:58:46Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.07238v1',\n",
       "  'authors': ['Sumeth Yuenyong',\n",
       "   'Kobkrit Viriyayudhakorn',\n",
       "   'Apivadee Piyatumrong',\n",
       "   'Jillaphat Jaroenkantasima'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.07238v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0067_OpenThaiGPT_15_A_Thai-Centric_Open_Source_Large_Language_Model.pdf'},\n",
       " {'paper_id': 68,\n",
       "  'title': 'Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation',\n",
       "  'summary': \"Retrieval-augmented generation (RAG) has shown impressive capability in\\nproviding reliable answer predictions and addressing hallucination problems. A\\ntypical RAG implementation uses powerful retrieval models to extract external\\ninformation and large language models (LLMs) to generate answers. In contrast,\\nrecent LLM-based retrieval has gained attention for its substantial\\nimprovements in information retrieval (IR) due to the LLMs' semantic\\nunderstanding capability. However, directly applying LLM to RAG systems\\npresents challenges. This may cause feature locality problems as massive\\nparametric knowledge can hinder effective usage of global information across\\nthe corpus; for example, an LLM-based retriever often inputs document summaries\\ninstead of full documents. Moreover, various pre-trained tasks in LLMs\\nintroduce variance, further weakening performance as a retriever.\\n  To address these issues, we propose a novel two-stage fine-tuning\\narchitecture called Invar-RAG. In the retrieval stage, an LLM-based retriever\\nis constructed by integrating LoRA-based representation learning to tackle\\nfeature locality issues. To enhance retrieval performance, we develop two\\npatterns (invariant and variant patterns) and an invariance loss to reduce LLM\\nvariance. In the generation stage, a refined fine-tuning method is employed to\\nimprove LLM accuracy in generating answers based on retrieved information.\\nExperimental results show that Invar-RAG significantly outperforms existing\\nbaselines across three open-domain question answering (ODQA) datasets. Code is\\navailable in the Supplementary Material for reproducibility.\",\n",
       "  'published': '2024-11-11T14:25:37Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.07021v2',\n",
       "  'authors': ['Ziwei Liu',\n",
       "   'Liang Zhang',\n",
       "   'Qian Li',\n",
       "   'Jianghua Wu',\n",
       "   'Guangxu Zhu'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.07021v2.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0068_Invar-RAG_Invariant_LLM-aligned_Retrieval_for_Better_Generation.pdf'},\n",
       " {'paper_id': 69,\n",
       "  'title': 'AssistRAG: Boosting the Potential of Large Language Models with an\\n  Intelligent Information Assistant',\n",
       "  'summary': 'The emergence of Large Language Models (LLMs) has significantly advanced\\nnatural language processing, but these models often generate factually\\nincorrect information, known as \"hallucination\". Initial retrieval-augmented\\ngeneration (RAG) methods like the \"Retrieve-Read\" framework was inadequate for\\ncomplex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised\\nFine-Tuning (SFT) methods improved performance but required frequent retraining\\nand risked altering foundational LLM capabilities. To cope with these\\nchallenges, we propose Assistant-based Retrieval-Augmented Generation\\n(AssistRAG), integrating an intelligent information assistant within LLMs. This\\nassistant manages memory and knowledge through tool usage, action execution,\\nmemory building, and plan specification. Using a two-phase training approach,\\nCurriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG\\nenhances information retrieval and decision-making. Experiments show AssistRAG\\nsignificantly outperforms benchmarks, especially benefiting less advanced LLMs,\\nby providing superior reasoning capabilities and accurate responses.',\n",
       "  'published': '2024-11-11T09:03:52Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.06805v1',\n",
       "  'authors': ['Yujia Zhou', 'Zheng Liu', 'Zhicheng Dou'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.06805v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0069_AssistRAG_Boosting_the_Potential_of_Large_Language_Models_with_an_Intelligent_Information_Assistant.pdf'},\n",
       " {'paper_id': 70,\n",
       "  'title': 'Region-Aware Text-to-Image Generation via Hard Binding and Soft\\n  Refinement',\n",
       "  'summary': 'Regional prompting, or compositional generation, which enables fine-grained\\nspatial control, has gained increasing attention for its practicality in\\nreal-world applications. However, previous methods either introduce additional\\ntrainable modules, thus only applicable to specific models, or manipulate on\\nscore maps within cross-attention layers using attention masks, resulting in\\nlimited control strength when the number of regions increases. To handle these\\nlimitations, we present RAG, a Regional-Aware text-to-image Generation method\\nconditioned on regional descriptions for precise layout composition. RAG\\ndecouple the multi-region generation into two sub-tasks, the construction of\\nindividual region (Regional Hard Binding) that ensures the regional prompt is\\nproperly executed, and the overall detail refinement (Regional Soft Refinement)\\nover regions that dismiss the visual boundaries and enhance adjacent\\ninteractions. Furthermore, RAG novelly makes repainting feasible, where users\\ncan modify specific unsatisfied regions in the last generation while keeping\\nall other regions unchanged, without relying on additional inpainting models.\\nOur approach is tuning-free and applicable to other frameworks as an\\nenhancement to the prompt following property. Quantitative and qualitative\\nexperiments demonstrate that RAG achieves superior performance over attribute\\nbinding and object relationship than previous tuning-free methods.',\n",
       "  'published': '2024-11-10T18:45:41Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.06558v2',\n",
       "  'authors': ['Zhennan Chen',\n",
       "   'Yajie Li',\n",
       "   'Haofan Wang',\n",
       "   'Zhibo Chen',\n",
       "   'Zhengkai Jiang',\n",
       "   'Jun Li',\n",
       "   'Qian Wang',\n",
       "   'Jian Yang',\n",
       "   'Ying Tai'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.06558v2.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0070_Region-Aware_Text-to-Image_Generation_via_Hard_Binding_and_Soft_Refinement.pdf'},\n",
       " {'paper_id': 71,\n",
       "  'title': 'LProtector: An LLM-driven Vulnerability Detection System',\n",
       "  'summary': \"This paper presents LProtector, an automated vulnerability detection system\\nfor C/C++ codebases driven by the large language model (LLM) GPT-4o and\\nRetrieval-Augmented Generation (RAG). As software complexity grows, traditional\\nmethods face challenges in detecting vulnerabilities effectively. LProtector\\nleverages GPT-4o's powerful code comprehension and generation capabilities to\\nperform binary classification and identify vulnerabilities within target\\ncodebases. We conducted experiments on the Big-Vul dataset, showing that\\nLProtector outperforms two state-of-the-art baselines in terms of F1 score,\\ndemonstrating the potential of integrating LLMs with vulnerability detection.\",\n",
       "  'published': '2024-11-10T15:21:30Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.06493v2',\n",
       "  'authors': ['Ze Sheng',\n",
       "   'Fenghua Wu',\n",
       "   'Xiangwu Zuo',\n",
       "   'Chao Li',\n",
       "   'Yuxin Qiao',\n",
       "   'Lei Hang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.06493v2.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0071_LProtector_An_LLM-driven_Vulnerability_Detection_System.pdf'},\n",
       " {'paper_id': 72,\n",
       "  'title': 'Leveraging Retrieval-Augmented Generation for University Knowledge\\n  Retrieval',\n",
       "  'summary': 'This paper introduces an innovative approach using Retrieval-Augmented\\nGeneration (RAG) pipelines with Large Language Models (LLMs) to enhance\\ninformation retrieval and query response systems for university-related\\nquestion answering. By systematically extracting data from the university\\nofficial webpage and employing advanced prompt engineering techniques, we\\ngenerate accurate, contextually relevant responses to user queries.\\n  We developed a comprehensive university benchmark, UniversityQuestionBench\\n(UQB), to rigorously evaluate our system performance, based on common key\\nmetrics in the filed of RAG pipelines, assessing accuracy and reliability\\nthrough various metrics and real-world scenarios. Our experimental results\\ndemonstrate significant improvements in the precision and relevance of\\ngenerated responses, enhancing user experience and reducing the time required\\nto obtain relevant answers. In summary, this paper presents a novel application\\nof RAG pipelines and LLMs, supported by a meticulously prepared university\\nbenchmark, offering valuable insights into advanced AI techniques for academic\\ndata retrieval and setting the stage for future research in this domain.',\n",
       "  'published': '2024-11-09T17:38:01Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.06237v1',\n",
       "  'authors': ['Arshia Hemmat',\n",
       "   'Kianoosh Vadaei',\n",
       "   'Mohammad Hassan Heydari',\n",
       "   'Afsaneh Fatemi'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.06237v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0072_Leveraging_Retrieval-Augmented_Generation_for_University_Knowledge_Retrieval.pdf'},\n",
       " {'paper_id': 73,\n",
       "  'title': 'Exploring Knowledge Boundaries in Large Language Models for Retrieval\\n  Judgment',\n",
       "  'summary': 'Large Language Models (LLMs) are increasingly recognized for their practical\\napplications. However, these models often encounter challenges in dynamically\\nchanging knowledge, as well as in managing unknown static knowledge.\\nRetrieval-Augmented Generation (RAG) tackles this challenge and has shown a\\nsignificant impact on LLMs. Actually, we find that the impact of RAG on the\\nquestion answering capabilities of LLMs can be categorized into three groups:\\nbeneficial, neutral, and harmful. By minimizing retrieval requests that yield\\nneutral or harmful results, we can effectively reduce both time and\\ncomputational costs, while also improving the overall performance of LLMs. This\\ninsight motivates us to differentiate between types of questions using certain\\nmetrics as indicators, to decrease the retrieval ratio without compromising\\nperformance. In our work, we propose a method that is able to identify\\ndifferent types of questions from this view by training a Knowledge Boundary\\nModel (KBM). Experiments conducted on 11 English and Chinese datasets\\nillustrate that the KBM effectively delineates the knowledge boundary,\\nsignificantly decreasing the proportion of retrievals required for optimal\\nend-to-end performance. Specifically, we evaluate the effectiveness of KBM in\\nthree complex scenarios: dynamic knowledge, long-tail static knowledge, and\\nmulti-hop problems, as well as its functionality as an external LLM plug-in.',\n",
       "  'published': '2024-11-09T15:12:28Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.06207v1',\n",
       "  'authors': ['Zhen Zhang',\n",
       "   'Xinyu Wang',\n",
       "   'Yong Jiang',\n",
       "   'Zhuo Chen',\n",
       "   'Feiteng Mu',\n",
       "   'Mengting Hu',\n",
       "   'Pengjun Xie',\n",
       "   'Fei Huang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.06207v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0073_Exploring_Knowledge_Boundaries_in_Large_Language_Models_for_Retrieval_Judgment.pdf'},\n",
       " {'paper_id': 74,\n",
       "  'title': 'Clustering Algorithms and RAG Enhancing Semi-Supervised Text\\n  Classification with Large LLMs',\n",
       "  'summary': 'This paper introduces an innovative semi-supervised learning approach for\\ntext classification, addressing the challenge of abundant data but limited\\nlabeled examples. Our methodology integrates few-shot learning with\\nretrieval-augmented generation (RAG) and conventional statistical clustering,\\nenabling effective learning from a minimal number of labeled instances while\\ngenerating high-quality labeled data. To the best of our knowledge, we are the\\nfirst to incorporate RAG alongside clustering in text data generation. Our\\nexperiments on the Reuters and Web of Science datasets demonstrate\\nstate-of-the-art performance, with few-shot augmented data alone producing\\nresults nearly equivalent to those achieved with fully labeled datasets.\\nNotably, accuracies of 95.41\\\\% and 82.43\\\\% were achieved for complex text\\ndocument classification tasks, where the number of categories can exceed 100.',\n",
       "  'published': '2024-11-09T13:17:39Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.06175v1',\n",
       "  'authors': ['Shan Zhong', 'Jiahao Zeng', 'Yongxin Yu', 'Bohong Lin'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.06175v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0074_Clustering_Algorithms_and_RAG_Enhancing_Semi-Supervised_Text_Classification_with_Large_LLMs.pdf'},\n",
       " {'paper_id': 75,\n",
       "  'title': 'Sufficient Context: A New Lens on Retrieval Augmented Generation Systems',\n",
       "  'summary': 'Augmenting LLMs with context leads to improved performance across many\\napplications. Despite much research on Retrieval Augmented Generation (RAG)\\nsystems, an open question is whether errors arise because LLMs fail to utilize\\nthe context from retrieval or the context itself is insufficient to answer the\\nquery. To shed light on this, we develop a new notion of sufficient context,\\nalong with a way to classify instances that have enough information to answer\\nthe query. We then use sufficient context to analyze several models and\\ndatasets. By stratifying errors based on context sufficiency, we find that\\nproprietary LLMs (Gemini, GPT, Claude) excel at answering queries when the\\ncontext is sufficient, but often output incorrect answers instead of abstaining\\nwhen the context is not. On the other hand, open-source LLMs (Llama, Mistral,\\nGemma) hallucinate or abstain often, even with sufficient context. We further\\ncategorize cases when the context is useful, and improves accuracy, even though\\nit does not fully answer the query and the model errs without the context.\\nBuilding on our findings, we explore ways to reduce hallucinations in RAG\\nsystems, including a new selective generation method that leverages sufficient\\ncontext information for guided abstention. Our method improves the fraction of\\ncorrect answers among times where the model responds by 2-10% for Gemini, GPT,\\nand Gemma.',\n",
       "  'published': '2024-11-09T02:13:14Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.06037v1',\n",
       "  'authors': ['Hailey Joren',\n",
       "   'Jianyi Zhang',\n",
       "   'Chun-Sung Ferng',\n",
       "   'Da-Cheng Juan',\n",
       "   'Ankur Taly',\n",
       "   'Cyrus Rashtchian'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.06037v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0075_Sufficient_Context_A_New_Lens_on_Retrieval_Augmented_Generation_Systems.pdf'},\n",
       " {'paper_id': 76,\n",
       "  'title': 'Multi-Document Financial Question Answering using LLMs',\n",
       "  'summary': 'We propose two new methods for multi-document financial question answering.\\nFirst, a method that uses semantic tagging, and then, queries the index to get\\nthe context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that\\nuses semantic tagging, and, retrieves knowledge graph triples from a graph\\ndatabase, as context. KG_RAG uses knowledge graphs constructed using a small\\nmodel that is fine-tuned using knowledge distillation using a large teacher\\nmodel. The data consists of 18 10K reports of Apple, Microsoft, Alphabet,\\nNVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of\\nquestions in the data consists of 111 complex questions including many esoteric\\nquestions that are difficult to answer and the answers are not completely\\nobvious. As evaluation metrics, we use overall scores as well as segmented\\nscores for measurement including the faithfulness, relevance, correctness,\\nsimilarity, an LLM based overall score and the rouge scores as well as a\\nsimilarity of embeddings. We find that both methods outperform plain RAG\\nsignificantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.',\n",
       "  'published': '2024-11-08T21:03:54Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.07264v1',\n",
       "  'authors': ['Shalin Shah', 'Srikanth Ryali', 'Ramasubbu Venkatesh'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.07264v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0076_Multi-Document_Financial_Question_Answering_using_LLMs.pdf'},\n",
       " {'paper_id': 77,\n",
       "  'title': 'Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for\\n  Bengali Mathematical Olympiad Problem Solving',\n",
       "  'summary': 'We present an innovative approach for solving mathematical problems in\\nBengali, developed for the DL Sprint 3.0 BUET CSE Fest 2024 Competition. Our\\nmethod uses advanced deep learning models, notably the Qwen 2.5 series, with\\nimprovements made through prompt engineering, model quantization, and Tool\\nIntegrated Reasoning (TIR) to handle complex calculations. Initially, we\\nexplored various model architectures, including fine-tuned Mistral and\\nquantized Qwen models, refining them with translation techniques,\\nRetrieval-Augmented Generation (RAG), and custom dataset curation. Manual\\nhyperparameter tuning optimized parameters like temperature and top-p to\\nenhance model adaptability and accuracy. Removal of RAG and parameter\\nadjustments further improved robustness. Our approach highlights the potential\\nof advanced NLP techniques in solving Bengali mathematical problems.',\n",
       "  'published': '2024-11-08T19:44:12Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.05934v1',\n",
       "  'authors': ['Saad Tahmid', 'Sourav Sarker'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.05934v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0077_Qwen25-32B_Leveraging_Self-Consistent_Tool-Integrated_Reasoning_for_Bengali_Mathematical_Olympiad_Problem_Solving.pdf'},\n",
       " {'paper_id': 78,\n",
       "  'title': 'FinDVer: Explainable Claim Verification over Long and Hybrid-Content\\n  Financial Documents',\n",
       "  'summary': 'We introduce FinDVer, a comprehensive benchmark specifically designed to\\nevaluate the explainable claim verification capabilities of LLMs in the context\\nof understanding and analyzing long, hybrid-content financial documents.\\nFinDVer contains 2,400 expert-annotated examples, divided into three subsets:\\ninformation extraction, numerical reasoning, and knowledge-intensive reasoning,\\neach addressing common scenarios encountered in real-world financial contexts.\\nWe assess a broad spectrum of LLMs under long-context and RAG settings. Our\\nresults show that even the current best-performing system, GPT-4o, still lags\\nbehind human experts. We further provide in-depth analysis on long-context and\\nRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering\\ninsights to drive future advancements. We believe that FinDVer can serve as a\\nvaluable benchmark for evaluating LLMs in claim verification over complex,\\nexpert-domain documents.',\n",
       "  'published': '2024-11-08T18:26:17Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.05764v1',\n",
       "  'authors': ['Yilun Zhao',\n",
       "   'Yitao Long',\n",
       "   'Yuru Jiang',\n",
       "   'Chengye Wang',\n",
       "   'Weiyuan Chen',\n",
       "   'Hongjun Liu',\n",
       "   'Yiming Zhang',\n",
       "   'Xiangru Tang',\n",
       "   'Chen Zhao',\n",
       "   'Arman Cohan'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.05764v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0078_FinDVer_Explainable_Claim_Verification_over_Long_and_Hybrid-Content_Financial_Documents.pdf'},\n",
       " {'paper_id': 79,\n",
       "  'title': 'IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge\\n  Delivery',\n",
       "  'summary': 'In the rapidly evolving landscape of cyber security, intelligent chatbots are\\ngaining prominence. Artificial Intelligence, Machine Learning, and Natural\\nLanguage Processing empower these chatbots to handle user inquiries and deliver\\nthreat intelligence. This helps cyber security knowledge readily available to\\nboth professionals and the public. Traditional rule-based chatbots often lack\\nflexibility and struggle to adapt to user interactions. In contrast, Large\\nLanguage Model-based chatbots offer contextually relevant information across\\nmultiple domains and adapt to evolving conversational contexts. In this work,\\nwe develop IntellBot, an advanced cyber security Chatbot built on top of\\ncutting-edge technologies like Large Language Models and Langchain alongside a\\nRetrieval-Augmented Generation model to deliver superior capabilities. This\\nchatbot gathers information from diverse data sources to create a comprehensive\\nknowledge base covering known vulnerabilities, recent cyber attacks, and\\nemerging threats. It delivers tailored responses, serving as a primary hub for\\ncyber security insights. By providing instant access to relevant information\\nand resources, this IntellBot enhances threat intelligence, incident response,\\nand overall security posture, saving time and empowering users with knowledge\\nof cyber security best practices. Moreover, we analyzed the performance of our\\ncopilot using a two-stage evaluation strategy. We achieved BERT score above 0.8\\nby indirect approach and a cosine similarity score ranging from 0.8 to 1, which\\naffirms the accuracy of our copilot. Additionally, we utilized RAGAS to\\nevaluate the RAG model, and all evaluation metrics consistently produced scores\\nabove 0.77, highlighting the efficacy of our system.',\n",
       "  'published': '2024-11-08T09:40:53Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.05442v1',\n",
       "  'authors': ['Dincy R. Arikkat',\n",
       "   'Abhinav M.',\n",
       "   'Navya Binu',\n",
       "   'Parvathi M.',\n",
       "   'Navya Biju',\n",
       "   'K. S. Arunima',\n",
       "   'Vinod P.',\n",
       "   'Rafidha Rehiman K. A.',\n",
       "   'Mauro Conti'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.05442v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0079_IntellBot_Retrieval_Augmented_LLM_Chatbot_for_Cyber_Threat_Knowledge_Delivery.pdf'},\n",
       " {'paper_id': 80,\n",
       "  'title': 'Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent\\n  Cluster Diagnosis System and Evaluation Framework',\n",
       "  'summary': 'Recent advancements in Large Language Models (LLMs) and related technologies\\nsuch as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have\\nenabled the creation of autonomous intelligent systems capable of performing\\ncluster diagnostics and troubleshooting. By integrating these technologies with\\nself-play methodologies, we have developed an LLM-agent system designed to\\nautonomously diagnose and resolve issues within AI clusters. Our innovations\\ninclude a knowledge base tailored for cluster diagnostics, enhanced LLM\\nalgorithms, practical deployment strategies for agents, and a benchmark\\nspecifically designed for evaluating LLM capabilities in this domain. Through\\nextensive experimentation across multiple dimensions, we have demonstrated the\\nsuperiority of our system in addressing the challenges faced in cluster\\ndiagnostics, particularly in detecting and rectifying performance issues more\\nefficiently and accurately than traditional methods.',\n",
       "  'published': '2024-11-08T06:12:56Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.05349v1',\n",
       "  'authors': ['Honghao Shi',\n",
       "   'Longkai Cheng',\n",
       "   'Wenli Wu',\n",
       "   'Yuhang Wang',\n",
       "   'Xuan Liu',\n",
       "   'Shaokai Nie',\n",
       "   'Weixv Wang',\n",
       "   'Xuebin Min',\n",
       "   'Chunlei Men',\n",
       "   'Yonghua Lin'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.05349v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0080_Enhancing_Cluster_Resilience_LLM-agent_Based_Autonomous_Intelligent_Cluster_Diagnosis_System_and_Evaluation_Framework.pdf'},\n",
       " {'paper_id': 81,\n",
       "  'title': 'A Taxonomy of AgentOps for Enabling Observability of Foundation Model\\n  based Agents',\n",
       "  'summary': 'The ever-improving quality of LLMs has fueled the growth of a diverse range\\nof downstream tasks, leading to an increased demand for AI automation and a\\nburgeoning interest in developing foundation model (FM)-based autonomous\\nagents. As AI agent systems tackle more complex tasks and evolve, they involve\\na wider range of stakeholders, including agent users, agentic system developers\\nand deployers, and AI model developers. These systems also integrate multiple\\ncomponents such as AI agent workflows, RAG pipelines, prompt management, agent\\ncapabilities, and observability features. In this case, obtaining reliable\\noutputs and answers from these agents remains challenging, necessitating a\\ndependable execution process and end-to-end observability solutions. To build\\nreliable AI agents and LLM applications, it is essential to shift towards\\ndesigning AgentOps platforms that ensure observability and traceability across\\nthe entire development-to-production life-cycle. To this end, we conducted a\\nrapid review and identified relevant AgentOps tools from the agentic ecosystem.\\nBased on this review, we provide an overview of the essential features of\\nAgentOps and propose a comprehensive overview of observability data/traceable\\nartifacts across the agent production life-cycle. Our findings provide a\\nsystematic overview of the current AgentOps landscape, emphasizing the critical\\nrole of observability/traceability in enhancing the reliability of autonomous\\nagent systems.',\n",
       "  'published': '2024-11-08T02:31:03Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.05285v1',\n",
       "  'authors': ['Liming Dong', 'Qinghua Lu', 'Liming Zhu'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.05285v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0081_A_Taxonomy_of_AgentOps_for_Enabling_Observability_of_Foundation_Model_based_Agents.pdf'},\n",
       " {'paper_id': 82,\n",
       "  'title': 'Deploying Large Language Models With Retrieval Augmented Generation',\n",
       "  'summary': \"Knowing that the generative capabilities of large language models (LLM) are\\nsometimes hampered by tendencies to hallucinate or create non-factual\\nresponses, researchers have increasingly focused on methods to ground generated\\noutputs in factual data. Retrieval Augmented Generation (RAG) has emerged as a\\nkey approach for integrating knowledge from data sources outside of the LLM's\\ntraining set, including proprietary and up-to-date information. While many\\nresearch papers explore various RAG strategies, their true efficacy is tested\\nin real-world applications with actual data. The journey from conceiving an\\nidea to actualizing it in the real world is a lengthy process. We present\\ninsights from the development and field-testing of a pilot project that\\nintegrates LLMs with RAG for information retrieval. Additionally, we examine\\nthe impacts on the information value chain, encompassing people, processes, and\\ntechnology. Our aim is to identify the opportunities and challenges of\\nimplementing this emerging technology, particularly within the context of\\nbehavioral research in the information systems (IS) field. The contributions of\\nthis work include the development of best practices and recommendations for\\nadopting this promising technology while ensuring compliance with industry\\nregulations through a proposed AI governance model.\",\n",
       "  'published': '2024-11-07T22:11:51Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.11895v1',\n",
       "  'authors': ['Sonal Prabhune', 'Donald J. Berndt'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.11895v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0082_Deploying_Large_Language_Models_With_Retrieval_Augmented_Generation.pdf'},\n",
       " {'paper_id': 83,\n",
       "  'title': 'PentestAgent: Incorporating LLM Agents to Automated Penetration Testing',\n",
       "  'summary': 'Penetration testing is a critical technique for identifying security\\nvulnerabilities, traditionally performed manually by skilled security\\nspecialists. This complex process involves gathering information about the\\ntarget system, identifying entry points, exploiting the system, and reporting\\nfindings. Despite its effectiveness, manual penetration testing is\\ntime-consuming and expensive, often requiring significant expertise and\\nresources that many organizations cannot afford. While automated penetration\\ntesting methods have been proposed, they often fall short in real-world\\napplications due to limitations in flexibility, adaptability, and\\nimplementation.\\n  Recent advancements in large language models (LLMs) offer new opportunities\\nfor enhancing penetration testing through increased intelligence and\\nautomation. However, current LLM-based approaches still face significant\\nchallenges, including limited penetration testing knowledge and a lack of\\ncomprehensive automation capabilities. To address these gaps, we propose\\nPentestAgent, a novel LLM-based automated penetration testing framework that\\nleverages the power of LLMs and various LLM-based techniques like Retrieval\\nAugmented Generation (RAG) to enhance penetration testing knowledge and\\nautomate various tasks. Our framework leverages multi-agent collaboration to\\nautomate intelligence gathering, vulnerability analysis, and exploitation\\nstages, reducing manual intervention. We evaluate PentestAgent using a\\ncomprehensive benchmark, demonstrating superior performance in task completion\\nand overall efficiency. This work significantly advances the practical\\napplicability of automated penetration testing systems.',\n",
       "  'published': '2024-11-07T21:10:39Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.05185v1',\n",
       "  'authors': ['Xiangmin Shen',\n",
       "   'Lingzhi Wang',\n",
       "   'Zhenyuan Li',\n",
       "   'Yan Chen',\n",
       "   'Wencheng Zhao',\n",
       "   'Dawei Sun',\n",
       "   'Jiashui Wang',\n",
       "   'Wei Ruan'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.05185v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0083_PentestAgent_Incorporating_LLM_Agents_to_Automated_Penetration_Testing.pdf'},\n",
       " {'paper_id': 84,\n",
       "  'title': 'Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with\\n  Retrieval-Augmented Generation',\n",
       "  'summary': 'Current leading Text-To-Audio (TTA) generation models suffer from degraded\\nperformance on zero-shot and few-shot settings. It is often challenging to\\ngenerate high-quality audio for audio events that are unseen or uncommon in the\\ntraining set. Inspired by the success of Retrieval-Augmented Generation (RAG)\\nin Large Language Model (LLM)-based knowledge-intensive tasks, we extend the\\nTTA process with additional conditioning contexts. We propose Audiobox TTA-RAG,\\na novel retrieval-augmented TTA approach based on Audiobox, a conditional\\nflow-matching audio generation model. Unlike the vanilla Audiobox TTA solution\\nwhich generates audio conditioned on text, we augmented the conditioning input\\nwith retrieved audio samples that provide additional acoustic information to\\ngenerate the target audio. Our retrieval method does not require the external\\ndatabase to have labeled audio, offering more practical use cases. To evaluate\\nour proposed method, we curated test sets in zero-shot and few-shot settings.\\nOur empirical results show that the proposed model can effectively leverage the\\nretrieved audio samples and significantly improve zero-shot and few-shot TTA\\nperformance, with large margins on multiple evaluation metrics, while\\nmaintaining the ability to generate semantically aligned audio for the\\nin-domain setting. In addition, we investigate the effect of different\\nretrieval methods and data sources.',\n",
       "  'published': '2024-11-07T19:50:28Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.05141v1',\n",
       "  'authors': ['Mu Yang',\n",
       "   'Bowen Shi',\n",
       "   'Matthew Le',\n",
       "   'Wei-Ning Hsu',\n",
       "   'Andros Tjandra'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.05141v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0084_Audiobox_TTA-RAG_Improving_Zero-Shot_and_Few-Shot_Text-To-Audio_with_Retrieval-Augmented_Generation.pdf'},\n",
       " {'paper_id': 85,\n",
       "  'title': 'M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page\\n  Multi-document Understanding',\n",
       "  'summary': 'Document visual question answering (DocVQA) pipelines that answer questions\\nfrom documents have broad applications. Existing methods focus on handling\\nsingle-page documents with multi-modal language models (MLMs), or rely on\\ntext-based retrieval-augmented generation (RAG) that uses text extraction tools\\nsuch as optical character recognition (OCR). However, there are difficulties in\\napplying these methods in real-world scenarios: (a) questions often require\\ninformation across different pages or documents, where MLMs cannot handle many\\nlong documents; (b) documents often have important information in visual\\nelements such as figures, but text extraction tools ignore them. We introduce\\nM3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various\\ndocument contexts (closed-domain and open-domain), question hops (single-hop\\nand multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG\\nfinds relevant documents and answers questions using a multi-modal retriever\\nand an MLM, so that it can efficiently handle single or many documents while\\npreserving visual information. Since previous DocVQA datasets ask questions in\\nthe context of a specific document, we also present M3DocVQA, a new benchmark\\nfor evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.\\nIn three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results\\nshow that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance\\nthan many strong baselines, including state-of-the-art performance in\\nMP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and\\nretrieval models. Lastly, we qualitatively show that M3DocRAG can successfully\\nhandle various scenarios, such as when relevant information exists across\\nmultiple pages and when answer evidence only exists in images.',\n",
       "  'published': '2024-11-07T18:29:38Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.04952v1',\n",
       "  'authors': ['Jaemin Cho',\n",
       "   'Debanjan Mahata',\n",
       "   'Ozan Irsoy',\n",
       "   'Yujie He',\n",
       "   'Mohit Bansal'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.04952v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0085_M3DocRAG_Multi-modal_Retrieval_is_What_You_Need_for_Multi-page_Multi-document_Understanding.pdf'},\n",
       " {'paper_id': 86,\n",
       "  'title': 'LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation\\n  Combining Hierarchical Agents and RAG',\n",
       "  'summary': \"The increasing use of smart devices has emphasized the critical role of\\nmaintenance in production activities. Interactive Electronic Technical Manuals\\n(IETMs) are vital tools that support the maintenance of smart equipment.\\nHowever, traditional IETMs face challenges such as transitioning from Graphical\\nUser Interfaces (GUIs) to natural Language User Interfaces (LUIs) and managing\\ncomplex logical relationships. Additionally, they must meet the current demands\\nfor higher intelligence. This paper proposes a Maintenance Scheme Generation\\nMethod based on Large Language Models (LLM-R). The proposed method includes\\nseveral key innovations: We propose the Low Rank Adaptation-Knowledge Retention\\n(LORA-KR) loss technology to proportionally adjust mixed maintenance data for\\nfine-tuning the LLM. This method prevents knowledge conflicts caused by mixed\\ndata, improving the model's adaptability and reasoning ability in specific\\nmaintenance domains, Besides, Hierarchical Task-Based Agent and\\nInstruction-level Retrieval-Augmented Generation (RAG) technologies are adopted\\nto optimize the generation steps and mitigate the phenomenon of hallucination\\ncaused by the model's Inability to access contextual information. This\\nenhancement improves the model's flexibility and accuracy in handling known or\\nunknown maintenance objects and maintenance scheme scenarios. To validate the\\nproposed method's effectiveness in maintenance tasks, a maintenance scheme\\ndataset was constructed using objects from different fields. The experimental\\nresults show that the accuracy of the maintenance schemes generated by the\\nproposed method reached 91.59%, indicating which improvement enhances the\\nintelligence of maintenance schemes and introduces novel technical approaches\\nfor equipment maintenance.\",\n",
       "  'published': '2024-11-07T07:07:34Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.04476v1',\n",
       "  'authors': ['Laifa Tao',\n",
       "   'Qixuan Huang',\n",
       "   'Xianjun Wu',\n",
       "   'Weiwei Zhang',\n",
       "   'Yunlong Wu',\n",
       "   'Bin Li',\n",
       "   'Chen Lu',\n",
       "   'Xingshuo Hai'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.04476v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0086_LLM-R_A_Framework_for_Domain-Adaptive_Maintenance_Scheme_Generation_Combining_Hierarchical_Agents_and_RAG.pdf'},\n",
       " {'paper_id': 87,\n",
       "  'title': 'ML-Promise: A Multilingual Dataset for Corporate Promise Verification',\n",
       "  'summary': 'Promises made by politicians, corporate leaders, and public figures have a\\nsignificant impact on public perception, trust, and institutional reputation.\\nHowever, the complexity and volume of such commitments, coupled with\\ndifficulties in verifying their fulfillment, necessitate innovative methods for\\nassessing their credibility. This paper introduces the concept of Promise\\nVerification, a systematic approach involving steps such as promise\\nidentification, evidence assessment, and the evaluation of timing for\\nverification. We propose the first multilingual dataset, ML-Promise, which\\nincludes English, French, Chinese, Japanese, and Korean, aimed at facilitating\\nin-depth verification of promises, particularly in the context of\\nEnvironmental, Social, and Governance (ESG) reports. Given the growing emphasis\\non corporate environmental contributions, this dataset addresses the challenge\\nof evaluating corporate promises, especially in light of practices like\\ngreenwashing. Our findings also explore textual and image-based baselines, with\\npromising results from retrieval-augmented generation (RAG) approaches. This\\nwork aims to foster further discourse on the accountability of public\\ncommitments across multiple languages and domains.',\n",
       "  'published': '2024-11-07T06:51:24Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.04473v1',\n",
       "  'authors': ['Yohei Seki',\n",
       "   'Hakusen Shu',\n",
       "   'Anas Lhuissier',\n",
       "   'Hanwool Lee',\n",
       "   'Juyeon Kang',\n",
       "   'Min-Yuh Day',\n",
       "   'Chung-Chi Chen'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.04473v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0087_ML-Promise_A_Multilingual_Dataset_for_Corporate_Promise_Verification.pdf'},\n",
       " {'paper_id': 88,\n",
       "  'title': 'GPT-Guided Monte Carlo Tree Search for Symbolic Regression in Financial\\n  Fraud Detection',\n",
       "  'summary': 'With the increasing number of financial services available online, the rate\\nof financial fraud has also been increasing. The traffic and transaction rates\\non the internet have increased considerably, leading to a need for fast\\ndecision-making. Financial institutions also have stringent regulations that\\noften require transparency and explainability of the decision-making process.\\nHowever, most state-of-the-art algorithms currently used in the industry are\\nhighly parameterized black-box models that rely on complex computations to\\ngenerate a score. These algorithms are inherently slow and lack the\\nexplainability and speed of traditional rule-based learners. This work\\nintroduces SR-MCTS (Symbolic Regression MCTS), which utilizes a foundational\\nGPT model to guide the MCTS, significantly enhancing its convergence speed and\\nthe quality of the generated expressions which are further extracted to rules.\\nOur experiments show that SR-MCTS can detect fraud more efficiently than widely\\nused methods in the industry while providing substantial insights into the\\ndecision-making process.',\n",
       "  'published': '2024-11-07T06:12:38Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.04459v1',\n",
       "  'authors': ['Prashank Kadam'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.04459v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0088_GPT-Guided_Monte_Carlo_Tree_Search_for_Symbolic_Regression_in_Financial_Fraud_Detection.pdf'},\n",
       " {'paper_id': 89,\n",
       "  'title': 'AMSnet-KG: A Netlist Dataset for LLM-based AMS Circuit Auto-Design Using\\n  Knowledge Graph RAG',\n",
       "  'summary': 'High-performance analog and mixed-signal (AMS) circuits are mainly\\nfull-custom designed, which is time-consuming and labor-intensive. A\\nsignificant portion of the effort is experience-driven, which makes the\\nautomation of AMS circuit design a formidable challenge. Large language models\\n(LLMs) have emerged as powerful tools for Electronic Design Automation (EDA)\\napplications, fostering advancements in the automatic design process for\\nlarge-scale AMS circuits. However, the absence of high-quality datasets has led\\nto issues such as model hallucination, which undermines the robustness of\\nautomatically generated circuit designs. To address this issue, this paper\\nintroduces AMSnet-KG, a dataset encompassing various AMS circuit schematics and\\nnetlists. We construct a knowledge graph with annotations on detailed\\nfunctional and performance characteristics. Facilitated by AMSnet-KG, we\\npropose an automated AMS circuit generation framework that utilizes the\\ncomprehensive knowledge embedded in LLMs. We first formulate a design strategy\\n(e.g., circuit architecture using a number of circuit components) based on\\nrequired specifications. Next, matched circuit components are retrieved and\\nassembled into a complete topology, and transistor sizing is obtained through\\nBayesian optimization. Simulation results of the netlist are fed back to the\\nLLM for further topology refinement, ensuring the circuit design specifications\\nare met. We perform case studies of operational amplifier and comparator design\\nto verify the automatic design flow from specifications to netlists with\\nminimal human effort. The dataset used in this paper will be open-sourced upon\\npublishing of this paper.',\n",
       "  'published': '2024-11-07T02:49:53Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.13560v1',\n",
       "  'authors': ['Yichen Shi',\n",
       "   'Zhuofu Tao',\n",
       "   'Yuhao Gao',\n",
       "   'Tianjia Zhou',\n",
       "   'Cheng Chang',\n",
       "   'Yaxing Wang',\n",
       "   'Bingyu Chen',\n",
       "   'Genhao Zhang',\n",
       "   'Alvin Liu',\n",
       "   'Zhiping Yu',\n",
       "   'Ting-Jung Lin',\n",
       "   'Lei He'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.13560v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0089_AMSnet-KG_A_Netlist_Dataset_for_LLM-based_AMS_Circuit_Auto-Design_Using_Knowledge_Graph_RAG.pdf'},\n",
       " {'paper_id': 90,\n",
       "  'title': 'Enhancing classroom teaching with LLMs and RAG',\n",
       "  'summary': 'Large Language Models have become a valuable source of information for our\\ndaily inquiries. However, after training, its data source quickly becomes\\nout-of-date, making RAG a useful tool for providing even more recent or\\npertinent data. In this work, we investigate how RAG pipelines, with the course\\nmaterials serving as a data source, might help students in K-12 education. The\\ninitial research utilizes Reddit as a data source for up-to-date cybersecurity\\ninformation. Chunk size is evaluated to determine the optimal amount of context\\nneeded to generate accurate answers. After running the experiment for different\\nchunk sizes, answer correctness was evaluated using RAGAs with average answer\\ncorrectness not exceeding 50 percent for any chunk size. This suggests that\\nReddit is not a good source to mine for data for questions about cybersecurity\\nthreats. The methodology was successful in evaluating the data source, which\\nhas implications for its use to evaluate educational resources for\\neffectiveness.',\n",
       "  'published': '2024-11-07T00:39:34Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.04341v1',\n",
       "  'authors': ['Elizabeth A Mullins',\n",
       "   'Adrian Portillo',\n",
       "   'Kristalys Ruiz-Rohena',\n",
       "   'Aritran Piplai'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.04341v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0090_Enhancing_classroom_teaching_with_LLMs_and_RAG.pdf'},\n",
       " {'paper_id': 91,\n",
       "  'title': 'LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\\n  for Design Space Exploration',\n",
       "  'summary': 'GraphRAG addresses significant challenges in Retrieval-Augmented Generation\\n(RAG) by leveraging graphs with embedded knowledge to enhance the reasoning\\ncapabilities of Large Language Models (LLMs). Despite its promising potential,\\nthe GraphRAG community currently lacks a unified framework for fine-grained\\ndecomposition of the graph-based knowledge retrieval process. Furthermore,\\nthere is no systematic categorization or evaluation of existing solutions\\nwithin the retrieval process. In this paper, we present LEGO-GraphRAG, a\\nmodular framework that decomposes the retrieval process of GraphRAG into three\\ninterconnected modules: subgraph-extraction, path-filtering, and\\npath-refinement. We systematically summarize and classify the algorithms and\\nneural network (NN) models relevant to each module, providing a clearer\\nunderstanding of the design space for GraphRAG instances. Additionally, we\\nidentify key design factors, such as Graph Coupling and Computational Cost,\\nthat influence the effectiveness of GraphRAG implementations. Through extensive\\nempirical studies, we construct high-quality GraphRAG instances using a\\nrepresentative selection of solutions and analyze their impact on retrieval and\\nreasoning performance. Our findings offer critical insights into optimizing\\nGraphRAG instance design, ultimately contributing to the advancement of more\\naccurate and contextually relevant LLM applications.',\n",
       "  'published': '2024-11-06T15:32:28Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.05844v1',\n",
       "  'authors': ['Yukun Cao',\n",
       "   'Zengyi Gao',\n",
       "   'Zhiyang Li',\n",
       "   'Xike Xie',\n",
       "   'S Kevin Zhou'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.05844v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0091_LEGO-GraphRAG_Modularizing_Graph-based_Retrieval-Augmented_Generation_for_Design_Space_Exploration.pdf'},\n",
       " {'paper_id': 92,\n",
       "  'title': \"Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in\\n  Retrieval-Augmented Generation\",\n",
       "  'summary': \"Retrieval-Augmented Generation (RAG) has proven to be an effective method for\\nmitigating hallucination issues inherent in large language models (LLMs).\\nPrevious approaches typically train retrievers based on semantic similarity,\\nlacking optimization for RAG. More recent works have proposed aligning\\nretrievers with the preference signals of LLMs. However, these preference\\nsignals are often difficult for dense retrievers, which typically have weaker\\nlanguage capabilities, to understand and learn effectively. Drawing inspiration\\nfrom pedagogical theories like Guided Discovery Learning, we propose a novel\\nframework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the\\nlanguage capabilities of LLMs to construct examples from a more granular,\\ninformation-centric perspective to guide the learning of retrievers.\\nSpecifically, our method utilizes LLMs to construct easy-to-understand examples\\nfrom samples where the retriever performs poorly, focusing on three learning\\nobjectives highly relevant to the RAG scenario: relevance, comprehensiveness,\\nand purity. These examples serve as scaffolding to ultimately align the\\nretriever with the LLM's preferences. Furthermore, we employ a dual curriculum\\nlearning strategy and leverage the reciprocal feedback between LLM and\\nretriever to further enhance the performance of the RAG system. A series of\\nexperiments demonstrate that our proposed framework enhances the performance of\\nRAG systems equipped with different retrievers and is applicable to various\\nLLMs.\",\n",
       "  'published': '2024-11-06T14:42:39Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.03957v1',\n",
       "  'authors': ['Yuhang Liu',\n",
       "   'Xueyu Hu',\n",
       "   'Shengyu Zhang',\n",
       "   'Jingyuan Chen',\n",
       "   'Fan Wu',\n",
       "   'Fei Wu'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.03957v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0092_Fine-Grained_Guidance_for_Retrievers_Leveraging_LLMs_Feedback_in_Retrieval-Augmented_Generation.pdf'},\n",
       " {'paper_id': 93,\n",
       "  'title': 'RAGulator: Lightweight Out-of-Context Detectors for Grounded Text\\n  Generation',\n",
       "  'summary': 'Real-time detection of out-of-context LLM outputs is crucial for enterprises\\nlooking to safely adopt RAG applications. In this work, we train lightweight\\nmodels to discriminate LLM-generated text that is semantically out-of-context\\nfrom retrieved text documents. We preprocess a combination of summarisation and\\nsemantic textual similarity datasets to construct training data using minimal\\nresources. We find that DeBERTa is not only the best-performing model under\\nthis pipeline, but it is also fast and does not require additional text\\npreprocessing or feature engineering. While emerging work demonstrates that\\ngenerative LLMs can also be fine-tuned and used in complex data pipelines to\\nachieve state-of-the-art performance, we note that speed and resource limits\\nare important considerations for on-premise deployment.',\n",
       "  'published': '2024-11-06T13:51:42Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.03920v1',\n",
       "  'authors': ['Ian Poey', 'Jiajun Liu', 'Qishuai Zhong', 'Adrien Chenailler'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.03920v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0093_RAGulator_Lightweight_Out-of-Context_Detectors_for_Grounded_Text_Generation.pdf'},\n",
       " {'paper_id': 94,\n",
       "  'title': 'Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge\\n  Reasoning and Text Generation',\n",
       "  'summary': 'This study aims to optimize the existing retrieval-augmented generation model\\n(RAG) by introducing a graph structure to improve the performance of the model\\nin dealing with complex knowledge reasoning tasks. The traditional RAG model\\nhas the problem of insufficient processing efficiency when facing complex graph\\nstructure information (such as knowledge graphs, hierarchical relationships,\\netc.), which affects the quality and consistency of the generated results. This\\nstudy proposes a scheme to process graph structure data by combining graph\\nneural network (GNN), so that the model can capture the complex relationship\\nbetween entities, thereby improving the knowledge consistency and reasoning\\nability of the generated text. The experiment used the Natural Questions (NQ)\\ndataset and compared it with multiple existing generation models. The results\\nshow that the graph-based RAG model proposed in this paper is superior to the\\ntraditional generation model in terms of quality, knowledge consistency, and\\nreasoning ability, especially when dealing with tasks that require\\nmulti-dimensional reasoning. Through the combination of the enhancement of the\\nretrieval module and the graph neural network, the model in this study can\\nbetter handle complex knowledge background information and has broad potential\\nvalue in multiple practical application scenarios.',\n",
       "  'published': '2024-11-06T00:23:55Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.03572v1',\n",
       "  'authors': ['Yuxin Dong',\n",
       "   'Shuo Wang',\n",
       "   'Hongye Zheng',\n",
       "   'Jiajing Chen',\n",
       "   'Zhenhong Zhang',\n",
       "   'Chihang Wang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.03572v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0094_Advanced_RAG_Models_with_Graph_Structures_Optimizing_Complex_Knowledge_Reasoning_and_Text_Generation.pdf'},\n",
       " {'paper_id': 95,\n",
       "  'title': 'Long Context RAG Performance of Large Language Models',\n",
       "  'summary': 'Retrieval Augmented Generation (RAG) has emerged as a crucial technique for\\nenhancing the accuracy of Large Language Models (LLMs) by incorporating\\nexternal information. With the advent of LLMs that support increasingly longer\\ncontext lengths, there is a growing interest in understanding how these models\\nperform in RAG scenarios. Can these new long context models improve RAG\\nperformance? This paper presents a comprehensive study of the impact of\\nincreased context length on RAG performance across 20 popular open source and\\ncommercial LLMs. We ran RAG workflows while varying the total context length\\nfrom 2,000 to 128,000 tokens (and 2 million tokens when possible) on three\\ndomain-specific datasets, and report key insights on the benefits and\\nlimitations of long context in RAG applications. Our findings reveal that while\\nretrieving more documents can improve performance, only a handful of the most\\nrecent state of the art LLMs can maintain consistent accuracy at long context\\nabove 64k tokens. We also identify distinct failure modes in long context\\nscenarios, suggesting areas for future research.',\n",
       "  'published': '2024-11-05T22:37:43Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.03538v1',\n",
       "  'authors': ['Quinn Leng',\n",
       "   'Jacob Portes',\n",
       "   'Sam Havens',\n",
       "   'Matei Zaharia',\n",
       "   'Michael Carbin'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.03538v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0095_Long_Context_RAG_Performance_of_Large_Language_Models.pdf'},\n",
       " {'paper_id': 96,\n",
       "  'title': 'HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge\\n  in RAG Systems',\n",
       "  'summary': 'Retrieval-Augmented Generation (RAG) has been shown to improve knowledge\\ncapabilities and alleviate the hallucination problem of LLMs. The Web is a\\nmajor source of external knowledge used in RAG systems, and many commercial\\nsystems such as ChatGPT and Perplexity have used Web search engines as their\\nmajor retrieval systems. Typically, such RAG systems retrieve search results,\\ndownload HTML sources of the results, and then extract plain texts from the\\nHTML sources. Plain text documents or chunks are fed into the LLMs to augment\\nthe generation. However, much of the structural and semantic information\\ninherent in HTML, such as headings and table structures, is lost during this\\nplain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,\\nwhich uses HTML instead of plain text as the format of retrieved knowledge in\\nRAG. We believe HTML is better than plain text in modeling knowledge in\\nexternal documents, and most LLMs possess robust capacities to understand HTML.\\nHowever, utilizing HTML presents new challenges. HTML contains additional\\ncontent such as tags, JavaScript, and CSS specifications, which bring extra\\ninput tokens and noise to the RAG system. To address this issue, we propose\\nHTML cleaning, compression, and pruning strategies, to shorten the HTML while\\nminimizing the loss of information. Specifically, we design a two-step\\nblock-tree-based pruning method that prunes useless HTML blocks and keeps only\\nthe relevant part of the HTML. Experiments on six QA datasets confirm the\\nsuperiority of using HTML in RAG systems.',\n",
       "  'published': '2024-11-05T09:58:36Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.02959v1',\n",
       "  'authors': ['Jiejun Tan',\n",
       "   'Zhicheng Dou',\n",
       "   'Wen Wang',\n",
       "   'Mang Wang',\n",
       "   'Weipeng Chen',\n",
       "   'Ji-Rong Wen'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.02959v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0096_HtmlRAG_HTML_is_Better_Than_Plain_Text_for_Modeling_Retrieved_Knowledge_in_RAG_Systems.pdf'},\n",
       " {'paper_id': 97,\n",
       "  'title': 'WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African\\n  clean water access, sanitation and hygiene',\n",
       "  'summary': \"This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate\\nrural African communities on clean water access, sanitation, and hygiene (WASH)\\nprinciples. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach\\nto address the limitations of previous approaches with limited reach or missing\\ncontextualization. The paper details the development process, employing Design\\nScience Research Methodology. The evaluation consisted of two phases: content\\nvalidation by four WASH experts and community validation by potential users.\\nContent validation confirmed WASHtsApp's ability to provide accurate and\\nrelevant WASH-related information. Community validation indicated high user\\nacceptance and perceived usefulness of the chatbot. The paper concludes by\\ndiscussing the potential for further development, including incorporating local\\nlanguages and user data analysis for targeted interventions. It also proposes\\nfuture research cycles focused on wider deployment and leveraging user data for\\neducational purposes.\",\n",
       "  'published': '2024-11-05T06:44:15Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.02850v1',\n",
       "  'authors': ['Simon Kloker', 'Alex Cedric Luyima', 'Matthew Bazanya'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.02850v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0097_WASHtsApp_--_A_RAG-powered_WhatsApp_Chatbot_for_supporting_rural_African_clean_water_access_sanitation_and_hygiene.pdf'},\n",
       " {'paper_id': 98,\n",
       "  'title': 'PersianRAG: A Retrieval-Augmented Generation System for Persian Language',\n",
       "  'summary': 'Retrieval augmented generation (RAG) models, which integrate large-scale\\npre-trained generative models with external retrieval mechanisms, have shown\\nsignificant success in various natural language processing (NLP) tasks.\\nHowever, applying RAG models in Persian language as a low-resource language,\\nposes distinct challenges. These challenges primarily involve the\\npreprocessing, embedding, retrieval, prompt construction, language modeling,\\nand response evaluation of the system. In this paper, we address the challenges\\ntowards implementing a real-world RAG system for Persian language called\\nPersianRAG. We propose novel solutions to overcome these obstacles and evaluate\\nour approach using several Persian benchmark datasets. Our experimental results\\ndemonstrate the capability of the PersianRAG framework to enhance question\\nanswering task in Persian.',\n",
       "  'published': '2024-11-05T06:11:17Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.02832v2',\n",
       "  'authors': ['Hossein Hosseini',\n",
       "   'Mohammad Sobhan Zare',\n",
       "   'Amir Hossein Mohammadi',\n",
       "   'Arefeh Kazemi',\n",
       "   'Zahra Zojaji',\n",
       "   'Mohammad Ali Nematbakhsh'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.02832v2.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0098_PersianRAG_A_Retrieval-Augmented_Generation_System_for_Persian_Language.pdf'},\n",
       " {'paper_id': 99,\n",
       "  'title': 'Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare\\n  Disease Knowledge',\n",
       "  'summary': \"Rare diseases present unique challenges in healthcare, often suffering from\\ndelayed diagnosis and fragmented information landscapes. The scarcity of\\nreliable knowledge in these conditions poses a distinct challenge for Large\\nLanguage Models (LLMs) in supporting clinical management and delivering precise\\npatient information underscoring the need for focused training on these 'zebra'\\ncases. We present Zebra-Llama, a specialized context-aware language model with\\nhigh precision Retrieval Augmented Generation (RAG) capability, focusing on\\nEhlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000\\nindividuals, exemplifies the complexities of rare diseases with its diverse\\nsymptoms, multiple subtypes, and evolving diagnostic criteria. By implementing\\na novel context-aware fine-tuning methodology trained on questions derived from\\nmedical literature, patient experiences, and clinical resources, along with\\nexpertly curated responses, Zebra-Llama demonstrates unprecedented capabilities\\nin handling EDS-related queries. On a test set of real-world questions\\ncollected from EDS patients and clinicians, medical experts evaluated the\\nresponses generated by both models, revealing Zebra-Llama's substantial\\nimprovements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.\\n70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation\\nreliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama\\nnot only provides more accessible and reliable EDS information but also\\nestablishes a framework for developing specialized AI solutions for other rare\\nconditions. This work represents a crucial step towards democratizing\\nexpert-level knowledge in rare disease management, potentially transforming how\\nhealthcare providers and patients navigate the complex landscape of rare\\ndiseases.\",\n",
       "  'published': '2024-11-04T22:45:52Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.02657v1',\n",
       "  'authors': ['Karthik Soman',\n",
       "   'Andrew Langdon',\n",
       "   'Catalina Villouta',\n",
       "   'Chinmay Agrawal',\n",
       "   'Lashaw Salta',\n",
       "   'Braian Peetoom',\n",
       "   'Gianmarco Bellucci',\n",
       "   'Orion J Buske'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.02657v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0099_Zebra-Llama_A_Context-Aware_Large_Language_Model_for_Democratizing_Rare_Disease_Knowledge.pdf'},\n",
       " {'paper_id': 100,\n",
       "  'title': 'TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context\\n  Support for Network',\n",
       "  'summary': \"The telecommunications industry's rapid evolution demands intelligent systems\\ncapable of managing complex networks and adapting to emerging technologies.\\nWhile large language models (LLMs) show promise in addressing these challenges,\\ntheir deployment in telecom environments faces significant constraints due to\\nedge device limitations and inconsistent documentation. To bridge this gap, we\\npresent TeleOracle, a telecom-specialized retrieval-augmented generation (RAG)\\nsystem built on the Phi-2 small language model (SLM). To improve context\\nretrieval, TeleOracle employs a two-stage retriever that incorporates semantic\\nchunking and hybrid keyword and semantic search. Additionally, we expand the\\ncontext window during inference to enhance the model's performance on\\nopen-ended queries. We also employ low-rank adaption for efficient fine-tuning.\\nA thorough analysis of the model's performance indicates that our RAG framework\\nis effective in aligning Phi-2 to the telecom domain in a downstream question\\nand answer (QnA) task, achieving a 30% improvement in accuracy over the base\\nPhi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our\\nmodel not only performs on par with the much larger LLMs but also achieves a\\nhigher faithfulness score, indicating higher adherence to the retrieved\\ncontext.\",\n",
       "  'published': '2024-11-04T21:12:08Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.02617v1',\n",
       "  'authors': ['Nouf Alabbasi',\n",
       "   'Omar Erak',\n",
       "   'Omar Alhussein',\n",
       "   'Ismail Lotfi',\n",
       "   'Sami Muhaidat',\n",
       "   'Merouane Debbah'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.02617v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0100_TeleOracle_Fine-Tuned_Retrieval-Augmented_Generation_with_Long-Context_Support_for_Network.pdf'},\n",
       " {'paper_id': 101,\n",
       "  'title': 'Social-RAG: Retrieving from Group Interactions to Socially Ground\\n  Proactive AI Generation to Group Preferences',\n",
       "  'summary': \"AI agents are increasingly tasked with making proactive suggestions in online\\nspaces where groups collaborate, but can be unhelpful or even annoying, due to\\nnot fitting the group's preferences or behaving in socially inappropriate ways.\\nFortunately, group spaces have a rich history of prior social interactions and\\naffordances for social feedback to support creating agents that align to a\\ngroup's interests and norms. We present Social-RAG, a workflow for grounding\\nagents to social information about a group, which retrieves from prior group\\ninteractions, selects relevant social signals, and then feeds the context into\\na large language model to generate messages to the group. We implement this\\ninto PaperPing, our system that posts academic paper recommendations in group\\nchat, leveraging social signals determined from formative studies with 39\\nresearchers. From a three-month deployment in 18 channels, we observed\\nPaperPing posted relevant messages in groups without disrupting their existing\\nsocial practices, fostering group common ground.\",\n",
       "  'published': '2024-11-04T18:21:53Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.02353v1',\n",
       "  'authors': ['Ruotong Wang',\n",
       "   'Xinyi Zhou',\n",
       "   'Lin Qiu',\n",
       "   'Joseph Chee Chang',\n",
       "   'Jonathan Bragg',\n",
       "   'Amy X. Zhang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.02353v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0101_Social-RAG_Retrieving_from_Group_Interactions_to_Socially_Ground_Proactive_AI_Generation_to_Group_Preferences.pdf'},\n",
       " {'paper_id': 102,\n",
       "  'title': 'QCG-Rerank: Chunks Graph Rerank with Query Expansion in\\n  Retrieval-Augmented LLMs for Tourism Domain',\n",
       "  'summary': 'Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in\\nLarge Language Models (LLMs) by integrating information retrieval techniques.\\nHowever, in the tourism domain, since the query is usually brief and the\\ncontent in the database is diverse, existing RAG may contain a significant\\namount of irrelevant or contradictory information contents after retrieval. To\\naddress this challenge, we propose the QCG-Rerank model. This model first\\nperforms an initial retrieval to obtain candidate chunks and then enhances\\nsemantics by extracting critical information to expand the original query.\\nNext, we utilize the expanded query and candidate chunks to calculate\\nsimilarity scores as the initial transition probability and construct the\\nchunks graph. Subsequently, We iteratively compute the transition probabilities\\nbased on an initial estimate until convergence. The chunks with the highest\\nscore are selected and input into the LLMs to generate responses. We evaluate\\nthe model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets.\\nThe experimental results demonstrate the effectiveness and superiority of the\\nQCG-Rerank method.',\n",
       "  'published': '2024-11-04T08:15:22Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.08724v1',\n",
       "  'authors': ['Qikai Wei',\n",
       "   'Mingzhi Yang',\n",
       "   'Chunlong Han',\n",
       "   'Jingfu Wei',\n",
       "   'Minghao Zhang',\n",
       "   'Feifei Shi',\n",
       "   'Huansheng Ning'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.08724v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0102_QCG-Rerank_Chunks_Graph_Rerank_with_Query_Expansion_in_Retrieval-Augmented_LLMs_for_Tourism_Domain.pdf'},\n",
       " {'paper_id': 103,\n",
       "  'title': 'Can Language Models Enable In-Context Database?',\n",
       "  'summary': \"Large language models (LLMs) are emerging as few-shot learners capable of\\nhandling a variety of tasks, including comprehension, planning, reasoning,\\nquestion answering, arithmetic calculations, and more. At the core of these\\ncapabilities is LLMs' proficiency in representing and understanding structural\\nor semi-structural data, such as tables and graphs. Numerous studies have\\ndemonstrated that reasoning on tabular data or graphs is not only feasible for\\nLLMs but also gives a promising research direction which treats these data as\\nin-context data. The lightweight and human readable characteristics of\\nin-context database can potentially make it an alternative for the traditional\\ndatabase in typical RAG (Retrieval Augmented Generation) settings. However,\\nalmost all current work focuses on static in-context data, which does not allow\\ndynamic update. In this paper, to enable dynamic database update, delta\\nencoding of database is proposed. We explore how data stored in traditional\\nRDBMS can be encoded as in-context text and evaluate LLMs' proficiency for CRUD\\n(Create, Read, Update and Delete) operations on in-context databases. A\\nbenchmark named InConDB is presented and extensive experiments are conducted to\\nshow the performance of different language models in enabling in-context\\ndatabase by varying the database encoding method, prompting method, operation\\ntype and input data distribution, revealing both the proficiency and\\nlimitations.\",\n",
       "  'published': '2024-11-04T05:25:39Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.01807v1',\n",
       "  'authors': ['Yu Pan', 'Hongfeng Yu', 'Tianjiao Zhao', 'Jianxin Sun'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.01807v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0103_Can_Language_Models_Enable_In-Context_Database.pdf'},\n",
       " {'paper_id': 104,\n",
       "  'title': 'RAGViz: Diagnose and Visualize Retrieval-Augmented Generation',\n",
       "  'summary': \"Retrieval-augmented generation (RAG) combines knowledge from domain-specific\\nsources into large language models to ground answer generation. Current RAG\\nsystems lack customizable visibility on the context documents and the model's\\nattentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool\\nthat visualizes the attentiveness of the generated tokens in retrieved\\ndocuments. With a built-in user interface, retrieval index, and Large Language\\nModel (LLM) backbone, RAGViz provides two main functionalities: (1) token and\\ndocument-level attention visualization, and (2) generation comparison upon\\ncontext document addition and removal. As an open-source toolkit, RAGViz can be\\neasily hosted with a custom embedding model and HuggingFace-supported LLM\\nbackbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,\\nmemory-efficient LLM inference tool, and custom context snippet method, RAGViz\\noperates efficiently with a median query time of about 5 seconds on a moderate\\nGPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo\\nvideo of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.\",\n",
       "  'published': '2024-11-04T02:30:05Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.01751v1',\n",
       "  'authors': ['Tevin Wang', 'Jingyuan He', 'Chenyan Xiong'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.01751v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0104_RAGViz_Diagnose_and_Visualize_Retrieval-Augmented_Generation.pdf'},\n",
       " {'paper_id': 105,\n",
       "  'title': 'RuAG: Learned-rule-augmented Generation for Large Language Models',\n",
       "  'summary': \"In-context learning (ICL) and Retrieval-Augmented Generation (RAG) have\\ngained attention for their ability to enhance LLMs' reasoning by incorporating\\nexternal knowledge but suffer from limited contextual window size, leading to\\ninsufficient information injection. To this end, we propose a novel framework,\\nRuAG, to automatically distill large volumes of offline data into interpretable\\nfirst-order logic rules, which are injected into LLMs to boost their reasoning\\ncapabilities. Our method begins by formulating the search process relying on\\nLLMs' commonsense, where LLMs automatically define head and body predicates.\\nThen, RuAG applies Monte Carlo Tree Search (MCTS) to address the combinational\\nsearching space and efficiently discover logic rules from data. The resulting\\nlogic rules are translated into natural language, allowing targeted knowledge\\ninjection and seamless integration into LLM prompts for LLM's downstream task\\nreasoning. We evaluate our framework on public and private industrial tasks,\\nincluding natural language processing, time-series, decision-making, and\\nindustrial tasks, demonstrating its effectiveness in enhancing LLM's capability\\nover diverse tasks.\",\n",
       "  'published': '2024-11-04T00:01:34Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.03349v1',\n",
       "  'authors': ['Yudi Zhang',\n",
       "   'Pei Xiao',\n",
       "   'Lu Wang',\n",
       "   'Chaoyun Zhang',\n",
       "   'Meng Fang',\n",
       "   'Yali Du',\n",
       "   'Yevgeniy Puzyrev',\n",
       "   'Randolph Yao',\n",
       "   'Si Qin',\n",
       "   'Qingwei Lin',\n",
       "   'Mykola Pechenizkiy',\n",
       "   'Dongmei Zhang',\n",
       "   'Saravan Rajmohan',\n",
       "   'Qi Zhang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.03349v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0105_RuAG_Learned-rule-augmented_Generation_for_Large_Language_Models.pdf'},\n",
       " {'paper_id': 106,\n",
       "  'title': 'Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors',\n",
       "  'summary': 'Despite significant advancements, large language models (LLMs) still struggle\\nwith providing accurate answers when lacking domain-specific or up-to-date\\nknowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by\\nincorporating external knowledge bases, but it also introduces new attack\\nsurfaces. In this paper, we investigate data extraction attacks targeting the\\nknowledge databases of RAG systems. We demonstrate that previous attacks on RAG\\nlargely depend on the instruction-following capabilities of LLMs, and that\\nsimple fine-tuning can reduce the success rate of such attacks to nearly zero.\\nThis makes these attacks impractical since fine-tuning is a common practice\\nwhen deploying LLMs in specific domains. To further reveal the vulnerability,\\nwe propose to backdoor RAG, where a small portion of poisoned data is injected\\nduring the fine-tuning phase to create a backdoor within the LLM. When this\\ncompromised LLM is integrated into a RAG system, attackers can exploit specific\\ntriggers in prompts to manipulate the LLM to leak documents from the retrieval\\ndatabase. By carefully designing the poisoned data, we achieve both verbatim\\nand paraphrased document extraction. We show that with only 3\\\\% poisoned data,\\nour method achieves an average success rate of 79.7\\\\% in verbatim extraction on\\nLlama2-7B, with a ROUGE-L score of 64.21, and a 68.6\\\\% average success rate in\\nparaphrased extraction, with an average ROUGE score of 52.6 across four\\ndatasets. These results underscore the privacy risks associated with the supply\\nchain when deploying RAG systems.',\n",
       "  'published': '2024-11-03T22:27:40Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.01705v1',\n",
       "  'authors': ['Yuefeng Peng', 'Junda Wang', 'Hong Yu', 'Amir Houmansadr'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.01705v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0106_Data_Extraction_Attacks_in_Retrieval-Augmented_Generation_via_Backdoors.pdf'},\n",
       " {'paper_id': 107,\n",
       "  'title': 'AttackQA: Development and Adoption of a Dataset for Assisting\\n  Cybersecurity Operations using Fine-tuned and Open-Source LLMs',\n",
       "  'summary': \"Retrieval-augmented generation (RAG) on specialized domain datasets has shown\\nimproved performance when large language models (LLMs) are fine-tuned for\\ngenerating responses to user queries. In this study, we develop a cybersecurity\\nquestion-answering (Q\\\\&A) dataset, called AttackQA, and employ it to build a\\nRAG-based Q\\\\&A system designed for analysts in security operations centers. The\\ndataset comprises 25,335 Q\\\\&A pairs, accompanied by rationales to facilitate\\nfine-tuning and evaluation. 80\\\\% of the dataset was generated with help of a\\nlightweight open-source LLM (LLama 3 8B), which produced over 1100 tokens per\\nsecond with full 16-bit precision on SambaNova System's SN40L specialized\\nhardware. To ensure dataset quality, we fine-tuned LLama 3 70B to detect and\\nreject low-quality Q\\\\&A pairs. In using the dataset for RAG, we demonstrate\\nthat fine-tuning open-source embeddings and LLMs can yield superior accuracy\\ncompared to OpenAI's state-of-the-art proprietary embedding and LLM (GPT-4o).\\nFurthermore, we use Llama 3.1 405B as a judge to evaluate answer correctness,\\nenabling the creation of a fully open-source, high-speed RAG and evaluation\\npipeline with a benchmark for model accuracy.\",\n",
       "  'published': '2024-11-01T23:03:40Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.01073v1',\n",
       "  'authors': ['Varun Badrinath Krishna'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.01073v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0107_AttackQA_Development_and_Adoption_of_a_Dataset_for_Assisting_Cybersecurity_Operations_using_Fine-tuned_and_Open-Source_LLMs.pdf'},\n",
       " {'paper_id': 108,\n",
       "  'title': 'Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM\\n  Generation Output',\n",
       "  'summary': 'We present a light-weight approach for detecting nonfactual outputs from\\nretrieval-augmented generation (RAG). Given a context and putative output, we\\ncompute a factuality score that can be thresholded to yield a binary decision\\nto check the results of LLM-based question-answering, summarization, or other\\nsystems. Unlike factuality checkers that themselves rely on LLMs, we use\\ncompact, open-source natural language inference (NLI) models that yield a\\nfreely accessible solution with low latency and low cost at run-time, and no\\nneed for LLM fine-tuning. The approach also enables downstream mitigation and\\ncorrection of hallucinations, by tracing them back to specific context chunks.\\nOur experiments show high area under the ROC curve (AUC) across a wide range of\\nrelevant open source datasets, indicating the effectiveness of our method for\\nfact-checking RAG output.',\n",
       "  'published': '2024-11-01T20:44:59Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.01022v1',\n",
       "  'authors': ['Hithesh Sankararaman',\n",
       "   'Mohammed Nasheed Yasin',\n",
       "   'Tanner Sorensen',\n",
       "   'Alessandro Di Bari',\n",
       "   'Andreas Stolcke'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.01022v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0108_Provenance_A_Light-weight_Fact-checker_for_Retrieval_Augmented_LLM_Generation_Output.pdf'},\n",
       " {'paper_id': 109,\n",
       "  'title': 'CORAG: A Cost-Constrained Retrieval Optimization System for\\n  Retrieval-Augmented Generation',\n",
       "  'summary': 'Large Language Models (LLMs) have demonstrated remarkable generation\\ncapabilities but often struggle to access up-to-date information, which can\\nlead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this\\nissue by incorporating knowledge from external databases, enabling more\\naccurate and relevant responses. Due to the context window constraints of LLMs,\\nit is impractical to input the entire external database context directly into\\nthe model. Instead, only the most relevant information, referred to as chunks,\\nis selectively retrieved. However, current RAG research faces three key\\nchallenges. First, existing solutions often select each chunk independently,\\noverlooking potential correlations among them. Second, in practice the utility\\nof chunks is non-monotonic, meaning that adding more chunks can decrease\\noverall utility. Traditional methods emphasize maximizing the number of\\nincluded chunks, which can inadvertently compromise performance. Third, each\\ntype of user query possesses unique characteristics that require tailored\\nhandling, an aspect that current approaches do not fully consider. To overcome\\nthese challenges, we propose a cost constrained retrieval optimization system\\nCORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search\\n(MCTS) based policy framework to find optimal chunk combinations sequentially,\\nallowing for a comprehensive consideration of correlations among chunks.\\nAdditionally, rather than viewing budget exhaustion as a termination condition,\\nwe integrate budget constraints into the optimization of chunk combinations,\\neffectively addressing the non-monotonicity of chunk utility.',\n",
       "  'published': '2024-11-01T17:11:16Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.00744v1',\n",
       "  'authors': ['Ziting Wang',\n",
       "   'Haitao Yuan',\n",
       "   'Wei Dong',\n",
       "   'Gao Cong',\n",
       "   'Feifei Li'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.00744v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0109_CORAG_A_Cost-Constrained_Retrieval_Optimization_System_for_Retrieval-Augmented_Generation.pdf'},\n",
       " {'paper_id': 110,\n",
       "  'title': 'Towards Multi-Source Retrieval-Augmented Generation via Synergizing\\n  Reasoning and Preference-Driven Retrieval',\n",
       "  'summary': 'Retrieval-Augmented Generation (RAG) has emerged as a reliable external\\nknowledge augmentation technique to mitigate hallucination issues and\\nparameterized knowledge limitations in Large Language Models (LLMs). Existing\\nAdaptive RAG (ARAG) systems struggle to effectively explore multiple retrieval\\nsources due to their inability to select the right source at the right time. To\\naddress this, we propose a multi-source ARAG framework, termed MSPR, which\\nsynergizes reasoning and preference-driven retrieval to adaptive decide \"when\\nand what to retrieve\" and \"which retrieval source to use\". To better adapt to\\nretrieval sources of differing characteristics, we also employ retrieval action\\nadjustment and answer feedback strategy. They enable our framework to fully\\nexplore the high-quality primary source while supplementing it with secondary\\nsources at the right time. Extensive and multi-dimensional experiments\\nconducted on three datasets demonstrate the superiority and effectiveness of\\nMSPR.',\n",
       "  'published': '2024-11-01T15:50:58Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.00689v1',\n",
       "  'authors': ['Qingfei Zhao',\n",
       "   'Ruobing Wang',\n",
       "   'Xin Wang',\n",
       "   'Daren Zha',\n",
       "   'Nan Mu'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.00689v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0110_Towards_Multi-Source_Retrieval-Augmented_Generation_via_Synergizing_Reasoning_and_Preference-Driven_Retrieval.pdf'},\n",
       " {'paper_id': 111,\n",
       "  'title': 'Rationale-Guided Retrieval Augmented Generation for Medical Question\\n  Answering',\n",
       "  'summary': 'Large language models (LLM) hold significant potential for applications in\\nbiomedicine, but they struggle with hallucinations and outdated knowledge.\\nWhile retrieval-augmented generation (RAG) is generally employed to address\\nthese issues, it also has its own set of challenges: (1) LLMs are vulnerable to\\nirrelevant or incorrect context, (2) medical queries are often not\\nwell-targeted for helpful information, and (3) retrievers are prone to bias\\ntoward the specific source corpus they were trained on. In this study, we\\npresent RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the\\nreliability of RAG in biomedical contexts. RAG$^2$ incorporates three key\\ninnovations: a small filtering model trained on perplexity-based labels of\\nrationales, which selectively augments informative snippets of documents while\\nfiltering out distractors; LLM-generated rationales as queries to improve the\\nutility of retrieved snippets; a structure designed to retrieve snippets evenly\\nfrom a comprehensive set of four biomedical corpora, effectively mitigating\\nretriever bias. Our experiments demonstrate that RAG$^2$ improves the\\nstate-of-the-art LLMs of varying sizes, with improvements of up to 6.1\\\\%, and\\nit outperforms the previous best medical RAG model by up to 5.6\\\\% across three\\nmedical question-answering benchmarks. Our code is available at\\nhttps://github.com/dmis-lab/RAG2.',\n",
       "  'published': '2024-11-01T01:40:23Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.00300v1',\n",
       "  'authors': ['Jiwoong Sohn',\n",
       "   'Yein Park',\n",
       "   'Chanwoong Yoon',\n",
       "   'Sihyeon Park',\n",
       "   'Hyeon Hwang',\n",
       "   'Mujeen Sung',\n",
       "   'Hyunjae Kim',\n",
       "   'Jaewoo Kang'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.00300v1.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0111_Rationale-Guided_Retrieval_Augmented_Generation_for_Medical_Question_Answering.pdf'},\n",
       " {'paper_id': 112,\n",
       "  'title': 'LLM-Ref: Enhancing Reference Handling in Technical Writing with Large\\n  Language Models',\n",
       "  'summary': \"Large Language Models (LLMs) excel in data synthesis but can be inaccurate in\\ndomain-specific tasks, which retrieval-augmented generation (RAG) systems\\naddress by leveraging user-provided data. However, RAGs require optimization in\\nboth retrieval and generation stages, which can affect output quality. In this\\npaper, we present LLM-Ref, a writing assistant tool that aids researchers in\\nwriting articles from multiple source documents with enhanced reference\\nsynthesis and handling capabilities. Unlike traditional RAG systems that use\\nchunking and indexing, our tool retrieves and generates content directly from\\ntext paragraphs. This method facilitates direct reference extraction from the\\ngenerated outputs, a feature unique to our tool. Additionally, our tool employs\\niterative response generation, effectively managing lengthy contexts within the\\nlanguage model's constraints. Compared to baseline RAG-based systems, our\\napproach achieves a $3.25\\\\times$ to $6.26\\\\times$ increase in Ragas score, a\\ncomprehensive metric that provides a holistic view of a RAG system's ability to\\nproduce accurate, relevant, and contextually appropriate responses. This\\nimprovement shows our method enhances the accuracy and contextual relevance of\\nwriting assistance tools.\",\n",
       "  'published': '2024-11-01T01:11:58Z',\n",
       "  'link': 'http://arxiv.org/abs/2411.00294v2',\n",
       "  'authors': ['Kazi Ahmed Asif Fuad', 'Lizhong Chen'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2411.00294v2.pdf',\n",
       "  'pdf_path': '/Users/pruthvibharadwaj/Documents/arxiv-summarizer-chat/data/RAG/Papers/0112_LLM-Ref_Enhancing_Reference_Handling_in_Technical_Writing_with_Large_Language_Models.pdf'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_pdf(results, papers_data):\n",
    "    # Initialize PDF with custom page size and margins\n",
    "    pdf = FPDF(format='A4')\n",
    "    pdf.set_auto_page_break(auto=True, margin=30)\n",
    "    pdf.add_page()\n",
    "    \n",
    "    # Set margins\n",
    "    margin = 25\n",
    "    pdf.set_margins(margin, margin, margin)\n",
    "    \n",
    "    # Calculate effective width for text\n",
    "    effective_width = pdf.w - (2 * margin)\n",
    "    \n",
    "    # Set font for main header\n",
    "    pdf.set_font('Arial', 'B', 16)\n",
    "    pdf.cell(effective_width, 10, 'Top 5 papers in RAG posted in November 2024', ln=True, align='C')\n",
    "    pdf.ln(10)\n",
    "    \n",
    "    # For each paper in results\n",
    "    for paper_id_str, paper_content in results.items():\n",
    "        paper_id = int(paper_id_str)\n",
    "        paper_data = next((p for p in papers_data if p['paper_id'] == paper_id), None)\n",
    "        if not paper_data:\n",
    "            continue\n",
    "        \n",
    "        # Paper title\n",
    "        pdf.set_font('Arial', 'B', 12)\n",
    "        title = paper_data['title'].strip()\n",
    "        pdf.multi_cell(effective_width, 8, title)\n",
    "        \n",
    "        # Hyperlinked text for paper link - aligned left\n",
    "        pdf.set_font('Arial', '', 9)\n",
    "        pdf.set_text_color(0, 0, 255)\n",
    "        # Move cursor to left margin\n",
    "        pdf.set_x(margin)\n",
    "        pdf.cell(30, 8, 'Link to paper', ln=True, link=paper_data.get('link', ''), align='L')\n",
    "        pdf.set_text_color(0, 0, 0)\n",
    "        pdf.ln(5)\n",
    "        \n",
    "        # Process questions and answers\n",
    "        for question, qa_content in paper_content.items():\n",
    "            # Check if need new page\n",
    "            if pdf.get_y() > pdf.h - 60:\n",
    "                pdf.add_page()\n",
    "            \n",
    "            # Question\n",
    "            pdf.set_font('Arial', 'B', 11)\n",
    "            pdf.multi_cell(effective_width, 8, question)\n",
    "            pdf.ln(2)\n",
    "            \n",
    "            # Answer\n",
    "            answer = qa_content.get('answer', '')\n",
    "            # Clean title references\n",
    "            title_pattern = f\"in the paper with( the title)? ['\\\"]?{paper_data['title']}['\\\"]?\"\n",
    "            answer = re.sub(title_pattern, \"in the paper\", answer, flags=re.IGNORECASE)\n",
    "            \n",
    "            pdf.set_font('Arial', '', 10)\n",
    "            pdf.multi_cell(effective_width, 8, answer.strip())\n",
    "            pdf.ln(8)\n",
    "        \n",
    "        # Add page break between papers\n",
    "        if paper_id_str != list(results.keys())[-1]:\n",
    "            pdf.add_page()\n",
    "    \n",
    "    # Save the PDF\n",
    "    pdf.output('top_5_rag_papers_analysis.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2l/bfgdjf7s3_5dkyx0jgmgv1l80000gn/T/ipykernel_47739/3095252605.py:15: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font('Arial', 'B', 16)\n",
      "/var/folders/2l/bfgdjf7s3_5dkyx0jgmgv1l80000gn/T/ipykernel_47739/3095252605.py:16: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=True use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
      "  pdf.cell(effective_width, 10, 'Top 5 papers in RAG posted in November 2024', ln=True, align='C')\n",
      "/var/folders/2l/bfgdjf7s3_5dkyx0jgmgv1l80000gn/T/ipykernel_47739/3095252605.py:27: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font('Arial', 'B', 12)\n",
      "/var/folders/2l/bfgdjf7s3_5dkyx0jgmgv1l80000gn/T/ipykernel_47739/3095252605.py:32: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font('Arial', '', 9)\n",
      "/var/folders/2l/bfgdjf7s3_5dkyx0jgmgv1l80000gn/T/ipykernel_47739/3095252605.py:36: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=True use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
      "  pdf.cell(30, 8, 'Link to paper', ln=True, link=paper_data.get('link', ''), align='L')\n",
      "/var/folders/2l/bfgdjf7s3_5dkyx0jgmgv1l80000gn/T/ipykernel_47739/3095252605.py:47: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font('Arial', 'B', 11)\n",
      "/var/folders/2l/bfgdjf7s3_5dkyx0jgmgv1l80000gn/T/ipykernel_47739/3095252605.py:57: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font('Arial', '', 10)\n"
     ]
    }
   ],
   "source": [
    "# Create the PDF\n",
    "create_rag_pdf(results, rag_papers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enefit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
